diff --git a/configs/swin/swin_large_patch4_window7_224_22kto1k_finetune.yaml b/configs/swin/swin_large_patch4_window7_224_22kto1k_finetune.yaml
index 0183826..f8cdb9e 100644
--- a/configs/swin/swin_large_patch4_window7_224_22kto1k_finetune.yaml
+++ b/configs/swin/swin_large_patch4_window7_224_22kto1k_finetune.yaml
@@ -8,9 +8,9 @@ MODEL:
     NUM_HEADS: [ 6, 12, 24, 48 ]
     WINDOW_SIZE: 7
 TRAIN:
-  EPOCHS: 30
+  EPOCHS: 1 
   WARMUP_EPOCHS: 5
   WEIGHT_DECAY: 1e-8
   BASE_LR: 2e-05
   WARMUP_LR: 2e-08
-  MIN_LR: 2e-07
\ No newline at end of file
+  MIN_LR: 2e-07
diff --git a/main.py b/main.py
index 62732e4..86a994e 100644
--- a/main.py
+++ b/main.py
@@ -62,6 +62,8 @@ def parse_option():
     parser.add_argument('--tag', help='tag of experiment')
     parser.add_argument('--eval', action='store_true', help='Perform evaluation only')
     parser.add_argument('--throughput', action='store_true', help='Test throughput only')
+    parser.add_argument('--iters', type=int, default=30000, metavar='N',help='iters per epoch')
+    parser.add_argument('--no-val', default=False, action='store_true',help='Do not validate in training epochs.')
 
     # distributed training
     parser.add_argument("--local_rank", type=int, required=True, help='local rank for DistributedDataParallel')
@@ -147,10 +149,11 @@ def main(config):
             save_checkpoint(config, epoch, model_without_ddp, max_accuracy, optimizer, lr_scheduler, loss_scaler,
                             logger)
 
-        acc1, acc5, loss = validate(config, data_loader_val, model)
-        logger.info(f"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%")
-        max_accuracy = max(max_accuracy, acc1)
-        logger.info(f'Max accuracy: {max_accuracy:.2f}%')
+        if args.no_val:
+            acc1, acc5, loss = validate(config, data_loader_val, model)
+            logger.info(f"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%")
+            max_accuracy = max(max_accuracy, acc1)
+            logger.info(f'Max accuracy: {max_accuracy:.2f}%')
 
     total_time = time.time() - start_time
     total_time_str = str(datetime.timedelta(seconds=int(total_time)))
@@ -213,8 +216,11 @@ def train_one_epoch(config, model, criterion, data_loader, optimizer, epoch, mix
                 f'grad_norm {norm_meter.val:.4f} ({norm_meter.avg:.4f})\t'
                 f'loss_scale {scaler_meter.val:.4f} ({scaler_meter.avg:.4f})\t'
                 f'mem {memory_used:.0f}MB')
+        if idx == args.iters:
+            break
     epoch_time = time.time() - start
     logger.info(f"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}")
+    
 
 
 @torch.no_grad()

