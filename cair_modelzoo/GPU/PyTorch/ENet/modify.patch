diff --git a/.gitignore b/.gitignore
old mode 100644
new mode 100755
diff --git a/LICENSE b/LICENSE
old mode 100644
new mode 100755
diff --git a/README.md b/README.md
old mode 100644
new mode 100755
diff --git a/args.py b/args.py
old mode 100644
new mode 100755
index d133835..3f9821b
--- a/args.py
+++ b/args.py
@@ -21,32 +21,43 @@ def get_arguments():
         action='store_true',
         help=("The model found in \"--save-dir/--name/\" and filename "
               "\"--name.h5\" is loaded."))
+    parser.add_argument("--model_path", type=str, default="")
 
     # Hyperparameters
+    parser.add_argument(
+        "--seed", 
+        type=int, 
+        default=1,
+        help="random seed")
     parser.add_argument(
         "--batch-size",
         "-b",
         type=int,
-        default=10,
+        default=2,
         help="The batch size. Default: 10")
     parser.add_argument(
         "--epochs",
         type=int,
         default=300,
         help="Number of training epochs. Default: 300")
+    parser.add_argument(
+        "--warmup",
+        type=int,
+        default=10,
+        help="")
     parser.add_argument(
         "--learning-rate",
         "-lr",
         type=float,
-        default=5e-4,
+        default=5e-4 * 3,
         help="The learning rate. Default: 5e-4")
     parser.add_argument(
         "--lr-decay",
         type=float,
-        default=0.1,
+        default=0.5,
         help="The learning rate decay factor. Default: 0.5")
     parser.add_argument(
-        "--lr-decay-epochs",
+        "--lr-decay-epochs",                                                                                                                  
         type=int,
         default=100,
         help="The number of epochs before adjusting the learning rate. "
@@ -62,23 +73,23 @@ def get_arguments():
     parser.add_argument(
         "--dataset",
         choices=['camvid', 'cityscapes'],
-        default='camvid',
+        default='cityscapes',
         help="Dataset to use. Default: camvid")
     parser.add_argument(
         "--dataset-dir",
         type=str,
-        default="data/CamVid",
+        default="/data/datasets-common/CityScapes",
         help="Path to the root directory of the selected dataset. "
         "Default: data/CamVid")
     parser.add_argument(
         "--height",
         type=int,
-        default=360,
+        default=512,
         help="The image height. Default: 360")
     parser.add_argument(
         "--width",
         type=int,
-        default=480,
+        default=1024,
         help="The image width. Default: 480")
     parser.add_argument(
         "--weighing",
@@ -124,4 +135,7 @@ def get_arguments():
         default='save',
         help="The directory where models are saved. Default: save")
 
+    parser.add_argument('--local_rank', type=int, default=0, help='local_rank')
+    parser.add_argument('--deterministic', action="store_true", default=True, help='local_rank')
+
     return parser.parse_args()
diff --git a/data/README.md b/data/README.md
old mode 100644
new mode 100755
diff --git a/data/__init__.py b/data/__init__.py
old mode 100644
new mode 100755
diff --git a/data/camvid.py b/data/camvid.py
old mode 100644
new mode 100755
diff --git a/data/cityscapes.py b/data/cityscapes.py
old mode 100644
new mode 100755
index ae3449f..f7ae61d
--- a/data/cityscapes.py
+++ b/data/cityscapes.py
@@ -20,16 +20,16 @@ class Cityscapes(data.Dataset):
 
     """
     # Training dataset root folders
-    train_folder = "leftImg8bit_trainvaltest/leftImg8bit/train"
-    train_lbl_folder = "gtFine_trainvaltest/gtFine/train"
+    train_folder = "leftImg8bit/train"
+    train_lbl_folder = "gtFine/train"
 
     # Validation dataset root folders
-    val_folder = "leftImg8bit_trainvaltest/leftImg8bit/val"
-    val_lbl_folder = "gtFine_trainvaltest/gtFine/val"
+    val_folder = "leftImg8bit/val"
+    val_lbl_folder = "gtFine/val"
 
     # Test dataset root folders
-    test_folder = "leftImg8bit_trainvaltest/leftImg8bit/test"
-    test_lbl_folder = "gtFine_trainvaltest/gtFine/test"
+    test_folder = "leftImg8bit/test"
+    test_lbl_folder = "gtFine/test"
 
     # Filters to find the images
     img_extension = '.png'
diff --git a/data/utils.py b/data/utils.py
old mode 100644
new mode 100755
index 297b514..3918928
--- a/data/utils.py
+++ b/data/utils.py
@@ -40,7 +40,8 @@ def get_files(folder, name_filter=None, extension_filter=None):
 
     # Explore the directory tree to get files that contain "name_filter" and
     # with extension "extension_filter"
-    for path, _, files in os.walk(folder):
+    for path, dir, files in os.walk(folder):
+        dir.sort()
         files.sort()
         for file in files:
             if name_cond(file) and ext_cond(file):
diff --git a/main.py b/main.py
index c44e8fd..5475e5b 100755
--- a/main.py
+++ b/main.py
@@ -1,11 +1,16 @@
 import os
+import time
+import random
+import numpy as np
 
 import torch
 import torch.nn as nn
-import torch.optim as optim
-import torch.optim.lr_scheduler as lr_scheduler
+# import torch.distributed as dist
 import torch.utils.data as data
 import torchvision.transforms as transforms
+import torch.optim as optim
+import torch.optim.lr_scheduler as lr_scheduler
+from torch.optim.lr_scheduler import _LRScheduler
 
 from PIL import Image
 
@@ -20,8 +25,57 @@ import utils
 
 # Get the arguments
 args = get_arguments()
-
-device = torch.device(args.device)
+print(args)
+
+def set_seed(SEED=0):
+    print("#"*20, "\n", "Use deterministic. SEED:", SEED)
+    random.seed(SEED)
+    np.random.seed(SEED)
+    torch.manual_seed(SEED)
+    torch.cuda.manual_seed(SEED)
+    torch.cuda.manual_seed_all(SEED)
+    torch.backends.cudnn.enabled = False 
+    torch.backends.cudnn.benchmark = False
+    torch.backends.cudnn.deterministic = True
+    torch.use_deterministic_algorithms(True)
+    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
+
+
+if args.deterministic:
+    set_seed(args.seed)
+    g = torch.Generator()
+    g.manual_seed(args.seed)
+
+def seed_worker(worker_id):
+    worker_seed = torch.initial_seed() % 2**32
+    np.random.seed(worker_seed)
+    random.seed(worker_seed)
+
+
+def dist_init(host_addr, rank, local_rank, world_size, port=1234):
+    host_addr_full = 'tcp://' + host_addr + ':' + str(port)
+    torch.distributed.init_process_group("nccl", init_method=host_addr_full,
+                                         rank=rank, world_size=world_size)
+    num_gpus = torch.cuda.device_count()
+    torch.cuda.set_device(local_rank)
+    assert torch.distributed.is_initialized()
+
+class WarmUpLR(_LRScheduler):
+    """warmup_training learning rate scheduler
+    Args:
+        optimizer: optimzier(e.g. SGD)
+        total_iters: totoal_iters of warmup phase
+    """
+    def __init__(self, optimizer, total_iters, last_epoch=-1):
+        
+        self.total_iters = total_iters
+        super().__init__(optimizer, last_epoch)
+ 
+    def get_lr(self):
+        """we will use the first m batches, and set the learning
+        rate to base_lr * m / total_iters
+        """
+        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]
 
 
 def load_dataset(dataset):
@@ -46,11 +100,25 @@ def load_dataset(dataset):
         args.dataset_dir,
         transform=image_transform,
         label_transform=label_transform)
-    train_loader = data.DataLoader(
-        train_set,
-        batch_size=args.batch_size,
-        shuffle=True,
-        num_workers=args.workers)
+    train_sampler = torch.utils.data.distributed.DistributedSampler(train_set, shuffle=True, num_replicas=args.world_size, rank=args.rank)
+    if args.deterministic:
+        train_loader = data.DataLoader(
+            dataset=train_set,
+            batch_size=args.batch_size,
+            shuffle=False,
+            num_workers=args.workers,
+            pin_memory=True, 
+            sampler=train_sampler,
+            worker_init_fn=seed_worker,
+            generator=g)
+    else:
+        train_loader = data.DataLoader(
+            dataset=train_set,
+            batch_size=args.batch_size,
+            shuffle=False,
+            num_workers=args.workers,
+            pin_memory=True, 
+            sampler=train_sampler)
 
     # Load the validation set as tensors
     val_set = dataset(
@@ -58,11 +126,25 @@ def load_dataset(dataset):
         mode='val',
         transform=image_transform,
         label_transform=label_transform)
-    val_loader = data.DataLoader(
-        val_set,
-        batch_size=args.batch_size,
-        shuffle=False,
-        num_workers=args.workers)
+    # val_sampler = torch.utils.data.distributed.DistributedSampler(val_set, shuffle=False, num_replicas=args.world_size, rank=args.rank)
+    if args.deterministic:
+        val_loader = data.DataLoader(
+            val_set,
+            batch_size=args.batch_size,
+            shuffle=False,
+            num_workers=args.workers,
+            pin_memory=True)#, 
+            # sampler=val_sampler,
+            # worker_init_fn=seed_worker,
+            # generator=g)
+    else:
+        val_loader = data.DataLoader(
+            val_set,
+            batch_size=args.batch_size,
+            shuffle=False,
+            num_workers=args.workers,
+            pin_memory=True)#, 
+            # sampler=val_sampler)
 
     # Load the test set as tensors
     test_set = dataset(
@@ -70,11 +152,25 @@ def load_dataset(dataset):
         mode='test',
         transform=image_transform,
         label_transform=label_transform)
-    test_loader = data.DataLoader(
-        test_set,
-        batch_size=args.batch_size,
-        shuffle=False,
-        num_workers=args.workers)
+    # test_sampler = torch.utils.data.distributed.DistributedSampler(val_set, shuffle=False, num_replicas=args.world_size, rank=args.rank)
+    if args.deterministic:
+        test_loader = data.DataLoader(
+            test_set,
+            batch_size=args.batch_size,
+            shuffle=False,
+            num_workers=args.workers,
+            pin_memory=True)#, 
+            # sampler=test_sampler,
+            # worker_init_fn=seed_worker,
+            # generator=g)
+    else:
+        test_loader = data.DataLoader(
+            test_set,
+            batch_size=args.batch_size,
+            shuffle=False,
+            num_workers=args.workers,
+            pin_memory=True)#, 
+            # sampler=test_sampler)
 
     # Get encoding between pixel valus in label images and RGB colors
     class_encoding = train_set.color_encoding
@@ -124,7 +220,7 @@ def load_dataset(dataset):
         class_weights = None
 
     if class_weights is not None:
-        class_weights = torch.from_numpy(class_weights).float().to(device)
+        class_weights = torch.from_numpy(class_weights).float().cuda()
         # Set the weight of the unlabeled class to 0
         if args.ignore_unlabeled:
             ignore_index = list(class_encoding).index('unlabeled')
@@ -133,7 +229,7 @@ def load_dataset(dataset):
     print("Class weights:", class_weights)
 
     return (train_loader, val_loader,
-            test_loader), class_weights, class_encoding
+            test_loader), class_weights, class_encoding, len(train_set)
 
 
 def train(train_loader, val_loader, class_weights, class_encoding):
@@ -142,9 +238,16 @@ def train(train_loader, val_loader, class_weights, class_encoding):
     num_classes = len(class_encoding)
 
     # Intialize ENet
-    model = ENet(num_classes).to(device)
+    model = ENet(num_classes)#.cuda()#to(device)
+    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).cuda()
+    model = torch.nn.parallel.DistributedDataParallel(
+        module=model, broadcast_buffers=True, device_ids=[args.local_rank], output_device=args.local_rank)
+    # model = torch.nn.parallel.DistributedDataParallel(
+        # module=model, broadcast_buffers=False, device_ids=[args.local_rank])
+    model.train()
+
     # Check if the network architecture is correct
-    print(model)
+    # print(model)
 
     # We are going to use the CrossEntropyLoss loss function as it's most
     # frequentely used in classification problems with multiple classes which
@@ -156,10 +259,12 @@ def train(train_loader, val_loader, class_weights, class_encoding):
         model.parameters(),
         lr=args.learning_rate,
         weight_decay=args.weight_decay)
+         
 
     # Learning rate decay scheduler
     lr_updater = lr_scheduler.StepLR(optimizer, args.lr_decay_epochs,
                                      args.lr_decay)
+    warmup_scheduler = WarmUpLR(optimizer, len(train_loader) * args.warmup)
 
     # Evaluation metric
     if args.ignore_unlabeled:
@@ -179,25 +284,32 @@ def train(train_loader, val_loader, class_weights, class_encoding):
         best_miou = 0
 
     # Start Training
-    print()
-    train = Train(model, train_loader, optimizer, criterion, metric, device)
-    val = Test(model, val_loader, criterion, metric, device)
+    train = Train(model, train_loader, optimizer, criterion, metric, args.rank, args.deterministic, args.warmup)
+    val = Test(model, val_loader, criterion, metric, args.rank, args.deterministic)
+    speed_total = []
     for epoch in range(start_epoch, args.epochs):
         print(">>>> [Epoch: {0:d}] Training".format(epoch))
-
-        epoch_loss, (iou, miou) = train.run_epoch(args.print_step)
-        lr_updater.step()
-
-        print(">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | Mean IoU: {2:.4f}".
-              format(epoch, epoch_loss, miou))
-
-        if (epoch + 1) % 10 == 0 or epoch + 1 == args.epochs:
+        time_start = time.time()
+        epoch_loss, (iou, miou) = train.run_epoch(epoch, warmup_scheduler, args.print_step)
+        time_end = time.time()
+        if epoch >= args.warmup:
+            lr_updater.step()
+
+        lrate = optimizer.state_dict()['param_groups'][0]['lr']
+        speed = train_size / (time_end - time_start) / args.world_size
+        speed_total.append(speed)
+        tflops = 5.516*2*3*speed/1000
+        
+        print(">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | LR: {2:.8f} | Mean IoU: {3:.4f} | Speed: {4:.2f} samples/sec | TFlops: {5:.3f} T".
+            format(epoch, epoch_loss, lrate, miou, speed, tflops))
+
+        if (epoch + 1) % 1 == 0 or epoch + 1 == args.epochs:
             print(">>>> [Epoch: {0:d}] Validation".format(epoch))
 
             loss, (iou, miou) = val.run_epoch(args.print_step)
 
-            print(">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | Mean IoU: {2:.4f}".
-                  format(epoch, loss, miou))
+            print(">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | Mean IoU: {2:.4f} | Best IoU: {3:.4f}".
+                  format(epoch, loss, miou, best_miou))
 
             # Print per class IoU on last epoch or if best iou
             if epoch + 1 == args.epochs or miou > best_miou:
@@ -210,7 +322,7 @@ def train(train_loader, val_loader, class_weights, class_encoding):
                 best_miou = miou
                 utils.save_checkpoint(model, optimizer, epoch + 1, best_miou,
                                       args)
-
+    print("Speed Avg. %.2f samples/sec"% np.mean(speed_total))
     return model
 
 
@@ -232,7 +344,7 @@ def test(model, test_loader, class_weights, class_encoding):
     metric = IoU(num_classes, ignore_index=ignore_index)
 
     # Test the trained model on the test set
-    test = Test(model, test_loader, criterion, metric, device)
+    test = Test(model, test_loader, criterion, metric, args.rank)
 
     print(">>>> Running test dataset")
 
@@ -253,7 +365,7 @@ def test(model, test_loader, class_weights, class_encoding):
 
 
 def predict(model, images, class_encoding):
-    images = images.to(device)
+    images = images.cuda() #to(device)
 
     # Make predictions!
     model.eval()
@@ -274,6 +386,14 @@ def predict(model, images, class_encoding):
 
 # Run only if this module is being run directly
 if __name__ == '__main__':
+    
+    host_addr = "127.0.2.1"
+    port = 1235
+    args.world_size = int(os.environ['WORLD_SIZE'])
+    args.rank = int(os.environ['RANK'])
+
+    dist_init(host_addr, args.rank, args.local_rank, args.world_size, port)
+    os.makedirs(args.save_dir, exist_ok=True)
 
     # Fail fast if the dataset directory doesn't exist
     assert os.path.isdir(
@@ -281,9 +401,9 @@ if __name__ == '__main__':
             args.dataset_dir)
 
     # Fail fast if the saving directory doesn't exist
-    assert os.path.isdir(
-        args.save_dir), "The directory \"{0}\" doesn't exist.".format(
-            args.save_dir)
+    # assert os.path.isdir(
+    #     args.save_dir), "The directory \"{0}\" doesn't exist.".format(
+    #         args.save_dir)
 
     # Import the requested dataset
     if args.dataset.lower() == 'camvid':
@@ -295,7 +415,7 @@ if __name__ == '__main__':
         raise RuntimeError("\"{0}\" is not a supported dataset.".format(
             args.dataset))
 
-    loaders, w_class, class_encoding = load_dataset(dataset)
+    loaders, w_class, class_encoding, train_size = load_dataset(dataset)
     train_loader, val_loader, test_loader = loaders
 
     if args.mode.lower() in {'train', 'full'}:
@@ -305,17 +425,28 @@ if __name__ == '__main__':
         if args.mode.lower() == 'test':
             # Intialize a new ENet model
             num_classes = len(class_encoding)
-            model = ENet(num_classes).to(device)
+            model = ENet(num_classes).cuda()#to(device)
 
         # Initialize a optimizer just so we can retrieve the model from the
         # checkpoint
         optimizer = optim.Adam(model.parameters())
 
         # Load the previoulsy saved model state to the ENet model
-        model = utils.load_checkpoint(model, optimizer, args.save_dir,
-                                      args.name)[0]
-
-        if args.mode.lower() == 'test':
-            print(model)
-
-        test(model, test_loader, w_class, class_encoding)
+        # model = utils.load_checkpoint(model, optimizer, args.save_dir,
+                                    #   args.name)[0]
+
+        # if args.mode.lower() == 'test':
+        #     print(model)
+
+        from collections import OrderedDict
+        checkpoint = torch.load(args.model_path)
+        new_state_dict = OrderedDict()
+        for k, v in checkpoint['state_dict'].items():
+            name = k[7:] # module字段在最前面，从第7个字符开始就可以去掉module
+            new_state_dict[name] = v #新字典的key值对应的value一一对应
+        # model.load_state_dict(checkpoint['state_dict'])
+        model.load_state_dict(new_state_dict)
+        epoch = checkpoint['epoch']
+        miou = checkpoint['miou']
+        # print("#"*20, "\n", epoch, miou)
+        test(model, val_loader, w_class, class_encoding)
diff --git a/metric/__init__.py b/metric/__init__.py
old mode 100644
new mode 100755
diff --git a/metric/confusionmatrix.py b/metric/confusionmatrix.py
old mode 100644
new mode 100755
diff --git a/metric/iou.py b/metric/iou.py
old mode 100644
new mode 100755
diff --git a/metric/metric.py b/metric/metric.py
old mode 100644
new mode 100755
diff --git a/models/enet.py b/models/enet.py
old mode 100644
new mode 100755
diff --git a/save/ENet_CamVid/ENet b/save/ENet_CamVid/ENet
old mode 100644
new mode 100755
diff --git a/save/ENet_CamVid/ENet_summary.txt b/save/ENet_CamVid/ENet_summary.txt
old mode 100644
new mode 100755
diff --git a/save/ENet_Cityscapes/ENet b/save/ENet_Cityscapes/ENet
old mode 100644
new mode 100755
diff --git a/save/ENet_Cityscapes/ENet_summary.txt b/save/ENet_Cityscapes/ENet_summary.txt
old mode 100644
new mode 100755
diff --git a/save/README.md b/save/README.md
old mode 100644
new mode 100755
diff --git a/test.py b/test.py
old mode 100644
new mode 100755
index d5bc443..48749d8
--- a/test.py
+++ b/test.py
@@ -16,12 +16,13 @@ class Test:
 
     """
 
-    def __init__(self, model, data_loader, criterion, metric, device):
+    def __init__(self, model, data_loader, criterion, metric, device, deterministic=True):
         self.model = model
         self.data_loader = data_loader
         self.criterion = criterion
         self.metric = metric
         self.device = device
+        self.deterministic = deterministic
 
     def run_epoch(self, iteration_loss=False):
         """Runs an epoch of validation.
@@ -46,7 +47,14 @@ class Test:
                 outputs = self.model(inputs)
 
                 # Loss computation
-                loss = self.criterion(outputs, labels)
+                if self.deterministic:
+                    torch.use_deterministic_algorithms(False)
+                    loss = self.criterion(outputs, labels)
+                    torch.use_deterministic_algorithms(True)
+                    # loss = self.criterion.cpu()(outputs.cpu(), labels.cpu()).cuda()
+                else:
+                    loss = self.criterion(outputs, labels)
+
 
             # Keep track of loss for current epoch
             epoch_loss += loss.item()
diff --git a/train.py b/train.py
old mode 100644
new mode 100755
index c68a6fc..cb97bd7
--- a/train.py
+++ b/train.py
@@ -1,3 +1,5 @@
+import torch
+
 class Train:
     """Performs the training of ``model`` given a training dataset data
     loader, the optimizer, and the loss criterion.
@@ -14,15 +16,17 @@ class Train:
 
     """
 
-    def __init__(self, model, data_loader, optim, criterion, metric, device):
+    def __init__(self, model, data_loader, optim, criterion, metric, device, deterministic, warmup):
         self.model = model
         self.data_loader = data_loader
         self.optim = optim
         self.criterion = criterion
         self.metric = metric
         self.device = device
+        self.deterministic = deterministic
+        self.warmup = warmup
 
-    def run_epoch(self, iteration_loss=False):
+    def run_epoch(self, epoch, warmup_scheduler, iteration_loss=False):
         """Runs an epoch of training.
 
         Keyword arguments:
@@ -44,12 +48,20 @@ class Train:
             outputs = self.model(inputs)
 
             # Loss computation
-            loss = self.criterion(outputs, labels)
+            if self.deterministic:
+                torch.use_deterministic_algorithms(False)
+                loss = self.criterion(outputs, labels)
+                torch.use_deterministic_algorithms(True)
+                # loss = self.criterion.cpu()(outputs.cpu(), labels.cpu()).cuda()
+            else:
+                loss = self.criterion(outputs, labels)
 
             # Backpropagation
             self.optim.zero_grad()
             loss.backward()
             self.optim.step()
+            if epoch < self.warmup:
+                warmup_scheduler.step()
 
             # Keep track of loss for current epoch
             epoch_loss += loss.item()
diff --git a/transforms.py b/transforms.py
old mode 100644
new mode 100755
diff --git a/utils.py b/utils.py
old mode 100644
new mode 100755
index 300c35c..4966c12
--- a/utils.py
+++ b/utils.py
@@ -66,7 +66,7 @@ def save_checkpoint(model, optimizer, epoch, miou, args):
         save_dir), "The directory \"{0}\" doesn't exist.".format(save_dir)
 
     # Save model
-    model_path = os.path.join(save_dir, name)
+    model_path = os.path.join(save_dir, name+"_"+str(epoch)+"_"+str(miou))
     checkpoint = {
         'epoch': epoch,
         'miou': miou,
@@ -89,7 +89,7 @@ def save_checkpoint(model, optimizer, epoch, miou, args):
         summary_file.write("Mean IoU: {0}\n". format(miou))
 
 
-def load_checkpoint(model, optimizer, folder_dir, filename):
+def load_checkpoint(model, optimizer, folder_dir, filename, model_path):
     """Saves the model in a specified directory with a specified name.save
 
     Keyword arguments:
@@ -110,13 +110,19 @@ def load_checkpoint(model, optimizer, folder_dir, filename):
         folder_dir), "The directory \"{0}\" doesn't exist.".format(folder_dir)
 
     # Create folder to save model and information
-    model_path = os.path.join(folder_dir, filename)
+    # model_path = os.path.join(folder_dir, filename)
     assert os.path.isfile(
         model_path), "The model file \"{0}\" doesn't exist.".format(filename)
-
+    
+    from collections import OrderedDict
     # Load the stored model parameters to the model instance
-    checkpoint = torch.load(model_path)
-    model.load_state_dict(checkpoint['state_dict'])
+    checkpoint = torch.load(model_path)#, map_location=torch.device('cuda:0'))
+    new_state_dict = OrderedDict()
+    for k, v in checkpoint['state_dict'].items():
+        name = k[7:] # module字段在最前面，从第7个字符开始就可以去掉module
+        new_state_dict[name] = v #新字典的key值对应的value一一对应
+    # model.load_state_dict(checkpoint['state_dict'])
+    model.load_state_dict(new_state_dict)
     optimizer.load_state_dict(checkpoint['optimizer'])
     epoch = checkpoint['epoch']
     miou = checkpoint['miou']
