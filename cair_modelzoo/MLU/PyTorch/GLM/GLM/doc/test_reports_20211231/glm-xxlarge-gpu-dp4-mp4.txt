Warning: Permanently added '10.0.2.15' (ECDSA) to the list of known hosts.
[2021-12-31 18:36:06,007] [INFO] [runner.py:291:main] Using IP address of 10.0.2.15 for node 10.0.2.15
[2021-12-31 18:36:06,008] [INFO] [multinode_runner.py:51:get_cmd] Running on the following workers: 10.0.2.15,10.0.2.16
[2021-12-31 18:36:06,008] [INFO] [runner.py:360:main] cmd = pdsh -f 1024 -w 10.0.2.15,10.0.2.16 export NCCL_VERSION=2.8.4; export NCCL_SOCKET_IFNAME=eth0; export NCCL_NET_GDR_LEVEL=2; export NCCL_DEBUG=info; export OMPI_MCA_pml=^ucx; export PYTHONIOENCODING=utf-8; export LD_LIBRARY_PATH=/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64; export NCCL_IB_DISABLE=0; export PATH=/opt/conda/bin:/usr/local/nvm/versions/node/v15.2.1/bin:/opt/conda/bin:/opt/conda/bin:/opt/cmake-3.14.6-Linux-x86_64/bin/:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin:/opt/tensorrt/bin; export OMPI_MCA_btl_tcp_if_include=eth0; export PYTHONPATH=/home/zhangchi1/glm_acc;  cd /home/zhangchi1/glm_acc; /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyIxMC4wLjIuMTUiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN10sICIxMC4wLjIuMTYiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --node_rank=%n --master_addr=10.0.2.15 --master_port=40991 pretrain_glm.py --block-lm --task-mask --bert-prob '0.5' --gap-sentence-prob '0.3' --avg-block-length '3' --gpt-min-ratio '0.25' --block-mask-prob '0.1' --short-seq-prob '0.02' --experiment-name 'blocklm-10b' --model-parallel-size '4' --num-layers '48' --hidden-size '4096' --num-attention-heads '64' --seq-length '512' --max-position-embeddings '1024' --train-iters '1000' --log-interval '1' --train-data 'pile' --filter-english --loader-scatter '2' --tokenizer-type 'GPT2BPETokenizer' --split '1000,0,0' --distributed-backend 'nccl' --lr-decay-style 'cosine' --lr-decay-ratio '0.1' --lr-decay-iters '175000' --warmup '0.04' --fp16 --block-lm --task-mask --bert-prob '0.5' --gap-sentence-prob '0.3' --avg-block-length '3' --gpt-min-ratio '0.25' --block-mask-prob '0.1' --short-seq-prob '0.02' --experiment-name 'blocklm-10b' --model-parallel-size '4' --num-layers '48' --hidden-size '4096' --num-attention-heads '64' --seq-length '512' --max-position-embeddings '1024' --train-iters '1000' --log-interval '1' --train-data 'pile' --filter-english --loader-scatter '2' --tokenizer-type 'GPT2BPETokenizer' --split '1000,0,0' --distributed-backend 'nccl' --lr-decay-style 'cosine' --lr-decay-ratio '0.1' --lr-decay-iters '175000' --warmup '0.04' --fp16 --checkpoint-activations --deepspeed-activation-checkpointing --deepspeed --deepspeed_config '/home/zhangchi1/glm_acc/config/config_block_10B_acc_ac1.json'
10.0.2.16: Warning: Permanently added '10.0.2.16' (ECDSA) to the list of known hosts.
10.0.2.15: Warning: Permanently added '10.0.2.15' (ECDSA) to the list of known hosts.
10.0.2.16: [2021-12-31 18:36:07,048] [INFO] [launch.py:73:main] 1 NCCL_VERSION 2.8.4
10.0.2.16: [2021-12-31 18:36:07,048] [INFO] [launch.py:73:main] 1 NCCL_SOCKET_IFNAME eth0
10.0.2.16: [2021-12-31 18:36:07,048] [INFO] [launch.py:73:main] 1 NCCL_NET_GDR_LEVEL 2
10.0.2.16: [2021-12-31 18:36:07,048] [INFO] [launch.py:73:main] 1 NCCL_DEBUG info
10.0.2.16: [2021-12-31 18:36:07,048] [INFO] [launch.py:73:main] 1 NCCL_IB_DISABLE 0
10.0.2.16: [2021-12-31 18:36:07,048] [INFO] [launch.py:80:main] WORLD INFO DICT: {'10.0.2.15': [0, 1, 2, 3, 4, 5, 6, 7], '10.0.2.16': [0, 1, 2, 3, 4, 5, 6, 7]}
10.0.2.16: [2021-12-31 18:36:07,048] [INFO] [launch.py:86:main] nnodes=2, num_local_procs=8, node_rank=1
10.0.2.16: [2021-12-31 18:36:07,048] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'10.0.2.15': [0, 1, 2, 3, 4, 5, 6, 7], '10.0.2.16': [8, 9, 10, 11, 12, 13, 14, 15]})
10.0.2.16: [2021-12-31 18:36:07,048] [INFO] [launch.py:102:main] dist_world_size=16
10.0.2.16: [2021-12-31 18:36:07,048] [INFO] [launch.py:106:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
10.0.2.15: [2021-12-31 18:36:07,078] [INFO] [launch.py:73:main] 0 NCCL_VERSION 2.8.4
10.0.2.15: [2021-12-31 18:36:07,078] [INFO] [launch.py:73:main] 0 NCCL_SOCKET_IFNAME eth0
10.0.2.15: [2021-12-31 18:36:07,078] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL 2
10.0.2.15: [2021-12-31 18:36:07,078] [INFO] [launch.py:73:main] 0 NCCL_DEBUG info
10.0.2.15: [2021-12-31 18:36:07,078] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE 0
10.0.2.15: [2021-12-31 18:36:07,078] [INFO] [launch.py:80:main] WORLD INFO DICT: {'10.0.2.15': [0, 1, 2, 3, 4, 5, 6, 7], '10.0.2.16': [0, 1, 2, 3, 4, 5, 6, 7]}
10.0.2.15: [2021-12-31 18:36:07,078] [INFO] [launch.py:86:main] nnodes=2, num_local_procs=8, node_rank=0
10.0.2.15: [2021-12-31 18:36:07,078] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'10.0.2.15': [0, 1, 2, 3, 4, 5, 6, 7], '10.0.2.16': [8, 9, 10, 11, 12, 13, 14, 15]})
10.0.2.15: [2021-12-31 18:36:07,078] [INFO] [launch.py:102:main] dist_world_size=16
10.0.2.15: [2021-12-31 18:36:07,078] [INFO] [launch.py:106:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
10.0.2.15: using world size: 16 and model-parallel size: 4 
10.0.2.15:  > using dynamic loss scaling
10.0.2.15: > initializing model parallel with size 4
10.0.2.15: [2021-12-31 18:36:17,140] [INFO] [checkpointing.py:734:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
10.0.2.15: [2021-12-31 18:36:17,140] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
10.0.2.15: > padded vocab (size: 50266) with 38 dummy tokens (new size: 50304)
10.0.2.15: > found end-of-document token: 50256
10.0.2.16: 94f182076445:139195:139195 [4] NCCL INFO Bootstrap : Using eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139195:139195 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.16: 94f182076445:139195:139195 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.16: 94f182076445:139195:139195 [4] NCCL INFO NET/IB : No device found.
10.0.2.16: 94f182076445:139195:139195 [4] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139195:139195 [4] NCCL INFO Using network Socket
10.0.2.16: NCCL version 2.8.4+cuda11.2
10.0.2.16: 94f182076445:139191:139191 [0] NCCL INFO Bootstrap : Using eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139191:139191 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.16: 94f182076445:139191:139191 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.16: 94f182076445:139191:139191 [0] NCCL INFO NET/IB : No device found.
10.0.2.16: 94f182076445:139191:139191 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139191:139191 [0] NCCL INFO Using network Socket
10.0.2.16: NCCL version 2.8.4+cuda11.2
10.0.2.16: 94f182076445:139198:139198 [7] NCCL INFO Bootstrap : Using eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139196:139196 [5] NCCL INFO Bootstrap : Using eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139197:139197 [6] NCCL INFO Bootstrap : Using eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139198:139198 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.16: 94f182076445:139197:139197 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.16: 94f182076445:139196:139196 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.16: 94f182076445:139198:139198 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.16: 94f182076445:139197:139197 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.16: 94f182076445:139196:139196 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.16: 94f182076445:139198:139198 [7] NCCL INFO NET/IB : No device found.
10.0.2.16: 94f182076445:139197:139197 [6] NCCL INFO NET/IB : No device found.
10.0.2.16: 94f182076445:139196:139196 [5] NCCL INFO NET/IB : No device found.
10.0.2.16: 94f182076445:139198:139198 [7] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139198:139198 [7] NCCL INFO Using network Socket
10.0.2.16: 94f182076445:139197:139197 [6] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139197:139197 [6] NCCL INFO Using network Socket
10.0.2.16: 94f182076445:139196:139196 [5] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139196:139196 [5] NCCL INFO Using network Socket
10.0.2.15: 2be7fa6883db:53333:53333 [4] NCCL INFO Bootstrap : Using eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53333:53333 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.15: 2be7fa6883db:53333:53333 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.15: 2be7fa6883db:53333:53333 [4] NCCL INFO NET/IB : No device found.
10.0.2.15: 2be7fa6883db:53333:53333 [4] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53333:53333 [4] NCCL INFO Using network Socket
10.0.2.15: NCCL version 2.8.4+cuda11.2
10.0.2.16: 94f182076445:139192:139192 [1] NCCL INFO Bootstrap : Using eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139193:139193 [2] NCCL INFO Bootstrap : Using eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139194:139194 [3] NCCL INFO Bootstrap : Using eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139193:139193 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.16: 94f182076445:139194:139194 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.16: 94f182076445:139192:139192 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.16: 94f182076445:139193:139193 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.16: 94f182076445:139194:139194 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.16: 94f182076445:139192:139192 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.16: 94f182076445:139192:139192 [1] NCCL INFO NET/IB : No device found.
10.0.2.16: 94f182076445:139194:139194 [3] NCCL INFO NET/IB : No device found.
10.0.2.16: 94f182076445:139193:139193 [2] NCCL INFO NET/IB : No device found.
10.0.2.16: 94f182076445:139192:139192 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139194:139194 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139193:139193 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.16<0>
10.0.2.16: 94f182076445:139192:139192 [1] NCCL INFO Using network Socket
10.0.2.16: 94f182076445:139194:139194 [3] NCCL INFO Using network Socket
10.0.2.16: 94f182076445:139193:139193 [2] NCCL INFO Using network Socket
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139476 [6] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139475 [5] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139470 [4] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139477 [7] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139476 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139475 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139470 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139477 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139476 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139475 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139470 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139477 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139476 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139475 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139470 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139477 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139476 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139475 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139470 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139477 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO Channel 00/02 :    0   1   2   3
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO Channel 01/02 :    0   1   2   3
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO Setting affinity for GPU 5 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO Setting affinity for GPU 6 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO Setting affinity for GPU 7 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO Setting affinity for GPU 4 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO Channel 00 : 0[3d000] -> 1[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO Channel 00 : 1[3e000] -> 2[40000] via P2P/IPC
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO Channel 00 : 3[41000] -> 0[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO Channel 00 : 2[40000] -> 3[41000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO Channel 01 : 0[3d000] -> 1[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO Channel 01 : 1[3e000] -> 2[40000] via P2P/IPC
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO Channel 01 : 3[41000] -> 0[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO Channel 01 : 2[40000] -> 3[41000] via P2P/IPC
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO Channel 00 : 1[3e000] -> 0[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO Channel 00 : 2[40000] -> 1[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO Channel 01 : 1[3e000] -> 0[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO Channel 01 : 2[40000] -> 1[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO Channel 00 : 3[41000] -> 2[40000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO Channel 01 : 3[41000] -> 2[40000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.16: 94f182076445:139195:139470 [4] NCCL INFO comm 0x7fcf88008e10 rank 0 nranks 4 cudaDev 4 busId 3d000 - Init COMPLETE
10.0.2.16: 94f182076445:139195:139195 [4] NCCL INFO Launch mode Parallel
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139196:139475 [5] NCCL INFO comm 0x7fb33c008e10 rank 1 nranks 4 cudaDev 5 busId 3e000 - Init COMPLETE
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.16: 94f182076445:139198:139477 [7] NCCL INFO comm 0x7ff0a4008e10 rank 3 nranks 4 cudaDev 7 busId 41000 - Init COMPLETE
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.16: 94f182076445:139197:139476 [6] NCCL INFO comm 0x7f3eec008e10 rank 2 nranks 4 cudaDev 6 busId 40000 - Init COMPLETE
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139488 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139486 [3] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139487 [2] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139474 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.16: Rank 12 is using scatter from /algo/datasets_training/pile/train_00/.scatter/1
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139488 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139486 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139487 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139474 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139488 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139486 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139487 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139474 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139488 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139486 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139487 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139474 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139488 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139486 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139487 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139474 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO Channel 00/02 :    0   1   2   3
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO Channel 01/02 :    0   1   2   3
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO Channel 00 : 1[1b000] -> 2[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53335:53335 [6] NCCL INFO Bootstrap : Using eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53335:53335 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO Channel 01 : 1[1b000] -> 2[1d000] via P2P/IPC
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO Channel 00 : 2[1d000] -> 3[1e000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO Channel 00 : 3[1e000] -> 0[1a000] via P2P/IPC
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO Channel 00 : 0[1a000] -> 1[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53337:53337 [7] NCCL INFO Bootstrap : Using eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53337:53337 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.15: 2be7fa6883db:53335:53335 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO Channel 01 : 2[1d000] -> 3[1e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53335:53335 [6] NCCL INFO NET/IB : No device found.
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO Channel 01 : 3[1e000] -> 0[1a000] via P2P/IPC
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO Channel 01 : 0[1a000] -> 1[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53335:53335 [6] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53334:53334 [5] NCCL INFO Bootstrap : Using eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53335:53335 [6] NCCL INFO Using network Socket
10.0.2.15: 2be7fa6883db:53334:53334 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.15: 2be7fa6883db:53337:53337 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.15: 2be7fa6883db:53337:53337 [7] NCCL INFO NET/IB : No device found.
10.0.2.15: 2be7fa6883db:53337:53337 [7] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53337:53337 [7] NCCL INFO Using network Socket
10.0.2.15: 2be7fa6883db:53334:53334 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.15: 2be7fa6883db:53334:53334 [5] NCCL INFO NET/IB : No device found.
10.0.2.15: 2be7fa6883db:53334:53334 [5] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53334:53334 [5] NCCL INFO Using network Socket
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53329:53329 [0] NCCL INFO Bootstrap : Using eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53329:53329 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO Channel 00 : 2[1d000] -> 1[1b000] via P2P/IPC
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO Channel 01 : 2[1d000] -> 1[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53329:53329 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.15: 2be7fa6883db:53329:53329 [0] NCCL INFO NET/IB : No device found.
10.0.2.15: 2be7fa6883db:53329:53329 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53329:53329 [0] NCCL INFO Using network Socket
10.0.2.15: NCCL version 2.8.4+cuda11.2
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO Channel 00 : 3[1e000] -> 2[1d000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO Channel 01 : 3[1e000] -> 2[1d000] via P2P/IPC
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO Channel 00 : 1[1b000] -> 0[1a000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO Channel 01 : 1[1b000] -> 0[1a000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139486 [3] NCCL INFO comm 0x7f68b4008e10 rank 3 nranks 4 cudaDev 3 busId 1e000 - Init COMPLETE
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.16: 94f182076445:139191:139474 [0] NCCL INFO comm 0x7f93b0008e10 rank 0 nranks 4 cudaDev 0 busId 1a000 - Init COMPLETE
10.0.2.16: 94f182076445:139191:139191 [0] NCCL INFO Launch mode Parallel
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.16: 94f182076445:139193:139487 [2] NCCL INFO comm 0x7ff51c008e10 rank 2 nranks 4 cudaDev 2 busId 1d000 - Init COMPLETE
10.0.2.16: 94f182076445:139192:139488 [1] NCCL INFO comm 0x7f7144008e10 rank 1 nranks 4 cudaDev 1 busId 1b000 - Init COMPLETE
10.0.2.16: Rank 8 is using scatter from /algo/datasets_training/pile/train_00/.scatter/0
10.0.2.15: 2be7fa6883db:53331:53331 [2] NCCL INFO Bootstrap : Using eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53331:53331 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.15: 2be7fa6883db:53332:53332 [3] NCCL INFO Bootstrap : Using eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53330:53330 [1] NCCL INFO Bootstrap : Using eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53332:53332 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.15: 2be7fa6883db:53330:53330 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
10.0.2.15: 2be7fa6883db:53331:53331 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.15: 2be7fa6883db:53331:53331 [2] NCCL INFO NET/IB : No device found.
10.0.2.15: 2be7fa6883db:53332:53332 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.15: 2be7fa6883db:53330:53330 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
10.0.2.15: 2be7fa6883db:53331:53331 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53331:53331 [2] NCCL INFO Using network Socket
10.0.2.15: 2be7fa6883db:53332:53332 [3] NCCL INFO NET/IB : No device found.
10.0.2.15: 2be7fa6883db:53330:53330 [1] NCCL INFO NET/IB : No device found.
10.0.2.15: 2be7fa6883db:53332:53332 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53330:53330 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.2.15<0>
10.0.2.15: 2be7fa6883db:53332:53332 [3] NCCL INFO Using network Socket
10.0.2.15: 2be7fa6883db:53330:53330 [1] NCCL INFO Using network Socket
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53436 [6] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53439 [5] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53434 [4] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53438 [7] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53439 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53436 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53434 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53438 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53439 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53436 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53434 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53438 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53439 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53436 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53434 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53438 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53439 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53436 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53434 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53438 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO Channel 00/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO Channel 01/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO Setting affinity for GPU 7 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO Setting affinity for GPU 5 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO Setting affinity for GPU 6 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO Setting affinity for GPU 4 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO Channel 00 : 2[40000] -> 3[41000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO Channel 00 : 3[41000] -> 0[3d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO Channel 00 : 0[3d000] -> 1[3e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO Channel 01 : 2[40000] -> 3[41000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO Channel 00 : 1[3e000] -> 2[40000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO Channel 01 : 0[3d000] -> 1[3e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO Channel 01 : 3[41000] -> 0[3d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO Channel 01 : 1[3e000] -> 2[40000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO Channel 00 : 1[3e000] -> 0[3d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO Channel 01 : 1[3e000] -> 0[3d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO Channel 00 : 3[41000] -> 2[40000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO Channel 01 : 3[41000] -> 2[40000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.15: 2be7fa6883db:53333:53434 [4] NCCL INFO comm 0x7f8a50008e10 rank 0 nranks 4 cudaDev 4 busId 3d000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53333:53333 [4] NCCL INFO Launch mode Parallel
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO Channel 00 : 2[40000] -> 1[3e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO Channel 01 : 2[40000] -> 1[3e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.15: 2be7fa6883db:53337:53438 [7] NCCL INFO comm 0x7f6e90008e10 rank 3 nranks 4 cudaDev 7 busId 41000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.15: 2be7fa6883db:53335:53436 [6] NCCL INFO comm 0x7f1290008e10 rank 2 nranks 4 cudaDev 6 busId 40000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53334:53439 [5] NCCL INFO comm 0x7f8d90008e10 rank 1 nranks 4 cudaDev 5 busId 3e000 - Init COMPLETE
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53445 [3] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53444 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53447 [2] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53437 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53445 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53447 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53444 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53437 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53445 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53447 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53444 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53437 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53445 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53447 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53444 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53437 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53445 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53447 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53444 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53437 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: Rank 4 is using scatter from /algo/datasets_training/pile/train_00/.scatter/1
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO Channel 00/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO Channel 01/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO Setting affinity for GPU 1 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO Setting affinity for GPU 2 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO Setting affinity for GPU 3 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO Setting affinity for GPU 0 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO Channel 00 : 3[1e000] -> 0[1a000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO Channel 00 : 1[1b000] -> 2[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO Channel 00 : 2[1d000] -> 3[1e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO Channel 00 : 0[1a000] -> 1[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO Channel 01 : 3[1e000] -> 0[1a000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO Channel 01 : 1[1b000] -> 2[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO Channel 01 : 2[1d000] -> 3[1e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO Channel 01 : 0[1a000] -> 1[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO Channel 00 : 3[1e000] -> 2[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO Channel 01 : 3[1e000] -> 2[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO Channel 00 : 2[1d000] -> 1[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO Channel 01 : 2[1d000] -> 1[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO Channel 00 : 1[1b000] -> 0[1a000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO Channel 01 : 1[1b000] -> 0[1a000] via P2P/IPC
10.0.2.16: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.15: 2be7fa6883db:53332:53445 [3] NCCL INFO comm 0x7fca28008e10 rank 3 nranks 4 cudaDev 3 busId 1e000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.15: 2be7fa6883db:53329:53437 [0] NCCL INFO comm 0x7f46fc008e10 rank 0 nranks 4 cudaDev 0 busId 1a000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53329:53329 [0] NCCL INFO Launch mode Parallel
10.0.2.15: configuring data
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
10.0.2.15: 2be7fa6883db:53331:53447 [2] NCCL INFO comm 0x7f2b34008e10 rank 2 nranks 4 cudaDev 2 busId 1d000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53330:53444 [1] NCCL INFO comm 0x7f0a94008e10 rank 1 nranks 4 cudaDev 1 busId 1b000 - Init COMPLETE
10.0.2.15: Rank 0 is using scatter from /algo/datasets_training/pile/train_00/.scatter/0
10.0.2.16: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.15: Create dataset pile at scatter 1 with 2509149 documents
10.0.2.15: [464, 285, 20836, 30060, 318, 531, 284, 423, 262, 749, 3716, 2951, 287, 477, 286, 262, 5044, 13239, 357, 8201, 5384, 737, 2312, 6156, 20048, 558, 504, 460, 1445, 511, 10618, 276, 2951, 14799, 290, 1123, 318, 6007, 286, 6795, 11202, 477, 319, 663, 898, 13, 12032, 11, 262, 285, 20836, 30060, 338, 2951, 3994, 517, 621, 1936, 1661, 262, 1271, 286, 3124, 20099, 355, 262, 1692, 4151, 11, 3616, 484, 460, 766, 7577, 326, 389, 11071, 984, 856, 284, 345, 290, 502, 11, 1390, 1811, 11760, 286, 49961, 1657, 13, 775, 760, 777, 3858, 286, 7577, 2152, 780, 356, 423, 2041, 12834, 326, 1249, 514, 284, 6842, 4973, 284, 606, 11, 611, 691, 832, 1366, 1100, 5269, 393, 584, 5874, 12531, 507, 13, 887, 284, 262, 285, 20836, 30060, 11, 49961, 1657, 318, 2147, 2041, 26, 340, 338, 655, 281, 2938, 39144, 286, 262, 5874, 995, 588, 597, 584, 37409, 13, 198, 198, 22697, 3478, 14548, 354, 1595, 470, 423, 285, 20836, 30060, 2951, 393, 597, 1611, 286, 47262, 7883, 11, 475, 287, 465, 983, 337, 15702, 461, 495, 339, 857, 530, 1365, 960, 258, 3607, 1938, 262, 1176, 284, 38138, 262, 5544, 15793, 13, 383, 976, 835, 326, 257, 13617, 396, 15085, 28372, 12036, 11455, 10209, 257, 835, 286, 13456, 257, 1115, 12, 19577, 18560, 319, 257, 6228, 6614, 11, 3478, 14548, 354, 15314, 284, 36717, 1440, 12, 19577, 24314, 656, 1115, 12, 19577, 9029, 357, 33469, 2884, 734, 12, 19577, 3644, 5671, 737, 1629, 717, 16086, 11, 337, 15702, 461, 495, 1595, 470, 1656, 517, 8253, 621, 597, 584, 318, 16996, 15027, 983, 13, 921, 1445, 257, 2095, 1088, 257, 1402, 6616, 286, 1956, 12462, 319, 257, 6614, 2029, 257, 8925, 4469, 39328, 13, 921, 460, 4391, 290, 4574, 1243, 284, 38138, 262, 1976, 268, 11376, 12, 2339, 2272, 11, 475, 6165, 345, 1183, 1057, 656, 257, 2636, 886, 810, 262, 691, 3038, 481, 307, 38869, 530, 15793, 329, 1194, 13, 1629, 1752, 534, 9037, 6616, 17488, 82, 656, 281, 39696, 515, 35991, 11, 290, 644, 373, 257, 7815, 3355, 468, 587, 6928, 416, 257, 6228, 11, 44039, 4417, 13, 632, 338, 257, 1182, 12, 2777, 23062, 2218, 11, 925, 1744, 832, 257, 35981, 286, 17290, 4213, 290, 18069, 3357, 960, 272, 16246, 326, 338, 379, 262, 2612, 286, 477, 36342, 1047, 11, 475, 10448, 994, 416, 3478, 14548, 354, 351, 257, 36304, 4437, 326, 5621, 340, 5475, 13, 198, 198, 1639, 1244, 307, 6655, 284, 760, 326, 314, 7836, 13067, 3478, 14548, 354, 318, 36391, 284, 257, 3218, 1115, 12, 19577, 3668, 12, 7784, 6224, 655, 588, 2506, 2073, 13, 554, 1109, 11, 996, 612, 338, 884, 3716, 3341, 379, 670, 287, 3478, 14548, 354, 338, 983, 11, 262, 835, 339, 9209, 546, 340, 318, 6454, 1729, 38009, 415, 13, 366, 40, 550, 284, 8067, 617, 649, 3858, 286, 10688, 329, 428, 553, 339, 22591, 503, 355, 636, 286, 257, 2392, 2882, 13, 9425, 11, 339, 655, 15646, 617, 649, 10688, 13, 1400, 1263, 1730, 11, 826, 30, 7486, 3478, 14548, 354, 468, 30166, 2354, 670, 329, 21128, 11, 11034, 11, 290, 2647, 11, 5922, 337, 15702, 461, 495, 318, 5688, 257, 12199, 14669, 13, 679, 12188, 465, 1440, 12, 19577, 11621, 319, 257, 2829, 13224, 706, 17548, 278, 606, 503, 351, 4096, 21613, 290, 3348, 13, 198, 198, 40, 1965, 3478, 14548, 354, 611, 465, 604, 35, 7166, 995, 1683, 4940, 698, 14924, 351, 262, 1103, 530, 11, 284, 543, 339, 39889, 11, 366, 3673, 1107, 526, 679, 1816, 319, 13, 366, 1212, 1429, 286, 2263, 262, 995, 356, 760, 290, 2111, 284, 787, 340, 4795, 286, 15793, 318, 1682, 845, 19128, 3101, 11, 475, 407, 287, 257, 705, 10919, 611, 428, 2119, 373, 604, 35, 6, 2565, 13, 3125, 588, 11, 705, 5812, 11, 314, 1833, 644, 6225, 281, 2134, 1724, 7692, 286, 703, 867, 15225, 340, 468, 11496, 632, 338, 281, 15962, 835, 286, 4547, 257, 2829, 2223, 845, 7744, 11, 281, 39676, 326, 2331, 47938, 656, 337, 15702, 461, 495, 338, 1486, 290, 3478, 14548, 354, 338, 7325, 1429, 13, 198, 198, 24893, 14548, 354, 468, 587, 1762, 319, 337, 15702, 461, 495, 329, 625, 3598, 812, 783, 11, 290, 340, 338, 281, 16441, 278, 1628, 329, 257, 530, 12, 6259, 1074, 11, 475, 339, 338, 8161, 284, 2948, 262, 983, 422, 5033, 1165, 881, 286, 257, 2116, 12, 634, 12907, 13, 366, 40, 1101, 262, 530, 508, 338, 17728, 685, 1169, 983, 60, 284, 345, 11, 523, 4084, 314, 1101, 257, 636, 286, 262, 1429, 11, 475, 314, 1101, 2111, 284, 22837, 3589, 422, 262, 3721, 780, 340, 338, 4950, 685, 261, 663, 898, 4357, 290, 314, 836, 470, 761, 284, 307, 287, 612, 526, 554, 1109, 11, 339, 1549, 2138, 407, 27861, 262, 3616, 2157, 262, 983, 379, 477, 11, 18877, 2683, 351, 517, 2683, 588, 11, 366, 2061, 611, 262, 6881, 550, 477, 777, 1243, 326, 345, 1422, 470, 760, 1701, 290, 366, 2061, 611, 340, 338, 691, 17728, 345, 351, 257, 2176, 636, 286, 340, 11, 290, 379, 617, 966, 345, 714, 6451, 766, 644, 373, 1464, 7104, 284, 345, 1701, 198, 198, 1169, 1242, 286, 2615, 257, 2272, 284, 262, 966, 326, 340, 338, 2048, 21546, 198, 198, 1, 44, 15702, 461, 495, 1, 23677, 284, 366, 24717, 290, 7766, 553, 290, 318, 973, 287, 4941, 284, 4960, 25476, 13, 632, 45370, 262, 3721, 326, 284, 4988, 1833, 290, 9144, 262, 11376, 11, 530, 1276, 1998, 340, 37318, 11, 31121, 477, 5642, 286, 45136, 3562, 290, 8752, 14963, 19428, 13, 1081, 636, 286, 262, 2478, 286, 337, 15702, 461, 495, 11, 3478, 14548, 354, 925, 257, 5296, 284, 2869, 11, 810, 339, 8672, 617, 286, 262, 1499, 338, 22140, 14312, 25476, 13, 366, 2215, 345, 766, 257, 4960, 11376, 11, 340, 1838, 345, 1254, 4998, 655, 284, 307, 1306, 284, 340, 11, 290, 314, 1422, 470, 1607, 326, 422, 655, 5986, 553, 339, 531, 13, 366, 1026, 338, 262, 1242, 286, 2615, 257, 2272, 284, 262, 966, 326, 340, 338, 2048, 21546, 526, 383, 1976, 268, 11376, 290, 12505, 35431, 286, 337, 15702, 461, 495, 2277, 428, 966, 286, 4941, 11777]
10.0.2.15: b'The mantis shrimp is said to have the most complex eyes in all of the animal kingdom (including humans). These ancient crustaceans can move their segmented eyes independently and each is capable of depth perception all on its own. Additionally, the mantis shrimp\'s eyes contain more than five times the number of color receptors as the human eye, meaning they can see colors that are imperceptible to you and me, including several bands of ultraviolet light. We know these types of colors exist because we have special instruments that allow us to bear witness to them, if only through data readouts or other visual abstractions. But to the mantis shrimp, ultraviolet light is nothing special; it\'s just an expected facet of the visual world like any other hue.\n\nMarc ten Bosch doesn\'t have mantis shrimp eyes or any kind of superhuman abilities, but in his game Miegakure he does one better\xe2\x80\x94he gives players the power to traverse the fourth dimension. The same way that a cubist Picasso painting grants viewers a way of experiencing a three-dimensional portrait on a flat plane, ten Bosch manages to carve four-dimensional slices into three-dimensional spaces (represented via two-dimensional computer monitor). At first glance, Miegakure doesn\'t appear more complicated than any other isometric puzzle game. You move a character around a small square of land floating on a plane above a dynamic background wallpaper. You can jump and push things to traverse the zen garden-like space, but ultimately you\'ll run into a dead end where the only option will be swapping one dimension for another. At once your static square morphs into an elongated rectangle, and what was a stone wall has been replaced by a flat, sandy surface. It\'s a head-spinning feat, made possible through a merging of artistic ideas and mathematical practice\xe2\x80\x94an intersection that\'s at the heart of all videogames, but approached here by ten Bosch with a pioneering spirit that sets it apart.\n\nYou might be surprised to know that IRL Marc ten Bosch is relegated to a regular three-dimensional Earth-bound existence just like everyone else. In fact, though there\'s such complex systems at work in ten Bosch\'s game, the way he speaks about it is somewhat nonchalant. "I had to invent some new types of math for this," he tossed out as part of a longer response. Yeah, he just invented some new math. No big deal, right? Though ten Bosch has enlisted outside work for modeling, animation, and music, developing Miegakure is largely a solo affair. He builds his four-dimensional worlds on a simple laptop after sketching them out with basic pencil and paper.\n\nI asked ten Bosch if his 4D virtual world ever starts blurring with the real one, to which he chuckled, "Not really." He went on. "This process of taking the world we know and trying to make it independent of dimension is actually very enlightening, but not in a \'what if this room was 4D\' sense. More like, \'Oh, I understand what turning an object means regardless of how many dimensions it has.\'" It\'s an elaborate way of understanding a simple action very deeply, an ethos that seems ingrained into Miegakure\'s design and ten Bosch\'s creative process.\n\nTen Bosch has been working on Miegakure for over seven years now, and it\'s an enveloping project for a one-person team, but he\'s careful to prevent the game from becoming too much of a self-portrait. "I\'m the one who\'s presenting [the game] to you, so clearly I\'m a part of the process, but I\'m trying to disconnect myself from the concept because it\'s beautiful [on its own], and I don\'t need to be in there." In fact, he\'d rather not dictate the meaning behind the game at all, answering questions with more questions like, "What if the universe had all these things that you didn\'t know?" and "What if it\'s only presenting you with a specific part of it, and at some point you could suddenly see what was always hidden to you?"\n\nthe art of building a space to the point that it\'s almost therapeutic\n\n"Miegakure" translates to "hide and reveal," and is used in reference to Japanese gardens. It embodies the concept that to truly understand and appreciate the garden, one must experience it firsthand, witnessing all manner of purposefully designed and naturally occurring phenomena. As part of the development of Miegakure, ten Bosch made a trip to Japan, where he visited some of the country\'s renowned imperial gardens. "When you see a Japanese garden, it makes you feel amazing just to be next to it, and I didn\'t expect that from just pictures," he said. "It\'s the art of building a space to the point that it\'s almost therapeutic." The zen garden and temple aesthetics of Miegakure hit this point of reference explicitly'
10.0.2.15: [47798, 198, 198, 26133, 11, 2805, 8487, 11, 2813, 198, 198, 34, 1501, 2473, 290, 43751, 198, 198, 44, 9795, 40, 10863, 1921, 318, 3236, 994, 287, 9281, 5593, 13, 10062, 22555, 736, 17291, 287, 262, 8069, 82, 11, 340, 468, 1716, 257, 1588, 14395, 11, 25142, 5179, 286, 4138, 286, 661, 284, 262, 1468, 8653, 446, 6232, 286, 262, 1748, 13, 770, 18466, 2473, 318, 287, 44827, 286, 262, 1468, 4141, 15012, 286, 262, 1748, 11, 290, 340, 545, 38654, 262, 22861, 1043, 287, 968, 12255, 11, 13340, 11, 543, 318, 33218, 422, 514, 319, 262, 13797, 5866, 13, 198, 198, 7594, 968, 12255, 11, 262, 9281, 5593, 22861, 423, 281, 33390, 2095, 13, 7945, 851, 393, 780, 30, 851, 286, 428, 11, 337, 22490, 1902, 292, 318, 4855, 290, 15901, 416, 262, 1748, 1230, 11, 1957, 10225, 11, 290, 23265, 2628, 13, 632, 338, 922, 329, 19277, 11, 484, 910, 13, 314, 910, 340, 338, 2089, 329, 534, 5848, 13, 198, 198, 1537, 262, 1738, 2157, 40886, 11, 290, 663, 2457, 1110, 11, 337, 22490, 1902, 292, 11, 373, 262, 3799, 590, 286, 262, 6049, 3112, 1817, 286, 43751, 13, 13794, 47675, 1346, 257, 1310, 14395, 318, 1444, 329, 878, 4581, 16571, 1528, 45319, 287, 257, 10326, 11, 543, 318, 257, 640, 329, 262, 3112, 270, 1843, 6529, 286, 11443, 11, 24845, 11, 290, 435, 907, 13992, 13, 887, 262, 9133, 4564, 1392, 5755, 286, 6049, 3112, 1817, 379, 262, 640, 655, 878, 337, 22490, 1902, 292, 2627, 2968, 685, 32, 7353, 4160, 7965, 11243, 268, 9186, 5362, 11, 13258, 3362, 13889, 11, 19322, 4083, 314, 1053, 6810, 867, 1661, 326, 8797, 262, 4564, 3568, 284, 1577, 510, 1223, 326, 318, 41802, 25144, 11, 2130, 2073, 481, 1011, 340, 329, 511, 898, 4959, 13, 198, 198, 1026, 561, 307, 257, 6049, 7457, 284, 892, 326, 262, 6992, 274, 286, 40886, 290, 337, 22490, 1902, 292, 423, 1997, 284, 466, 351, 46802, 11, 2845, 287, 262, 749, 13875, 1843, 290, 6569, 835, 13, 2141, 262, 9890, 364, 12414, 43751, 30, 383, 968, 12255, 40886, 357, 259, 663, 1944, 1296, 8, 373, 2067, 416, 1729, 12, 48919, 3399, 286, 3594, 18598, 13, 383, 29702, 40886, 373, 302, 27718, 287, 262, 8069, 82, 416, 262, 34700, 306, 3098, 12, 48919, 1230, 286, 8031, 11, 290, 262, 15338, 390, 42799, 40886, 373, 2067, 416, 6727, 12, 27171, 1398, 19749, 37681, 351, 39057, 4213, 13, 40886, 11, 2845, 287, 663, 749, 4096, 1296, 286, 257, 10768, 14395, 11, 318, 407, 7835, 13, 198, 198, 4821, 284, 262, 9576, 5416, 13, 9666, 41342, 1962, 42445, 2564, 25, 198, 198, 58, 43, 298, 60, 2644, 318, 262, 4564, 338, 11824, 329, 20111, 11, 290, 373, 32954, 379, 262, 845, 32609, 286, 13624, 13, 3954, 33398, 4453, 2241, 30572, 340, 416, 465, 24845, 16571, 1528, 290, 16571, 12513, 287, 262, 10326, 26, 290, 996, 339, 561, 407, 13551, 340, 319, 262, 995, 416, 281, 4911, 3141, 434, 11, 357, 4758, 11, 788, 11, 714, 407, 423, 587, 1280, 284, 262, 1176, 286, 18905, 341, 35751, 1865, 339, 3751, 30723, 1576, 416, 465, 898, 1672, 11, 326, 376, 9222, 11, 543, 1793, 550, 523, 6777, 6149, 287, 262, 5706, 3854, 11, 373, 284, 307, 635, 1970, 1417, 416, 262, 8990, 286, 262, 968, 13, 198, 198, 464, 3167, 6418, 286, 520, 13, 1757, 262, 21724, 1625, 11, 530, 1110, 11, 284, 5803, 11, 290, 531, 284, 683, 25, 4162, 466, 356, 290, 262, 48141, 786, 274, 3049, 1690, 11, 475, 11906, 3167, 6418, 466, 407, 3049, 30, 843, 5803, 531, 284, 606, 25, 1680, 262, 1751, 286, 262, 26619, 70, 3823, 25722, 11, 355, 890, 355, 262, 26619, 70, 3823, 318, 351, 606, 30, 887, 262, 1528, 481, 1282, 11, 618, 262, 26619, 70, 3823, 2236, 307, 2077, 1497, 422, 606, 11, 290, 788, 484, 2236, 3049, 13, 685, 1273, 45524, 13, 220, 844, 13, 1478, 11, 1314, 4083, 198, 198, 4821, 284, 1962, 42445, 2564, 11, 262, 3799, 590, 286, 43751, 9667, 355, 1290, 736, 355, 19295, 4160, 1661, 11, 543, 318, 262, 845, 3726, 11, 290, 4477, 262, 6593, 1043, 1871, 262, 6771, 878, 1951, 13, 7507, 590, 373, 6198, 2407, 6049, 11, 290, 625, 262, 10675, 3744, 36919, 414, 4120, 13, 6288, 11, 356, 423, 5284, 379, 262, 1877, 966, 11, 810, 1310, 393, 645, 3112, 590, 16014, 318, 6515, 11, 329, 6159, 466, 9316, 466, 407, 766, 597, 8557, 4414, 284, 340, 11, 4249, 389, 356, 23278, 284, 466, 881, 286, 340, 13, 198, 198, 46172, 28697, 42667, 11, 287, 1596, 3901, 11, 2630, 25, 198, 198, 464, 3799, 590, 286, 43751, 318, 262, 845, 23009, 286, 262, 4302, 15611, 13, 2750, 340, 11, 356, 5879, 6731, 407, 284, 307, 5775, 286, 262, 6372, 286, 1951, 13, 2750, 340, 11, 356, 45347, 262, 629, 454, 3212, 286, 11871, 5316, 13, 2750, 340, 11, 356, 4461, 4202, 1028, 262, 42676, 286, 11854, 11, 329, 340, 21132, 514, 351, 32271, 1037, 13, 10358, 18019, 1663, 816, 747, 287, 511, 3799, 590, 286, 43751, 11, 340, 561, 307, 257, 42959, 284, 1793, 338, 13476, 11, 257, 27690, 284, 262, 7835, 5737, 11, 290, 257, 3514, 284, 4302, 15625, 13, 16126, 460, 340, 307, 37104, 11, 475, 326, 884, 28612, 561, 1716, 262, 2723, 286, 24672, 284, 262, 995, 11, 286, 1171, 35765, 414, 11, 290, 286, 2839, 266, 2577, 13, 198, 198, 960, 2039, 15539, 605, 8504, 18203, 20704, 198, 198, 8205, 42445, 2564, 3651, 319, 428, 25, 198, 198, 1870, 1276, 612, 407, 1255, 422, 428, 1683, 12, 25167, 4437, 286, 2296, 419, 2649, 11, 257, 2276, 914, 14857, 1590, 286, 2095, 11, 543, 481, 1085, 11, 379, 938, 11, 284, 12773, 913, 1919, 11916, 30, 383, 6507, 16277, 286, 13258, 28697, 262, 6675, 20283, 389, 475, 1165, 4988, 19000, 13, 5845, 7027, 11, 1871, 3025, 661, 262, 4437, 290, 3357, 286, 3112, 590, 389, 28881, 11, 389, 339, 9269, 1028, 2405, 262, 26485, 286, 1793, 11, 290, 45625, 465, 5316, 284, 4117, 606, 416, 530, 393, 584, 286, 777, 629, 454, 3212, 11, 532, 3026, 36446, 11, 393, 29179, 13, 198, 198, 32, 17623, 28322, 5600, 13, 770]
10.0.2.15: b"Pages\n\nTuesday, March 08, 2011\n\nCarnival and Lent\n\nMARDI GRAS is huge here in Saint Louis. Established back sometime in the 1970s, it has become a large celebration, attracting hundreds of thousands of people to the old Soulard neighborhood of the city. This carnival is in remembrance of the old French heritage of the city, and it imitates the celebrations found in New Orleans, Louisiana, which is downstream from us on the Mississippi River.\n\nLike New Orleans, the Saint Louis celebrations have an obscene character. Despite \xe2\x80\x94 or because? \xe2\x80\x94 of this, Mardi Gras is supported and sponsored by the city government, local corporations, and civic groups. It's good for tourism, they say. I say it's bad for your soul.\n\nBut the reason behind Carnival, and its final day, Mardi Gras, was the observance of the severe penances of Lent. Undoubtably a little celebration is called for before spending forty days spiritually in a desert, which is a time for the penitential acts of prayer, fasting, and almsgiving. But the Latin Church got rid of severe penances at the time just before Mardi Gras became popular [Apostolic Constitution Paenitemini, Pope Paul VI, 1966]. I've noticed many times that whenever the Church appears to give up something that is rightfully hers, someone else will take it for their own purposes.\n\nIt would be a severe mistake to think that the excesses of Carnival and Mardi Gras have anything to do with Catholicism, except in the most tangential and remote way. Do the revelers observe Lent? The New Orleans Carnival (in its present form) was started by non-Catholic Americans of English descent. The Venice Carnival was reestablished in the 1970s by the staunchly anti-Catholic government of Italy, and the Rio de Janeiro Carnival was started by upper-middle class bourgeois infused with Enlightenment ideas. Carnival, except in its most basic form of a moderate celebration, is not Catholic.\n\nAccording to the Very Rev. Dom Prosper Gu\xc3\xa9ranger:\n\n[Lent] ... is the Church's preparation for Easter, and was instituted at the very commencement of Christianity. Our Blessed Lord himself sanctioned it by his fasting forty days and forty nights in the desert; and though he would not impose it on the world by an express commandment, (which, then, could not have been open to the power of dispensation,) yet he showed plainly enough by his own example, that Fasting, which God had so frequently ordered in the Old Law, was to be also practised by the Children of the New.\n\nThe Disciples of St. John the Baptist came, one day, to Jesus, and said to him: Why do we and the Pharisees fast often, but thy Disciples do not fast? And Jesus said to them: Can the children of the bridegroom mourn, as long as the bridegroom is with them? But the days will come, when the bridegroom shall be taken away from them, and then they shall fast. [St Matth. ix. 14,15].\n\nAccording to Gu\xc3\xa9ranger, the observance of Lent dates as far back as Apostolic times, which is the very beginning, and continues the practices found among the Jews before Christ. Penance was originally quite severe, and over the centuries greater laxity appeared. Today, we have arrived at the low point, where little or no penance whatsoever is observed, for neither do Christians do not see any spiritual benefit to it, nor are we obliged to do much of it.\n\nPope Benedict XIV, in 1741, wrote:\n\nThe observance of Lent is the very badge of the Christian warfare. By it, we prove ourselves not to be enemies of the Cross of Christ. By it, we avert the scourges of divine justice. By it, we gain strength against the princes of darkness, for it shields us with heavenly help. Should mankind grow remiss in their observance of Lent, it would be a detriment to God's glory, a disgrace to the Catholic religion, and a danger to Christian souls. Neither can it be doubted, but that such negligence would become the source of misery to the world, of public calamity, and of private woe.\n\n\xe2\x80\x94 Encyclical Non ambigimus\n\nGu\xc3\xa9ranger comments on this:\n\nAnd must there not result from this ever-growing spirit of immortification, a general effeminacy of character, which will lead, at last, to frightful social disorders? The sad predictions of Pope Benedict the Fourteenth are but too truly verified. Those nations, among whose people the spirit and practice of penance are extinct, are heaping against themselves the wrath of God, and provoking his justice to destroy them by one or other of these scourges, - civil discord, or conquest.\n\nA terrifying prophecy indeed. This"
10.0.2.15: [27150, 11, 1737, 1467, 11, 2321, 5956, 10133, 25, 1367, 25, 3901, 3001, 19310, 198, 198, 2949, 10096, 329, 47513, 19942, 287, 44874, 1951, 72, 198, 198, 44879, 47, 2937, 48354, 40, 11, 3936, 357, 44175, 8, 532, 33147, 286, 257, 1957, 6333, 286, 47513, 1211, 2876, 11322, 11370, 423, 3750, 284, 5618, 3078, 625, 663, 5749, 20319, 338, 366, 4976, 2582, 1, 25210, 326, 2222, 14718, 4297, 284, 663, 8215, 11, 2045, 329, 257, 366, 75, 672, 1706, 22979, 21219, 1, 484, 466, 407, 2897, 13, 383, 47513, 1211, 2441, 461, 4803, 26078, 290, 47513, 1211, 2441, 461, 1222, 9300, 31313, 10203, 910, 511, 11135, 287, 44874, 1951, 72, 3128, 736, 284, 11104, 11, 618, 262, 717, 47513, 1211, 2441, 461, 4803, 26078, 4721, 663, 8215, 13, 383, 6333, 373, 9393, 416, 734, 8312, 4172, 508, 550, 7366, 257, 8507, 329, 366, 3575, 803, 922, 3081, 10808, 326, 2810, 922, 2057, 287, 257, 3621, 8137, 379, 257, 6397, 2756, 553, 832, 4388, 367, 7833, 309, 7833, 10808, 326, 4721, 287, 22717, 13, 4930, 47513, 1211, 2441, 461, 1222, 9300, 10808, 5399, 262, 44874, 1951, 72, 21336, 625, 262, 1306, 1936, 812, 11, 1864, 284, 262, 8224, 11, 290, 1943, 468, 1201, 3181, 262, 6333, 6409, 366, 13014, 286, 262, 6705, 41969, 6, 18502, 1, 13304, 422, 262, 44874, 1951, 72, 10244, 3782, 13, 887, 9011, 329, 257, 11780, 5882, 39, 1211, 2441, 461, 4803, 1057, 416, 360, 5872, 22463, 1187, 468, 7910, 4073, 262, 8312, 4172, 6, 4297, 284, 923, 1642, 6283, 7007, 13, 366, 4677, 24378, 734, 812, 2084, 11, 262, 16757, 2067, 2491, 5581, 9011, 287, 262, 44874, 1951, 72, 1989, 329, 262, 47513, 1211, 2441, 461, 4803, 553, 1864, 284, 262, 8224, 13, 366, 1722, 2582, 355, 883, 9011, 2067, 12655, 11, 4036, 10802, 5091, 416, 262, 1171, 526, 366, 1890, 1672, 11, 262, 16757, 561, 32740, 257, 705, 75, 672, 1706, 22979, 21219, 6, 416, 262, 47513, 1211, 2441, 461, 4803, 11, 475, 16757, 750, 407, 423, 597, 10808, 287, 44874, 1951, 72, 13, 366, 25341, 286, 262, 1171, 561, 1282, 656, 21880, 6, 10808, 11, 5361, 739, 262, 3891, 705, 43, 19757, 1211, 2441, 461, 1222, 9300, 6, 290, 705, 43, 19757, 1211, 2441, 461, 4803, 26078, 6, 290, 1265, 329, 262, 43657, 22979, 21219, 13, 366, 464, 4297, 561, 788, 651, 25890, 780, 262, 23944, 2378, 852, 4438, 416, 16757, 373, 407, 1695, 287, 21880, 6, 10808, 526, 360, 5872, 635, 2540, 6301, 6979, 4116, 329, 663, 7072, 11, 475, 4297, 2222, 606, 284, 262, 10808, 12228, 416, 262, 8312, 4172, 290, 3512, 326, 484, 307, 21014, 11, 1864, 284, 262, 8224, 13, 1675, 1394, 4297, 3772, 11, 262, 8312, 12, 11990, 47513, 1211, 1139, 340, 468, 9258, 38276, 663, 20319, 338, 9780, 45972, 13, 360, 5872, 4952, 9692, 284, 663, 3052, 326, 663, 5882, 39, 1211, 26078, 318, 366, 4976, 2582, 1, 284, 257, 44874, 1951, 72, 17374, 13, 383, 8312, 12, 11990, 2706, 765, 284, 551, 22179, 14423, 9256, 414, 422, 1262, 262, 47513, 1211, 2441, 461, 4803, 11112, 11, 1317, 11, 366, 273, 597, 584, 3891, 393, 8849, 15337, 306, 2092, 44597, 287, 262, 44874, 1951, 72, 11, 3936, 1989, 526, 360, 5872, 6823, 663, 5882, 39, 1211, 1317, 329, 366, 2118, 2899, 415, 2594, 1, 287, 2932, 3717, 11, 1864, 284, 262, 8224, 11, 543, 1139, 262, 13861, 6, 779, 286, 262, 11112, 290, 1317, 9667, 736, 284, 14745, 13, 383, 2717, 8224, 5380, 12616, 329, 3991, 22566, 286, 8159, 739, 262, 14730, 2763, 2191, 11, 11675, 5449, 290, 16028, 11844, 1009, 739, 1181, 1099, 13, 360, 5872, 22463, 1187, 290, 663, 22347, 360, 5872, 50053, 389, 3706, 355, 16757, 7848, 14423, 9256, 414, 8549, 11, 262, 2656, 5882, 39, 1211, 4870, 326, 360, 5872, 9477, 287, 4343, 13, 554, 3090, 284, 517, 621, 13803, 5882, 39, 1211, 2876, 11322, 11370, 13673, 11, 360, 5872, 12216, 1811, 584, 14659, 1390, 30012, 11652, 13]
10.0.2.15: b'Wednesday, May 16, 2012Last Update: 11:41 AM PT\n\nNo Room for Longhorns in Corpus Christi\n\nCORPUS CHRISTI, Texas (CN) - Owners of a local chain of Longhorn steakhouses have gone to Federal Court over its bigger competitor\'s "coming soon" advertisements that bring frustrated customers to its doors, looking for a "lobster baked potato" they do not offer. The Longhorn Steakhouse Restaurant and Longhorn Steak & Ale Plaintiffs say their roots in Corpus Christi date back to 1989, when the first Longhorn Steakhouse Restaurant opened its doors. The chain was founded by two Greek families who had earned a reputation for "operating good quality restaurants that provided good food in a nice atmosphere at a reasonable price," through successful Hasty Tasty restaurants that opened in 1946. Two Longhorn Steak & Ale restaurants joined the Corpus Christi flagship over the next five years, according to the complaint, and success has since brought the chain numerous "Best of the Best Readers\' Choice" awards from the Corpus Christi Caller Times. But ads for a competing LongHorn Steakhouse run by Darden Restaurants has allegedly caused the Greek families\' customers to start making strange requests. "Approximately two years ago, the defendants started running television ads in the Corpus Christi area for the Longhorn Steakhouse," according to the complaint. "As soon as those ads started appearing, actual confusion occurred by the public." "For example, the defendants would advertise a \'lobster baked potato\' by the Longhorn Steakhouse, but defendants did not have any restaurants in Corpus Christi. "Members of the public would come into plaintiffs\' restaurants, operating under the names \'Longhorn Steak & Ale\' and \'Longhorn Steakhouse Restaurant\' and ask for the lobster baked potato. "The customers would then get aggravated because the advertised item being offered by defendants was not available in plaintiffs\' restaurants." Darden also began selling gift cards for its restaurant, but customers bring them to the restaurants operated by the Greek families and demand that they be honored, according to the complaint. To keep customers happy, the Greek-owned Longhorn says it has begun honoring its competitor\'s discount coupons. Darden tells visitors to its website that its LongHorn Restaurant is "coming soon" to a Corpus Christi mall. The Greek-owned companies want to enjoin Rare Hospitality from using the Longhorn Steakhouse logo, mark, "or any other names or marks confusingly similar thereto in the Corpus Christi, Texas area." Darden registered its LongHorn mark for "restaurant services" in August 2009, according to the complaint, which says the competitors\' use of the logo and mark dates back to 1981. The federal complaint seek damages for false designation of origin under the Lanham Act, unfair competition and trademark dilution under state law. Darden Restaurants and its subsidiary Darden Concepts are named as defendants alongside Rare Hospitality Management, the original LongHorn owner that Darden acquired in 2007. In addition to more than 350 LongHorn steakhouses nationwide, Darden owns several other chains including Olive Garden.'
10.0.2.15: [9, 6836, 1063, 3402, 389, 4013, 5330, 5220, 6308, 4536, 691, 290, 466, 407, 2291, 5704, 11, 198, 43085, 393, 257, 720, 37128, 10965, 290, 9041, 3877, 13, 33520, 4038, 14, 15526, 652, 3484, 11, 198, 75, 4820, 11, 290, 9988, 7565, 13, 15138, 364, 900, 511, 898, 4536, 13, 4222, 5725, 534, 6163, 16456, 13, 198, 198, 9, 6836, 1063, 3402, 389, 4013, 5330, 5220, 6308, 4536, 691, 290, 466, 407, 2291, 5704, 11, 5964, 393, 257, 720, 37128, 198, 16520, 1883, 290, 9041, 3877, 13, 33520, 5672, 14, 15526, 652, 3484, 11, 4827, 290, 9988, 7565, 13, 198, 45776, 364, 900, 511, 898, 4536, 13, 4222, 5725, 534, 6163, 16456, 13, 198, 198, 5304, 45715, 55, 198, 198, 9, 6836, 1063, 3402, 389, 4013, 5330, 5220, 6308, 4536, 691, 290, 466, 407, 2291, 5704, 11, 5964, 393, 281, 720, 37128, 198, 16520, 1883, 290, 9041, 3877, 13, 33520, 5672, 14, 15526, 652, 3484, 11, 4827, 290, 9988, 7565, 13, 198, 45776, 364, 900, 511, 898, 4536, 13, 4222, 5725, 534, 6163, 16456, 13, 198, 198, 5304, 31475, 55, 198, 198, 9, 6836, 1063, 3402, 389, 4013, 5330, 5220, 6308, 4536, 691, 290, 466, 407, 2291, 5704, 11, 5964, 393, 257, 720, 37128, 10965, 290, 9041, 3877, 13, 33520, 4038, 14, 15526, 652, 3484, 11, 4827, 11, 290, 9988, 7565, 13, 15138, 364, 900, 511, 898, 4536, 13, 4222, 5725, 534, 6163, 16456, 13, 198, 198, 9, 6836, 1063, 3402, 389, 4013, 5330, 5220, 6308, 4536, 691, 290, 466, 407, 2291, 5704, 11, 198, 43085, 393, 257, 720, 37128, 10965, 290, 9041, 3877, 13, 33520, 5672, 14, 15526, 652, 3484, 11, 198, 75, 4820, 290, 9988, 7565, 13, 15138, 364, 900, 511, 898, 4536, 13, 4222, 5725, 534, 6163, 16456, 13, 198, 198, 23739, 4200, 287, 262, 717, 2237, 1933, 286, 1946, 423, 27324, 262, 2368, 12, 20158, 10670, 55, 338, 2292, 355, 2253, 338, 1266, 12, 16473, 1115, 12, 808, 13064, 26763, 532, 407, 691, 287, 1946, 11, 475, 286, 477, 640, 13, 9561, 2795, 11, 4200, 286, 10670, 55, 389, 510, 8257, 13, 19, 1411, 357, 5589, 1144, 284, 262, 976, 2278, 938, 614, 8, 284, 1542, 11, 21, 2414, 4991, 11, 6079, 23818, 4200, 286, 262, 10670, 55, 625, 663, 3016, 1315, 12, 1941, 2106, 284, 718, 5892, 11, 43147, 4991, 11, 1642, 340, 262, 749, 2968, 1115, 12, 808, 13064, 26763, 286, 477, 640, 24620, 554, 1109, 11, 262, 10670, 55, 468, 20633, 477, 584, 1115, 12, 808, 13064, 13558, 23266, 287, 262, 5079, 4200, 16905, 287, 790, 614, 1201, 6244, 13, 27843, 13628, 471, 13, 50, 13, 4200, 286, 10670, 55, 389, 14486, 284, 17341, 13037, 11, 830, 4991, 878, 262, 886, 286, 262, 614, 13, 198, 198, 1, 43, 2821, 1601, 1097, 14456, 423, 9635, 11, 290, 356, 1107, 765, 284, 5875, 674, 4013, 5330, 4297, 329, 1642, 262, 10670, 55, 262, 1598, 8464, 1871, 1115, 12, 808, 13064, 13558, 23266, 553, 531, 4995, 6366, 615, 715, 72, 11, 4664, 7927, 1893, 290, 2276, 4706, 286, 4013, 5330, 7458, 13, 366, 12740, 55, 318, 262, 1598, 18335, 287, 663, 1398, 11, 9835, 13630, 262, 1266, 6087, 286, 3033, 11, 2163, 290, 2854, 326, 13064, 14456, 389, 706, 526, 198, 198, 12832, 5330, 43185, 262, 1115, 12, 808, 27668, 13064, 26763, 10618, 618, 340, 5611, 262, 717, 12, 20158, 10670, 55, 287, 4751, 355, 257, 5878, 2746, 11, 618, 262, 10670, 55, 7907, 1111, 262, 12533, 22836, 26763, 286, 262, 6280, 290, 2258, 1605, 24892, 286, 262, 6280, 8714, 13, 1114, 1115, 10439, 11, 262, 471, 13, 50, 13, 8705, 1074, 4497, 329, 10670, 55, 468, 18434, 6190, 290, 20449, 262, 2656, 3721, 286, 262, 10670, 55, 532, 257, 1115, 12, 808, 11, 767, 12, 6603, 6540, 13064, 26763, 351, 9098, 16846, 11, 5252, 9332, 11, 290, 6594, 290, 9041, 2854, 13, 198, 198, 464, 2368, 12, 20158, 11, 1946, 4013, 5330, 10670, 55, 11, 5495, 287, 2795, 2211, 11, 468, 29900, 257, 1844, 787, 2502, 11, 3599, 351, 262, 2478, 286, 281, 477, 12, 3605, 11, 517, 20831, 290, 18700, 1767, 290, 24587, 11, 12047, 517, 621, 25829, 8059, 9051, 262, 2180, 2746, 290, 40776, 1688, 8810, 287, 1767, 7805, 17995, 11, 9551, 34743, 9332, 290, 1877, 2491, 6625, 13, 198, 198, 32, 3747, 3554, 11, 262, 2368, 12, 20158, 10670, 55, 8991, 4013, 5330, 338, 1306, 12, 20158, 40488, 8151, 1767, 4645, 290, 262, 717, 12, 964, 779, 286, 4013, 5330, 338, 649, 3024, 12, 301, 13322, 7771, 11, 530, 12, 12239, 3420, 15175, 877, 5858, 13, 1081, 257, 1255, 286, 777, 290, 584, 1486, 29754, 11, 262, 10670, 55, 6492, 1398, 12, 12294, 10193, 5252, 12, 13926, 88, 10109, 286, 1160, 14, 2078, 14, 1954, 285, 6024, 1748, 14, 8929, 1014, 14, 24011, 1389, 981, 13748, 1111, 257, 1936, 12, 7364, 14674, 21501, 15178, 422, 262, 399, 6535, 4090, 290, 257, 28662, 37630, 2767, 56, 350, 11860, 10, 7955, 422, 262, 2873, 7998, 11, 257, 2218, 691, 530, 584, 13064, 26763, 468, 14451, 13, 383, 10670, 55, 635, 4394, 3744, 11087, 4467, 11, 6190, 19843, 2884, 663, 1306, 12, 20158, 4013, 5330, 11280, 8151, 1080, 11, 290, 9257, 6596, 2368, 12, 808, 5726, 290, 8420, 2884, 663, 5141, 12, 13120, 1881, 12, 35211, 6857, 12, 818, 3895, 13, 198, 198, 464, 10670, 55, 468, 587, 3562, 11, 4166, 290, 15943, 287, 2258, 2253, 329, 1115, 10439, 13, 383, 2368, 12, 20158, 10670, 55, 318, 15943, 11541, 379, 262, 1664, 338, 12406, 11, 9266, 27930, 290, 3113, 3227, 6841, 11, 1262, 5928, 290, 18309, 18229, 3354, 13, 198, 198, 8585, 4013, 5330, 198, 12832, 5330, 4394, 257, 1336, 1627, 286, 48299, 6190, 2854, 12, 22564, 1601, 5672, 832, 257, 3127, 286, 25829, 471, 13, 50, 13, 4013, 5330, 15737, 13, 383, 4013, 5330, 12750, 3033, 1936, 18778, 4981, 25, 262, 45715, 55, 13064, 21336, 38988, 11, 262, 24811, 55, 2854, 13064, 38988, 357, 283, 380, 1075, 428, 3931, 828, 262, 14639, 55, 16001, 13064, 38988, 11, 262, 642, 12, 6603, 6540, 31475, 55, 13064, 27668, 26763, 11, 290, 262, 767, 12, 6603, 6540, 10670, 55, 13064, 6332, 10361, 4038, 13, 4013, 5330, 373, 2904, 8018, 416]
10.0.2.15: b'*Prices shown are Acura suggested retail prices only and do not include taxes,\nlicense or a $920 destination and handling charge. Actual vehicle/accessory costs,\nlabor, and installation vary. Dealers set their own prices. Please consult your selected dealer.\n\n*Prices shown are Acura suggested retail prices only and do not include taxes, license or a $920\ndestination and handling charge. Actual vehicles/accessory costs, labor and installation vary.\nDealers set their own prices. Please consult your selected dealer.\n\n2016 RLX\n\n*Prices shown are Acura suggested retail prices only and do not include taxes, license or an $920\ndestination and handling charge. Actual vehicles/accessory costs, labor and installation vary.\nDealers set their own prices. Please consult your selected dealer.\n\n2016 RDX\n\n*Prices shown are Acura suggested retail prices only and do not include taxes, license or a $920 destination and handling charge. Actual vehicle/accessory costs, labor, and installation vary. Dealers set their own prices. Please consult your selected dealer.\n\n*Prices shown are Acura suggested retail prices only and do not include taxes,\nlicense or a $920 destination and handling charge. Actual vehicles/accessory costs,\nlabor and installation vary. Dealers set their own prices. Please consult your selected dealer.\n\nRecord sales in the first six months of 2014 have strengthened the third-generation MDX\'s position as America\'s best-selling three-row luxury SUV - not only in 2014, but of all time. Through June, sales of MDX are up 68.4 percent (compared to the same period last year) to 30,664 units, bringing cumulative sales of the MDX over its nearly 15-year history to 692,710 units, making it the most popular three-row luxury SUV of all time*. In fact, the MDX has topped all other three-row luxury SUVs in the annual sales rankings in every year since 2002. Cumulative U.S. sales of MDX are anticipated to surpass 700,000 units before the end of the year.\n\n"Luxury car buyers have spoken, and we really want to thank our Acura customers for making the MDX the clear winner among three-row luxury SUVs," said Mike Accavitti, senior vice president and general manager of Acura Division. "MDX is the clear benchmark in its class, consistently delivering the best combination of features, function and performance that luxury buyers are after."\n\nAcura pioneered the three-row crossover luxury SUV segment when it launched the first-generation MDX in 2000 as a 2001 model, when the MDX captured both the Motor Trend SUV of the Year and North American Truck of the Year titles. For three generations, the U.S. engineering team responsible for MDX has steadily advanced and refined the original concept of the MDX - a three-row, 7-passenger luxury SUV with superior packaging, fuel efficiency, and ride and handling performance.\n\nThe third-generation, 2014 Acura MDX, introduced in June 2013, has undergone a complete makeover, starting with the development of an all-new, more rigid and lightweight body and chassis, dropping more than 275 pounds versus the previous model and boasting major gains in body rigidity, aerodynamic efficiency and low running resistance.\n\nA safety leader, the third-generation MDX applies Acura\'s next-generation ACE\xe2\x84\xa2 body structure and the first-ever use of Acura\'s new hot-stamped steel, one-piece door stiffener ring. As a result of these and other design enhancements, the MDX obtained class-leading EPA fuel-economy ratings of 20/28/23 mpg city/highway/combined while earning both a five-star Overall Vehicle Score from the NHTSA and a TOP SAFETY PICK+ rating from the IIHS, a feat only one other luxury SUV has matched. The MDX also offers greater interior comfort, advanced connectivity via its next-generation AcuraLink\xe2\x84\xa2 system, and greatly improved third-row entry and exit via its kid-friendly One-Touch Walk-In feature.\n\nThe MDX has been designed, developed and manufactured in North America for three generations. The third-generation MDX is manufactured exclusively at the company\'s Lincoln, Alabama automobile and engine production facility, using domestic and globally sourced parts.\n\nAbout Acura\nAcura offers a full line of technologically advanced performance-luxury vehicles through a network of 275 U.S. Acura dealers. The Acura lineup features five distinctive models: the RLX luxury flagship sedan, the TLX performance luxury sedan (arriving this summer), the ILX compact luxury sedan, the 5-passenger RDX luxury crossover SUV, and the 7-passenger MDX luxury sport utility vehicle. Acura was recently recognized by'
10.0.2.15: [32069, 28284, 449, 417, 912, 354, 2630, 25, 198, 29, 6674, 340, 561, 307, 1365, 284, 779, 198, 29, 1223, 588, 366, 4906, 26011, 1, 329, 2099, 47217, 11, 1201, 1262, 366, 4906, 1, 329, 18684, 198, 29, 2099, 47217, 318, 15850, 6949, 13, 198, 40, 561, 307, 3772, 611, 366, 4906, 1, 547, 4504, 284, 262, 5933, 286, 24284, 42814, 13, 198, 464, 584, 1438, 314, 6777, 4601, 314, 714, 779, 318, 366, 12286, 1911, 632, 338, 16655, 198, 5562, 340, 338, 10395, 329, 884, 257, 8365, 973, 3895, 13, 9425, 11, 314, 760, 11, 48986, 1754, 338, 1099, 13, 198, 438, 3932]
10.0.2.15: b'Wolfgang Jeltsch wrote:\n> Maybe it would be better to use\n> something like "typealias" for type aliases, since using "type" for declaring\n> type aliases is misleading anyway.\nI would be happy if "type" were returned to the pool of usable identifiers.\nThe other name I frequently wish I could use is "default". It\'s frustrating\nthat it\'s reserved for such a rarely used feature. Yeah, I know, Wadler\'s law.\n-- Ben'
10.0.2.15: [1135, 994, 379, 8444, 672, 6404, 423, 1775, 617, 7650, 2267, 3640, 5952, 287, 262, 1438, 286, 3783, 13, 770, 318, 530, 286, 674, 18852, 422, 428, 614, 25, 4837, 11065, 262, 41897, 6608, 286, 34972, 13, 8105, 1335, 306, 11, 1466, 508, 423, 1714, 1231, 33298, 389, 1342, 1884, 284, 307, 19095, 0]
10.0.2.15: b'We here at Discoblog have seen some weird research studies conducted in the name of science. This is one of our favorites from this year: researchers studying the antidepressant properties of semen. Supposedly, women who have sex without condoms are less likely to be depressed!'
10.0.2.15: [4863, 25, 2124, 62, 2655, 6570, 320, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 2231, 9114, 357, 17429, 8, 198, 198, 72, 460, 470, 5409, 611, 428, 4047, 14851, 393, 20105, 13, 628, 198, 198, 72, 1183, 467, 351, 1111, 13, 1693, 880, 1760, 13, 198, 198, 4863, 25, 13791, 1359, 1485, 198, 198, 10531, 12, 2999, 12, 486, 8702, 25, 4349, 716, 357, 17429, 8, 198, 198, 464, 1573, 345, 821, 2045, 329, 318, 366, 71, 1794, 396, 333, 4623, 526, 367, 1794, 699, 1343, 14851, 13, 1766, 1389, 416, 44075, 28894, 319, 607, 2691, 11389, 5337, 366, 51, 2040, 286, 25108, 526, 5626, 17795, 780, 340, 338, 19678, 278, 355, 277, 12, 278, 8469, 290, 314, 1053, 587, 3424, 329, 2237, 1933, 783, 13, 198, 198, 4863, 25, 474, 6475, 798, 375, 1362, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 2231, 9114, 357, 17429, 8, 198, 198, 13014, 30160, 12770, 20498, 13, 198, 198, 7, 3919, 6907, 45610, 1413, 7923, 1058, 47, 8, 628, 198, 198, 45882, 379, 3717, 12, 486, 12, 3132, 9130, 25, 3510, 9114, 357, 17429, 8, 198, 198, 4863, 25, 10610, 560, 198, 198, 10531, 12, 486, 12, 3132, 8487, 25, 2999, 9114, 357, 17429, 8, 198, 198, 19836, 11, 3737, 0, 475, 47156, 11, 645, 24518, 13, 45610, 1413, 7923, 796, 1592, 2488, 1711, 13508, 13, 198, 198, 4863, 25, 502, 413, 2954, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 3510, 9114, 357, 17429, 8, 198, 198, 22017, 11, 644, 30, 314, 765, 284, 760, 45038, 1016, 319, 2157, 477, 883, 2042, 13043, 986, 198, 198, 4863, 25, 2124, 4111, 280, 305, 2865, 418, 198, 198, 10531, 12, 486, 12, 3132, 8753, 25, 1495, 9114, 357, 17429, 8, 198, 198, 2396, 857, 339, 11, 502, 413, 2954, 986, 523, 857, 339, 13, 198, 198, 4863, 25, 12524, 15883, 2502, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 2857, 9114, 357, 17429, 8, 198, 198, 16643, 46537, 198, 198, 4863, 25, 5935, 12135, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 2857, 9114, 357, 17429, 8, 198, 198, 2396, 14603, 6, 50, 703, 345, 1057, 257, 4388, 3992, 785, 291, 0, 198, 198, 4863, 25, 10662, 66, 73, 27446, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 4349, 9114, 357, 17429, 8, 198, 198, 1639, 1549, 307, 24562, 703, 867, 661, 466, 340, 262, 2642, 835, 0, 198, 198, 4863, 25, 35966, 8691, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 2857, 9114, 357, 17429, 8, 198, 198, 40, 1100, 262, 513, 4372, 6103, 11, 290, 788, 46379, 10719, 326, 616, 3725, 286, 2679, 550, 4054, 502, 290, 326, 314, 550, 4020, 26084, 17249, 13, 8989, 11, 314, 8020, 470, 13, 628, 628, 198, 198, 4863, 25, 10662, 66, 73, 27446, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 4309, 9114, 357, 17429, 8, 198, 198, 40, 373, 1107, 6655, 703, 2952, 673, 4499, 703, 284, 910, 340, 287, 3594, 628, 198, 198, 8499, 674, 717, 366, 29891, 1, 198, 198, 7, 5005, 33342, 2912, 8, 198, 198, 4863, 25, 2576, 62, 261, 62, 64, 62, 21809, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 1120, 9114, 357, 17429, 8, 198, 198, 4053, 428, 6688, 257, 1256, 986, 198, 198, 4863, 25, 39518, 380, 690, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 2816, 9114, 357, 17429, 8, 198, 198, 4598, 345, 588, 5206, 2927, 1040, 30, 198, 198, 4863, 25, 285, 488, 3609, 20042, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 3980, 9114, 357, 17429, 8, 198, 198, 2215, 314, 1663, 510, 11, 314, 18869, 307, 257, 3992, 785, 291, 6802, 0, 198, 198, 4863, 25, 686, 21680, 3699, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 3553, 9114, 357, 17429, 8, 198, 198, 40, 1053, 587, 33217, 661, 790, 1110, 290, 991, 836, 470, 423, 355, 867, 3296, 355, 345, 13, 1058, 47, 198, 198, 4863, 25, 15552, 9703, 36244, 198, 198, 10531, 12, 486, 12, 3132, 9130, 25, 3553, 9114, 357, 17429, 8, 198, 198, 2396, 14603, 561, 4727, 1367, 938, 1755, 986]
10.0.2.15: b'From: x_seraphim\n\n2009-01-31 06:45 pm (UTC)\n\ni can\'t decide if this highly disturbing or hilarious.\n\n\n\ni\'ll go with both. job well done.\n\nFrom: greyling13\n\n2009-02-01 04:51 am (UTC)\n\nThe word you\'re looking for is "hilaristurbing." Hilarious + disturbing. Coined by Alexandra Erin on her online serial novel "Tales of MU." NOT linking because it\'s addicting as f-ing crack and I\'ve been clean for six months now.\n\nFrom: jammiedodger\n\n2009-01-31 06:45 pm (UTC)\n\nBest hourly comics EVER.\n\n(no offense john campbell :P)\n\n\n\nEdited at 2009-01-31 06:46 pm (UTC)\n\nFrom: inflationary\n\n2009-01-31 08:02 pm (UTC)\n\nclose, perhaps! but alas, no cigar. john campbell = win @ hourlies.\n\nFrom: meewunk\n\n2009-01-31 06:46 pm (UTC)\n\nWow, what? I want to know whats going on behind all those black panels...\n\nFrom: xratedouroboros\n\n2009-01-31 07:25 pm (UTC)\n\nSo does he, meewunk... so does he.\n\nFrom: hostilemakeover\n\n2009-01-31 06:47 pm (UTC)\n\nWin XD\n\nFrom: eggstorm\n\n2009-01-31 06:47 pm (UTC)\n\nSo THAT\'S how you run a successful webcomic!\n\nFrom: qcjeph\n\n2009-01-31 06:51 pm (UTC)\n\nYou\'d be amazed how many people do it the wrong way!\n\nFrom: devalmont\n\n2009-01-31 06:47 pm (UTC)\n\nI read the 3rd panel, and then dearly hoped that my knowledge of German had failed me and that I had mistranslated. Unfortunately, I hadn\'t.\n\n\n\n\n\nFrom: qcjeph\n\n2009-01-31 06:52 pm (UTC)\n\nI was really surprised how quickly she learned how to say it in English\n\n\n\nafter our first "session"\n\n(Deleted comment)\n\nFrom: girl_on_a_wire\n\n2009-01-31 06:50 pm (UTC)\n\nwell this explains a lot...\n\nFrom: flamingrivers\n\n2009-01-31 06:55 pm (UTC)\n\ndo you like phil collins?\n\nFrom: michaeleen\n\n2009-01-31 06:56 pm (UTC)\n\nWhen I grow up, I wanna be a webcomic artist!\n\nFrom: rosalarian\n\n2009-01-31 06:57 pm (UTC)\n\nI\'ve been murdering people every day and still don\'t have as many fans as you. :P\n\nFrom: pupdog311\n\n2009-01-31 06:57 pm (UTC)\n\nSo THAT would explain 11 last night...'
10.0.2.15: [75, 2899, 12, 15087, 338, 2818, 1281, 319, 8092, 338, 14953, 2157, 3501, 6031, 2848, 262, 2000, 1630, 9839, 1107, 7867, 502, 1058, 35]
10.0.2.15: b"laur-rants's perfect post on Ford's intentions behind giving Dipper the mind control tie really inspired me :D"
10.0.2.15: [7112, 56, 2885, 39, 532, 5514, 1528, 706, 2677, 31995, 286, 7420, 9671, 4884, 257, 15100, 25931, 7464, 262, 3958, 319, 1466, 5059, 11, 257, 582, 373, 5169, 329, 7910, 11123, 1466, 2263, 284, 262, 2975, 11, 4498, 2056, 2098, 994, 319, 3217, 13, 198, 198, 14906, 4477, 706, 262, 15422, 198, 198, 464, 1499, 338, 19614, 9475, 3414, 262, 582, 338, 3251, 319, 3009, 319, 3217, 11, 12316, 326, 262, 27022, 1981, 550, 587, 6412, 284, 262, 1171, 13683, 13, 198, 198, 1, 40, 21192, 284, 1793, 11, 597, 2415, 3025, 1097, 9457, 866, 532, 314, 481, 4245, 607, 290, 607, 1097, 553, 262, 582, 531, 287, 257, 2008, 4888, 319, 1919, 2056, 11, 281, 3230, 1705, 4086, 2098, 13, 198, 198, 34, 1780, 1644, 4237, 290, 1957, 2056, 11, 262, 1705, 4086, 3136, 326, 262, 582, 373, 287, 465, 1160, 82, 290, 326, 465, 3251, 550, 587, 6149, 416, 262, 8153, 286, 262, 8345, 22783, 13, 632, 338, 10061, 644, 9837, 262, 582, 481, 1986, 611, 1043, 6717, 13, 198, 198, 13689, 428, 1285, 11, 262, 7420, 2677, 4884, 257, 25931, 16216, 262, 19614, 4139, 284, 4538, 290, 11206, 3306, 17835, 284, 4979, 6647, 13, 383, 649, 3173, 389, 2938, 284, 1282, 656, 2700, 319, 2795, 1987, 11, 2864, 13, 198, 198, 13689, 287, 262, 1227, 11, 257, 7420, 33824, 373, 21376, 329, 2282, 326, 1466, 836, 470, 10925, 284, 3708, 780, 484, 366, 8807, 423, 257, 3860, 286, 257, 3632, 526, 30843, 10318, 324, 978, 12, 39, 2926, 380, 373, 9301, 422, 9489, 465, 4158, 10741, 780, 286, 465, 3651, 13]
10.0.2.15: b'RIYADH - Only days after King Salman of Saudi Arabia issued a royal decree ending the ban on women driving, a man was arrested for allegedly threatening women taking to the road, Arab media reported here on Friday.\n\nArticle continues after the advertisement\n\nThe country\'s Interior Ministry announced the man\'s arrest on Twitter on Friday, stating that the unidentified individual had been referred to the public prosecutor.\n\n"I swear to God, any woman whose car breaks down - I will burn her and her car," the man said in a video shared on social media, an international news agency reported.\n\nCiting police sources and local media, the news agency reports that the man was in his 20s and that his arrest had been ordered by the governor of the Eastern Province. It\'s unclear what punishment the man will face if found guilty.\n\nEarlier this week, the Saudi King issued a decree ordering the Interior Minister to draft and adopt necessary amendments to traffic regulations. The new rules are expected to come into force on June 24, 2018.\n\nEarlier in the month, a Saudi cleric was slammed for saying that women don\'t deserve to drive because they "only have a quarter of a brain." Sheikh Saad Al-Hijri was banned from performing his religious duties because of his comments.'
10.0.2.15: [24, 2791, 376, 13, 17, 67, 20959, 198, 3270, 7011, 2295, 489, 13, 6836, 330, 13, 35155, 13, 357, 33, 4535, 8, 604, 5237, 11, 3270, 2295, 489, 13, 1736, 330, 13, 4280, 13, 350, 6073, 11, 43610, 33, 11369, 402, 13, 4810, 8476, 11, 41990, 12, 4677, 695, 415, 11, 6372, 12, 4677, 417, 7197, 11, 85, 13, 40569, 9693, 7036, 13793, 35, 10725, 1222, 24994, 4503, 40, 29462, 11, 19387, 1581, 44680, 11617, 11, 290, 14430, 40202, 49589, 11, 44162, 12, 4677, 13485, 274, 11, 6372, 12, 4677, 695, 1187, 13, 198, 45, 418, 13, 10495, 12, 19214, 18, 11, 10495, 12, 1954, 4790, 290, 10495, 12, 2718, 1983, 13, 198, 17013, 1829, 3078, 286, 20172, 11, 4653, 20987, 13588, 13, 198, 28100, 1739, 3158, 13, 1105, 11, 9768, 13, 10707, 1384, 2901, 807, 11, 9768, 13, 3041, 258, 1723, 290, 797, 258, 1723, 2039, 347, 1192, 21306, 798, 2447, 13, 604, 11, 9768, 13, 198, 198, 13256, 371, 13, 5426, 357, 853, 1739, 8, 290, 5335, 412, 13, 7158, 6148, 11, 5426, 11, 5426, 11, 35756, 41027, 1222, 26169, 8847, 11, 14909, 11, 24884, 1539, 329, 17155, 402, 13, 7886, 13, 198, 7554, 47005, 357, 853, 1739, 8, 290, 22937, 449, 13, 5256, 4447, 268, 11, 337, 23225, 11, 10120, 11, 2631, 589, 1222, 371, 7456, 306, 11, 14909, 11, 24884, 1539, 329, 13606, 16455, 805, 1222, 29306, 11, 3457, 13, 290, 14430, 11023, 49589, 13, 198, 8421, 23715, 52, 1137, 11, 5953, 8974, 11, 28069, 21479, 11, 13588, 8974, 11, 290, 9677, 4663, 3398, 26761, 11, 14017, 13588, 8974, 13, 198, 37997, 21479, 11, 13588, 8974, 13, 628, 198, 16, 198, 818, 12122, 11, 13606, 16455, 805, 290, 29306, 11, 543, 357, 35131, 584, 4568, 8, 9824, 290, 12188, 3315, 6832, 11, 6294, 17155, 7886, 11, 257, 42414, 287, 663, 3095, 7038, 7297, 13, 220, 220, 7886, 16334, 16455, 805, 11, 290, 262, 1182, 286, 262, 3095, 7038, 7297, 11, 11023, 49589, 11, 739, 262, 7129, 49298, 287, 24656, 2191, 11, 2808, 471, 13, 50, 13, 34, 13, 47171, 718, 2481, 2123, 33756, 13, 220, 220, 7886, 6492, 257, 9002, 5764, 357, 32826, 1111, 16757, 8, 286, 720, 23188, 11, 9879, 287, 736, 15577, 11, 17747, 286, 644, 339, 561, 423, 7366, 422, 16455, 805, 1022, 262, 3128, 286, 465, 17655, 290, 262, 3128, 286, 4473, 11, 550, 339, 407, 587, 6294, 11, 20208, 644, 339, 750, 5160, 422, 1854, 26, 220, 780, 262, 9002, 1043, 262, 16757, 6, 8747, 286, 262, 2479, 8839, 1099, 284, 423, 587, 47860, 11, 262, 5052, 15229, 428, 5764, 13, 220, 2808, 471, 13, 50, 13, 34, 13, 8460, 718, 2075, 7, 65, 737, 220, 220, 383, 9002, 635, 11343, 7886, 720, 15426, 11, 830, 287, 366, 8534, 1414, 553, 10200, 262, 1944, 1988, 286, 262, 2003, 12042, 326, 7886, 561, 423, 550, 422, 16455, 805, 11, 20208, 262, 1944, 1988, 286, 465, 14486, 2003, 12042, 287, 465, 1459, 13755, 286, 1103, 7964, 20426, 13, 220, 220, 8880, 1414, 318, 281, 5559, 284, 6865, 26090, 11, 543, 7886, 550, 9167, 475, 543, 262, 5052, 6520, 284, 1502, 780, 286, 366, 21973, 723, 23109, 290, 16757, 6, 3767, 4459, 326, 20870, 318, 33840, 526, 220, 220, 383, 5052, 19032, 9617, 503, 262, 9002, 338, 5764, 286, 2166, 1414, 319, 262, 9384, 326, 262, 2370, 286, 7886, 338, 2626, 2003, 12042, 373, 28991, 290, 326, 281, 5764, 286, 2166, 1414, 561, 23418, 262, 5764, 286, 8122, 515, 12616, 357, 25916, 338, 1438, 329, 262, 26862, 286, 12616, 284, 543, 262, 3117, 286, 257, 47860, 8747, 286, 262, 2479, 8839, 1099, 318, 9080, 737, 220, 220, 7886, 338, 3272, 12, 1324, 10621, 4427, 1111, 262, 14425, 286, 2166, 1414, 290, 262, 5052, 338, 5764, 286, 691, 720, 6469, 11, 830, 287, 14449, 6, 6642, 26, 220, 7886, 550, 7194, 720, 22980, 11, 830, 13, 220, 220, 679, 857, 407, 5198, 422, 262, 14425, 286, 6865, 26090, 13, 628, 198, 17, 198, 464, 2370, 286, 2479, 8839, 373, 7888, 26, 220, 523, 7888, 11, 262, 16757, 7267, 11, 355, 284, 44594, 606, 284, 8492, 29951, 262, 15593, 13, 220, 220, 7886, 373, 4153, 618, 339, 2627, 257, 3315, 12, 11249, 654, 42414, 329, 16455, 805, 357, 14363, 2180, 1693, 550, 587, 355, 257, 5103, 21277, 329, 262, 1664, 737, 220, 220, 770, 373, 262, 976, 2479, 355, 465, 6478, 11, 14873, 891, 23048, 11023, 49589, 13, 220, 220, 4930, 812, 706, 852, 9657, 11, 7886, 373, 6294, 11, 422, 262, 1664, 355, 880, 355, 422, 262, 7297, 11, 706, 617, 4297, 550, 13832, 546, 465, 287, 1078, 1463, 284, 3703, 290, 706, 11023, 49589, 550, 587, 1297, 416, 465, 6478, 284, 4646, 262, 4200, 2700, 287, 262, 3095, 7038, 7297, 416, 530, 355, 636, 286, 257, 1664, 12, 4421, 3773, 3708, 438, 64, 22644, 326, 11023, 49589, 27278, 878, 35462, 6872, 340, 503, 416, 9645, 7886, 11, 465, 15530, 42414, 13, 220, 220, 632, 318, 2081, 326, 3805, 262, 523, 12, 7174, 3773, 3708, 11, 11023, 49589, 373, 10431, 284, 2222, 287, 1194, 42414, 11, 475, 262, 649, 582, 1625, 422, 1194, 7297, 286, 16455, 805, 11, 523, 612, 373, 645, 3224, 1575, 284, 262, 1664, 13, 220, 220, 843, 981, 7886, 11, 3584, 262, 15530, 42414, 11, 373, 407, 262, 18887, 530, 290, 373, 6928, 416, 257, 582, 287, 465, 44471, 357, 37, 726, 828, 257, 881, 4697, 42414, 373, 17383, 13, 220, 220, 383, 5466, 286, 3315, 6832, 284, 7519, 290, 11301, 857, 407, 1283, 262, 2099, 286, 3842, 287, 543, 257, 35444, 2939, 318, 17560, 13, 220, 220, 5414, 373, 2687, 287, 257, 4664, 2292, 379, 16455, 805, 7099, 621, 7886, 13, 628, 198, 18, 198, 4711, 389, 262, 6419, 15033, 416, 16455, 805, 475, 612, 389, 1854, 13, 220, 220, 7945, 262, 6491, 9687, 11, 16455, 805, 18141, 7886, 351, 13788, 2440, 4200, 621, 262, 1862, 582, 508, 6928, 683, 11, 772, 996, 7886, 338, 7674, 373, 8195, 46296, 284, 787, 1243, 4577, 329, 376, 726, 13, 220, 357, 2484, 2367, 286, 911, 3536, 410, 13, 3205, 30686, 1766, 1539, 860, 1485, 376, 13, 17, 67, 39260, 357, 22, 400, 21239, 13, 19891, 828]
10.0.2.15: b'966 F.2d 320\n59 Fair Empl.Prac.Cas. (BNA) 462,59 Empl. Prac. Dec. P 41,612Bobby G. PRICE, Plaintiff-Appellant, Cross-Appellee,v.MARSHALL ERDMAN & ASSOCIATES, INCORPORATED, and RonaldHalverson, Defendants-Appellees, Cross-Appellants.\nNos. 91-2303, 91-2373 and 91-3727.\nUnited States Court of Appeals,Seventh Circuit.\nArgued Feb. 12, 1992.Decided July 8, 1992.Rehearing and Rehearing En BancDenied Aug. 4, 1992.\n\nMichael R. Fox (argued) and Mary E. Kennelly, Fox, Fox, Schaefer & Gingras, Madison, Wis., for Bobby G. Price.\nJohn Sweeney (argued) and Dana J. Erlandsen, Melli, Walker, Pease & Ruhly, Madison, Wis., for Marshall Erdman & Associates, Inc. and Ronald Halverson.\nBefore BAUER, Chief Judge, POSNER, Circuit Judge, and FAIRCHILD, Senior Circuit Judge.\nPOSNER, Circuit Judge.\n\n\n1\nIn 1988, Marshall Erdman and Associates, which (among other activities) designs and builds medical buildings, fired Bobby Price, a salesman in its midwest division.   Price sued Erdman, and the head of the midwest division, Halverson, under the Age Discrimination in Employment Act, 29 U.S.C. \xc2\xa7\xc2\xa7 621 et seq.   Price obtained a jury award (against both defendants) of $178,700 in backpay, consisting of what he would have earned from Erdman between the date of his discharge and the date of trial, had he not been fired, minus what he did earn from others;  because the jury found the defendants\' violation of the age discrimination law to have been willful, the judge doubled this award.  29 U.S.C. \xc2\xa7 626(b).   The jury also awarded Price $750,000 in "front pay," representing the present value of the future earnings that Price would have had from Erdman, minus the present value of his anticipated future earnings in his current occupation of real estate broker.   Front pay is an alternative to reinstatement, which Price had requested but which the judge refused to order because of "mutual dislike and defendants\' continued opinion that plaintiff is incompetent."   The judge nevertheless threw out the jury\'s award of front pay on the grounds that the evidence of Price\'s lost future earnings was speculative and that an award of front pay would duplicate the award of liquidated damages (Congress\'s name for the doubling of damages to which the victim of a willful violation of the age discrimination law is entitled).   Price\'s cross-appeals challenge both the denial of front pay and the judge\'s award of only $82,000 in attorneys\' fees;  Price had sought $265,000.   He does not appeal from the denial of reinstatement.\n\n\n2\nThe evidence of age discrimination was thin;  so thin, the defendants argue, as to entitle them to judgment notwithstanding the verdict.   Price was 45 when he became a medical-buildings salesman for Erdman (his previous job had been as a construction supervisor for the company).   This was the same age as his boss, codefendant Halverson.   Two years after being hired, Price was fired, from the company as well as from the division, after some customers had complained about his inattention to detail and after Halverson had been told by his boss to reduce the sales force in the midwest division by one as part of a company-wide economy drive--a directive that Halverson protested before reluctantly carrying it out by firing Price, his newest salesman.   It is true that despite the so-called economy drive, Halverson was permitted to bring in another salesman, but the new man came from another division of Erdman, so there was no additional cost to the company.   And while Price, although the newest salesman, was not the youngest one and was replaced by a man in his twenties (Foy), a much older salesman was retained.   The sale of medical buildings to doctors and hospitals does not seem the type of activity in which a youthful image is valued.   Nor was anyone in a senior position at Erdman younger than Price.\n\n\n3\nThese are the facts stressed by Erdman but there are others.   Despite the customer complaints, Erdman credited Price with substantially higher sales than the young man who replaced him, even though Price\'s territory was reconfigured to make things easier for Foy.  (Shades of Shager v. Upjohn Co., 913 F.2d 398 (7th Cir.1990),'
10.0.2.15: Create dataset pile at scatter 0 with 2509150 documents
10.0.2.15: [1858, 318, 257, 4693, 286, 262, 41674, 287, 3597, 11, 543, 198, 325, 5232, 19501, 284, 262, 25595, 11, 290, 286, 543, 11, 6105, 11, 645, 7481, 198, 4360, 883, 286, 11871, 16084, 389, 6007, 11, 2080, 514, 11, 1243, 198, 1939, 684, 1304, 540, 287, 2405, 389, 15321, 416, 37196, 4263, 11, 543, 198, 26535, 606, 281, 4156, 6817, 3675, 644, 484, 460, 655, 306, 1624, 13, 198, 19093, 262, 21810, 11, 618, 12059, 257, 3344, 1871, 17002, 11, 416, 257, 2553, 6243, 198, 49283, 286, 21240, 258, 912, 290, 5538, 11, 2859, 2737, 287, 262, 9017, 286, 465, 7183, 11, 198, 1169, 2126, 286, 734, 18680, 18837, 542, 1571, 329, 13735, 13, 887, 262, 2499, 290, 198, 1322, 286, 1793, 389, 1165, 1049, 287, 2405, 11, 284, 9159, 286, 597, 6001, 3101, 198, 15603, 341, 13, 54, 721, 261, 15164, 517, 30522, 286, 1402, 1243, 416, 198, 359, 436, 9143, 22546, 422, 883, 543, 389, 3744, 26, 475, 262, 25595, 198, 69, 37971, 21290, 1049, 1243, 416, 40737, 606, 351, 883, 11, 198, 4758, 287, 674, 31850, 11, 389, 20861, 290, 46299, 13, 1881, 4554, 503, 286, 198, 21834, 543, 1244, 307, 4750, 11, 318, 326, 4988, 41674, 10066, 286, 262, 198, 22930, 3202, 11, 1870, 477, 262, 2583, 286, 9538, 2236, 307, 26306, 26, 290, 198, 1169, 27636, 2236, 307, 11686, 1978, 355, 257, 10743, 26, 290, 477, 511, 2583, 198, 49271, 2121, 866, 355, 262, 12835, 277, 6765, 400, 572, 422, 262, 17793, 11, 290, 355, 257, 7463, 198, 5647, 422, 262, 2336, 12, 21048, 7, 39443, 9520, 4974, 25, 19, 737, 383, 29748, 11, 618, 38931, 351, 257, 32271, 5761, 11, 198, 27427, 728, 262, 976, 1807, 11, 2048, 287, 262, 976, 2456, 13, 1870, 262, 5788, 286, 9538, 3214, 12722, 262, 4534, 11, 772, 355, 198, 64, 2336, 12, 21048, 26217, 607, 1418, 524, 306, 2336, 82, 11, 618, 673, 318, 27821, 286, 257, 18680, 198, 7972, 25, 290, 262, 27636, 24057, 355, 257, 10743, 618, 340, 318, 11686, 198, 45525, 7, 3041, 626, 341, 718, 25, 1485, 11, 1478, 737, 8013, 5107, 286, 5408, 389, 5033, 58, 1676, 525, 26, 16686, 284, 262, 2095, 286, 60, 262, 25788, 198, 1659, 262, 1049, 1793, 11, 878, 4150, 262, 3580, 1022, 262, 1049, 290, 262, 198, 17470, 287, 674, 8492, 11, 318, 36572, 515, 13, 554, 2399, 1570, 11, 477, 262, 17622, 198, 1659, 262, 4534, 389, 475, 355, 257, 4268, 543, 8953, 33755, 422, 262, 19236, 11, 198, 273, 355, 262, 8977, 543, 1190, 3080, 284, 262, 5236, 7, 39443, 9520, 2319, 25, 1314, 828, 1231, 13891, 663, 29163, 13, 1629, 262, 976, 198, 2435, 11, 262, 21654, 286, 777, 27128, 11, 523, 880, 16662, 284, 1013, 633, 198, 1169, 11293, 286, 262, 10787, 11, 318, 8871, 290, 3489, 284, 262, 9016, 198, 11128, 330, 871, 13, 1002, 9, 39, 12057, 393, 9, 53, 343, 37718, 550, 587, 1965, 284, 6901, 262, 17596, 295, 290, 198, 10760, 286, 262, 1176, 286, 1793, 11, 287, 35041, 278, 290, 28641, 2399, 5775, 11, 484, 198, 19188, 2192, 423, 2248, 8167, 329, 257, 985, 576, 17338, 4490, 13, 887, 314, 198, 29482, 1808, 611, 484, 561, 423, 1807, 286, 262, 2939, 287, 616, 2420, 11, 198, 2016, 4844, 460, 307, 517, 38084, 286, 10517, 11331, 37064, 16866, 11, 393, 286, 262, 198, 68, 589, 351, 543, 340, 318, 13013, 13, 1544, 2236, 14470, 606, 287, 5207, 588, 257, 1787, 353, 338, 198, 1158, 741, 7, 12016, 38182, 362, 25, 24, 8, 198, 198, 464, 2168, 286, 262, 22674, 11, 356, 423, 16537, 198, 5936, 3089, 11, 318, 845, 3218, 290, 4950, 198, 13, 337, 7597, 3539, 39, 198, 3372, 1631, 319, 1029, 11, 290, 2722, 13201, 329, 1450, 13, 464, 717, 290, 7103, 12921, 286, 2399, 409, 2501, 341, 198, 259, 674, 3450, 271, 262, 9207, 286, 262, 23244, 13, 6423, 5679, 262, 3772, 290, 13205, 4588, 286, 198, 1169, 23244, 319, 883, 508, 36308, 3328, 340, 13, 2437, 4950, 389, 262, 3625, 286, 606, 326, 32950, 777, 198, 4743, 324, 29770, 654, 13, 464, 1306, 10066, 792, 942, 290, 8477, 663, 7667, 198, 33723, 851, 464, 2128, 1816, 6071, 656, 477, 262, 4534, 13, 464, 5471, 39217, 416, 340, 318, 612, 3417, 11, 198, 292, 403, 42275, 11, 4162, 466, 262, 12308, 831, 14404, 30, 26, 355, 500, 4812, 723, 11, 262, 4453, 22051, 379, 2399, 38642, 364, 26, 1544, 10718, 2402, 2399, 2296, 21985, 19262, 11, 290, 4587, 1460, 511, 198, 1078, 1791, 82, 13, 383, 2457, 2071, 286, 511, 8805, 6625, 11, 24571, 10802, 290, 16866, 11, 318, 262, 2426, 286, 262, 18527, 314, 423, 1100, 11, 543, 198, 46012, 3565, 329, 262, 1969, 286, 262, 1218, 636, 286, 262, 5574, 1352, 952, 13, 2399, 5775, 2236, 42531, 11, 2399, 7526, 2236, 307, 198, 27718, 290, 2784, 76, 515, 13, 843, 788, 477, 11386, 12661, 9791, 2236, 198, 22179, 287, 257, 3496, 286, 15499, 11, 39, 6765, 2290, 31558, 11, 329, 262, 4453, 1793, 31816, 541, 33715, 198, 260, 570, 2788, 13, 198, 198, 464, 11545, 42712, 507, 286, 366, 13395, 351, 257, 15299, 286, 198, 1934, 1, 290, 366, 67, 2140, 287, 5207, 1, 1950, 3016, 262, 976, 198, 485, 64, 13, 887, 355, 8057, 679, 318, 531, 7332, 2261, 6653, 5775, 4480, 257, 15299, 286, 6953, 7, 3041, 626, 341, 678, 25, 1314, 737, 314, 2236, 29107, 3589, 286, 428, 12291, 11, 287, 1502, 284, 198, 26535, 345, 257, 517, 1844, 1570, 286, 262, 35483, 5219, 1659, 25591, 1929, 2238, 381, 577, 198, 44, 7597, 3539, 39, 198, 392, 6653, 15708, 3438, 13, 9360, 377, 395, 10024, 25579, 4480, 64, 15299, 1659, 1934, 11, 392, 1456, 8499, 39, 413, 359, 42460, 18855, 287, 5207, 588, 257, 1787, 353, 338, 8837, 13, 5756, 514, 11, 8117, 754, 11, 44353, 25, 198, 198, 40, 13, 198, 198, 2437, 262, 4453, 198, 44, 7597, 3539, 39, 198, 38785, 625, 848, 268, 48324, 290, 9710, 4559, 49279, 287, 262, 198, 25579, 1204, 13, 1119, 2230, 357, 259, 23469, 8, 284, 8399, 422, 2399, 2426, 295, 13, 198, 2990, 12856, 2399, 11386, 481, 13, 1119, 11148, 284, 9199, 284, 2399, 10861]
10.0.2.15: b'There is a species of the sublime in writing, which\nseems peculiar to the Scripture, and of which, properly, no subjects\nbut those of divine revelation are capable, With us, things\ninconsiderable in themselves are elevated by splendid images, which\ngive them an apparent importance beyond what they can justly claim.\nThus the poet, when describing a battle among bees, by a judicious\nselection of epithets and figures, excites in the minds of his readers,\nthe idea of two mighty armies contending for empire. But the works and\nways of God are too great in themselves, to admit of any heightening\nrepresentation.Weconceive more forcibly of small things by\nillustrations borrowed from those which are greater; but the Scripture\nfrequently illustrates great things by contrasting them with those,\nwhich in our estimation, are trivial and feeble. One instance out of\nmany which might be mentioned, is that truly sublime passage of the\nprophet,And all the host of heaven shall be dissolved; and\nthe heavens shall be rolled together as a scroll; and all their host\nshall fall down as the leaf falleth off from the vine, and as a falling\nfig from the fig-tree(Isaiah 34:4). The Apostle, when favoured with a heavenly vision,\nintroduces the same thought, almost in the same words.And the stars of heaven fell unto the earth, even as\na fig-tree casts her untimely figs, when she is shaken of a mighty\nwind: and the heavens departed as a scroll when it is rolled\ntogether(Revelation 6:13, 14). Such forms of expression are becoming[proper; belonging to the character of] the Majesty\nof the great God, before whom the difference between the great and the\nsmall in our judgment, is annihilated. In His view, all the inhabitants\nof the earth are but as a drop which falls unnoticed from the bucket,\nor as the dust which cleaves to the balance(Isaiah 40:15), without affecting its equilibrium. At the same\ntime, the simplicity of these illustrations, so well suited to confound\nthe pride of the wise, is striking and obvious to the lowest\ncapacities. If*Homer or*Virgil had been asked to describe the exertion and\neffect of the power of God, in subduing and punishing His enemies, they\nwould probably have laboured for a simile sufficiently grand. But I\nmuch question if they would have thought of the image in my text,\nthough none can be more expressive of utter irreparable ruin, or of the\nease with which it is accomplished.He shall dash them in pieces like a potter\'s\nvessel(Psalm 2:9)\n\nThe series of the passages, we have lately\nconsidered, is very regular and beautiful\n. MESSIAH\nascended on high, and received gifts for men.The first and immediate consequence of His exaltation\nin our natureis the publication of the Gospel.Then follows the happy and beneficial influence of\nthe Gospel on those who thankfully receive it.How beautiful are the feet of them that preach these\nglad tidings.The next passage secures and describes its extensive\nprogress \xe2\x80\x94The sound went forth into all the earth.The opposition awakened by it is there described,\nasunreasonable, Why do the Heathen rage?; asineffectual, the Lord laughs at His opposers;He sits upon His immovable throne, and derides their\nattempts. The final issue of their mad resistance,their confusion and ruin, is the subject of the verse I have read, which\nprepares for the close of the second part of theOratorio. His enemies shall perish, His Kingdom shall be\nestablished and consummated. And then all holy intelligent beings shall\njoin in a song of triumph,Hallelujah, for the Lord God Omnipotent\nreigneth.\n\nThetwoexpressions of "breaking with a rod of\niron" and "dashing in pieces" suggest nearly the same\nidea. But as elsewhere He is said toruleHis enemieswith a rod of iron(Revelation 19:15). I shall avail myself of this variation, in order to\ngive you a more complete view of the dreadfulstateofthosewhooppose\nMESSIAH\nandHisKingdom.Herulesthematpresentwitha rodofiron,andhereafterHewilldashthem in pieces like a potter\'s vessel.Let us,therefore,consider:\n\nI.\n\nHow the Lord\nMESSIAH\nrules over impenitent and obstinate sinners in the\npresent life. They attempt (in vain) to withdraw from His subjection.\nThey oppose His holy will. They refuse to submit to His golden'
10.0.2.15: [43198, 839, 5408, 286, 14383, 25276, 776, 7935, 583, 1140, 312, 589, 287, 5882, 12, 15200, 504, 29181, 30118, 13623, 2747, 8802, 1335, 284, 37439, 290, 32070, 6086, 13, 198, 464, 5882, 12, 15200, 504, 48932, 4227, 318, 257, 30118, 14022, 326, 8592, 49718, 37439, 290, 11, 4191, 11, 25319, 32070, 6086, 13, 23413, 11, 18801, 15317, 10507, 5768, 287, 5882, 12, 15200, 504, 48932, 4227, 300, 1191, 547, 3402, 284, 307, 20807, 6692, 284, 262, 2478, 286, 37439, 13, 4362, 5322, 25276, 776, 7935, 290, 25276, 776, 7935, 12, 5363, 29120, 389, 1900, 284, 711, 1593, 9176, 287, 19824, 6625, 284, 6801, 6147, 28152, 11, 356, 5295, 262, 2974, 286, 5322, 25276, 776, 7935, 290, 25276, 776, 7935, 12, 5363, 29120, 287, 3598, 1180, 21379, 286, 5882, 12, 15200, 504, 48932, 290, 1630, 5882, 12, 15200, 504, 2449, 448, 72, 13623, 13, 3226, 262, 29120, 11068, 11, 691, 47585, 1512, 25276, 776, 7935, 583, 1140, 312, 589, 373, 36841, 11832, 287, 5882, 12, 15200, 504, 48932, 13623, 13, 2671, 315, 776, 7935, 583, 1140, 312, 589, 2695, 287, 262, 14383, 286, 5882, 12, 15200, 504, 48932, 13623, 373, 5014, 7441, 7192, 4, 290, 7618, 4, 286, 262, 1630, 3815, 379, 860, 357, 11265, 3800, 828, 678, 357, 330, 1133, 37439, 3800, 8, 290, 2681, 357, 354, 4565, 37439, 3800, 8, 266, 74, 286, 2479, 11, 8148, 13, 8342, 12, 2436, 313, 3781, 4602, 326, 31228, 25897, 2974, 286, 25276, 776, 7935, 583, 1140, 312, 589, 287, 262, 300, 1191, 286, 5882, 12, 15200, 504, 48932, 13623, 547, 546, 2319, 4, 286, 262, 1630, 2974, 13, 383, 3842, 286, 25276, 776, 7935, 311, 12, 39437, 589, 373, 4622, 11832, 287, 262, 300, 1191, 286, 5882, 12, 15200, 504, 48932, 13623, 13, 2312, 1366, 1950, 326, 262, 14383, 286, 262, 5882, 12, 15200, 504, 48932, 4227, 318, 13455, 6861, 1028, 4075, 11863, 4693, 11, 262, 3227, 286, 543, 318, 13105, 287, 262, 4931, 286, 6992, 15317, 13, 2671, 315, 776, 7935, 12, 445, 4782, 589, 3842, 287, 262, 300, 1191, 286, 5882, 12, 15200, 504, 48932, 13623, 3220, 284, 26753, 4, 290, 22613, 4, 286, 262, 1630, 2974, 379, 678, 290, 2681, 266, 74, 286, 2479, 11, 8148, 13, 1400, 2383, 2458, 547, 6515, 287, 262, 3842, 286, 34236, 12, 4743, 315, 321, 2645, 948, 4169, 500, 7419, 3202, 589, 393, 287, 262, 2695, 286, 2472, 5322, 25276, 776, 7935, 287, 262, 14383, 286, 262, 5882, 12, 15200, 504, 48932, 4227, 12195, 6242, 18601, 10659, 7579, 4944, 34, 11617, 5161, 8646, 21881, 5258, 8]
10.0.2.15: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.15: b'Decreased expression of liver glutathione peroxidase in Long-Evans cinnamon mutant rats predisposed to hepatitis and hepatoma.\nThe Long-Evans Cinnamon rat is a mutant strain that contracts hereditary hepatitis and, eventually, spontaneous hepatoma. Recently, abnormal copper accumulations in Long-Evans Cinnamon rat livers were shown to be genetically linked to the development of hepatitis. Because reduced glutathione and glutathione-related enzymes are known to play important roles in cellular resistance to transition metal toxicity, we determined the levels of reduced glutathione and glutathione-related enzymes in seven different tissues of Long-Evans Cinnamon and control Long-Evans Agouti rats. Of the enzymes examined, only hepatic glutathione peroxidase was markedly decreased in Long-Evans Cinnamon rats. Glutathione peroxidase content in the liver of Long-Evans Cinnamon rats was 39%, 53% and 58% of the control values at 9 (normal stage), 19 (acute hepatitis stage) and 27 (chronic hepatitis stage) wk of age, respectively. Northern-blot analysis revealed that messenger RNA levels of glutathione peroxidase in the livers of Long-Evans Cinnamon rats were about 40% of the control levels. The activity of glutathione S-transferase was slightly decreased in the livers of Long-Evans Cinnamon rats. These data suggest that the liver of the Long-Evans Cinnamon rat is poorly protected against active oxygen species, the production of which is enhanced in the presence of excess copper. Glutathione-reductase activity in the livers of Long-Evans Cinnamon rats increased to 166% and 148% of the control levels at 19 and 27 wk of age, respectively. No significant changes were observed in the activity of gamma-glutamylcysteine synthetase or in the content of total reduced glutathione in the liver of the Long-Evans Cinnamon rat.(ABSTRACT TRUNCATED AT 250 WORDS)'
10.0.2.15: [51, 21870, 17130, 290, 38971, 1117, 9386, 10712, 2465, 290, 10345, 8055, 706, 25115, 3632, 5095, 287, 10693, 13, 198, 51, 388, 273, 27576, 37172, 5766, 12, 26591, 357, 51, 21870, 26591, 8, 290, 38971, 389, 18268, 706, 25115, 3632, 5095, 357, 51, 3483, 1776, 2158, 11, 511, 10345, 9176, 389, 17503, 306, 7247, 13, 8554, 6856, 35001, 2928, 357, 4093, 40, 8, 290, 10693, 40043, 287, 309, 21870, 26591, 11, 38971, 11, 393, 1111, 357, 51, 21870, 26591, 14, 37, 292, 12, 16327, 828, 356, 43714, 326, 309, 21870, 26591, 290, 38971, 17408, 1117, 9386, 9233, 309, 3483, 287, 257, 30806, 5642, 13, 27492, 351, 4295, 2099, 357, 39386, 828, 309, 21870, 26591, 14, 37, 292, 12, 16327, 10693, 550, 6596, 5584, 2854, 422, 352, 284, 604, 1528, 357, 47, 27, 15, 13, 2713, 828, 6596, 21739, 4088, 12673, 379, 807, 284, 1478, 1528, 357, 47, 27, 15, 13, 2713, 828, 290, 11832, 3632, 10287, 295, 2546, 379, 362, 290, 718, 2745, 706, 12624, 40, 357, 47, 27, 15, 13, 2713, 737, 9985, 287, 309, 21870, 26591, 14, 37, 292, 12, 16327, 10693, 422, 1554, 18569, 2770, 290, 5584, 22729, 373, 17687, 416, 8195, 301, 2738, 351, 44759, 42483, 309, 21870, 26591, 878, 12624, 40, 11, 290, 309, 21870, 26591, 12, 16327, 10693, 17169, 3098, 12, 37, 292, 26106, 392, 30869, 550, 6596, 21739, 4088, 12673, 9051, 12470, 5716, 41281, 10693, 357, 47, 27, 15, 13, 2713, 737, 309, 388, 273, 27576, 37172, 5766, 12, 26591, 14, 37, 292, 12, 16327, 10693, 550, 11832, 262, 3146, 286, 35001, 4778, 351, 458, 292, 7617, 368, 2611, 2465, 379, 718, 289, 357, 47, 27, 15, 13, 2713, 9051, 41281, 828, 290, 5322, 17593, 1138, 439, 404, 35574, 589, 12, 24, 3842, 287, 6686, 3632, 379, 4764, 290, 7724, 289, 706, 12624, 40, 13, 554, 40620, 10693, 16164, 284, 12624, 40, 11, 8513, 30725, 286, 309, 21870, 26591, 290, 38971, 39376, 13205, 3048, 319, 1554, 18569, 1435, 290, 21739, 4088, 12673, 287, 28003, 357, 16885, 350, 27, 15, 13, 2713, 9051, 41281, 828, 9524, 326, 262, 13205, 3048, 286, 309, 21870, 26591, 14, 37, 292, 30725, 743, 307, 7748, 13, 383, 1366, 1950, 326, 30806, 22049, 22963, 16862, 416, 309, 21870, 26591, 290, 38971, 711, 28992, 9176, 287, 262, 3108, 25908, 286, 309, 3483, 11, 290, 326, 47685, 11701, 33218, 286, 309, 21870, 26591, 14, 37, 292, 743, 307, 5337, 21546, 6670, 284, 4179, 30232, 16304, 3609, 287, 1751, 290, 6490, 351, 6049, 309, 3483, 13]
10.0.2.15: b'TNF alpha and Fas mediate tissue damage and functional outcome after traumatic brain injury in mice.\nTumor necrosis factor-alpha (TNFalpha) and Fas are induced after traumatic brain injury (TBI); however, their functional roles are incompletely understood. Using controlled cortical impact (CCI) and mice deficient in TNFalpha, Fas, or both (TNFalpha/Fas-/-), we hypothesized that TNFalpha and Fas receptor mediate secondary TBI in a redundant manner. Compared with wild type (WT), TNFalpha/Fas-/- mice had improved motor performance from 1 to 4 days (P<0.05), improved spatial memory acquisition at 8 to 14 days (P<0.05), and decreased brain lesion size at 2 and 6 weeks after CCI (P<0.05). Protection in TNFalpha/Fas-/- mice from histopathological and motor deficits was reversed by reconstitution with recombinant TNFalpha before CCI, and TNFalpha-/- mice administered anti-Fas ligand antibodies had improved spatial memory acquisition versus similarly treated WT mice (P<0.05). Tumor necrosis factor-alpha/Fas-/- mice had decreased the numbers of cortical cells with plasmalemma damage at 6 h (P<0.05 versus WT), and reduced matrix metalloproteinase-9 activity in injured brain at 48 and 72 h after CCI. In immature mice subjected to CCI, genetic inhibition of TNFalpha and Fas conferred beneficial effects on histopathology and spatial memory acquisition in adulthood (both P<0.05 versus WT), suggesting that the beneficial effects of TNFalpha/Fas inhibition may be permanent. The data suggest that redundant signaling pathways initiated by TNFalpha and Fas play pivotal roles in the pathogenesis of TBI, and that biochemical mechanisms downstream of TNFalpha/Fas may be novel therapeutic targets to limit neurological sequelae in children and adults with severe TBI.'
10.0.2.15: [16, 13, 6065, 4341, 678, 1411, 517, 621, 1466, 466, 13, 198, 198, 1870, 484, 4341, 513, 1411, 517, 319, 9912, 9735, 621, 1466, 11, 1864, 284, 2267, 416, 262, 2614, 12, 69, 14149, 4081, 25282, 13, 785, 290, 262, 2351, 26702, 11937, 13, 366, 1026, 338, 3499, 326, 1450, 389, 4143, 407, 3114, 379, 355, 5749, 4341, 364, 621, 1466, 553, 1139, 317, 4703, 5172, 12627, 11, 1772, 286, 1338, 298, 25, 4942, 10840, 82, 286, 257, 39109, 3060, 713, 13, 366, 40, 892, 530, 286, 262, 1243, 326, 7622, 262, 7918, 546, 1466, 4581, 517, 621, 1450, 6776, 318, 326, 1450, 1690, 18507, 511, 9735, 287, 10345, 3709, 960, 6, 5812, 11, 339, 5839, 257, 1097, 13679, 960, 273, 7808, 511, 9735, 13870, 287, 45578, 13, 632, 338, 7069, 329, 1466, 284, 910, 326, 17292, 9735, 318, 257, 20005, 11, 3584, 314, 760, 257, 1178, 508, 423, 3088, 526, 198, 198, 464, 3050, 25282, 6358, 25, 5573, 829, 3691, 13, 6405, 3691, 13, 3397, 13, 5338, 16887, 517, 30, 5357, 23811, 7802, 25, 838, 5849, 22770, 34308, 329, 3050, 11, 257, 2351, 26702, 11937, 5526, 13, 198, 198, 17, 13, 4380, 287, 9533, 11, 3936, 4341, 1440, 1661, 355, 881, 1637, 355, 661, 287, 8488, 13, 198, 198, 40245, 318, 1363, 284, 262, 4094, 4341, 364, 287, 262, 1499, 960, 1169, 2811, 6623, 286, 3936, 6, 7270, 49842, 20385, 625, 720, 3134, 11, 830, 583, 614, 11, 290, 326, 1595, 470, 2291, 13682, 393, 5602, 13, 9533, 318, 3940, 416, 7130, 912, 14597, 11, 7943, 290, 2986, 5264, 11, 3442, 11, 1864, 284, 16905, 14102, 416, 25282, 13, 785, 1912, 319, 6641, 22895, 329, 9735, 11, 6600, 11, 7722, 11, 9358, 11, 11409, 11, 290, 9739, 13, 8488, 364, 4341, 262, 1551, 11, 1088, 720, 1433, 11, 7029, 583, 614, 11, 351, 968, 38244, 290, 3905, 28380, 826, 287, 262, 3504, 13, 198, 198, 464, 3050, 25282, 6358, 25, 383, 1679, 1353, 12, 2777, 1571, 4736, 287, 262, 471, 13, 50, 13, 198, 198, 18, 13, 17897, 16565, 1542, 13, 19, 1411, 517, 1637, 284, 11016, 621, 17194, 466, 13, 198, 198, 4821, 284, 29141, 2059, 1597, 6240, 13514, 327, 13, 17704, 11, 3025, 1492, 23474, 428, 3785, 11, 5940, 1605, 12503, 1577, 720, 16, 11, 8054, 257, 614, 319, 2811, 11, 3688, 351, 262, 720, 16, 11, 24403, 1813, 416, 7270, 12503, 13, 17704, 6673, 326, 3399, 508, 1975, 262, 471, 13, 50, 13, 1230, 16887, 366, 18820, 1310, 1637, 319, 9490, 1, 357, 68, 13, 70, 1539, 17194, 8, 389, 1342, 1884, 621, 883, 508, 1975, 262, 1230, 16887, 366, 18820, 881, 1637, 319, 9490, 1, 357, 68, 13, 70, 1539, 12471, 8, 284, 1441, 3131, 1487, 284, 5003, 3183, 393, 1577, 1637, 284, 10463, 3425, 4993, 8116, 13, 198, 198, 29874, 327, 13, 17704, 13, 5338, 16123, 327, 3565, 30, 383, 4198, 14619, 14056, 7994, 3082, 11857, 378, 7162, 26185, 13, 968, 1971, 25, 14392, 13661, 11, 4793, 11, 2443, 4570, 290, 2443, 7632, 13, 198, 198, 19, 13, 4380, 508, 9835, 6128, 2691, 4341, 1987, 13, 21, 1411, 517, 621, 661, 508, 6128, 287, 7000, 13, 198, 198, 4821, 284, 257, 2351, 26702, 11937, 5526, 11, 2691, 30303, 635, 923, 511, 9912, 9735, 2961, 621, 17214, 12, 392, 12, 30171, 283, 30303, 290, 389, 604, 1411, 517, 1884, 284, 2822, 1243, 329, 2405, 981, 9912, 9735, 319, 262, 3433, 13, 9754, 883, 508, 898, 4451, 9512, 11, 2681, 1411, 481, 779, 606, 284, 787, 9912, 13339, 13, 366, 14439, 9735, 318, 257, 19972, 329, 552, 22220, 14456, 553, 22145, 23540, 3035, 33940, 11, 1772, 286, 1675, 11763, 393, 1892, 284, 11763, 25, 4162, 775, 29425, 8548, 290, 1374, 284, 13707, 13, 366, 2025, 5177, 414, 318, 257, 1263, 5766, 11, 780, 618, 345, 6128, 2691, 11, 645, 530, 468, 284, 766, 703, 881, 345, 821, 4581, 393, 644, 345, 821, 4581, 340, 319, 526, 198, 198, 16186, 26702, 11937, 3050, 22770, 13084, 357, 8726, 318, 284, 1336, 2050, 8, 198, 198, 20, 13, 4380, 422, 14493, 389, 262, 3277, 338, 4094, 4341, 364, 11, 503, 2777, 1571, 5575, 272, 504, 416, 13539, 1411, 13, 198, 198, 42347, 286, 262, 5665, 286, 9309, 11, 13708, 11, 290, 3442, 389, 262, 4916, 338, 1218, 11, 2368, 11, 290, 5544, 4094, 4341, 364, 960, 439, 2585, 1900, 329, 12577, 257, 43244, 12263, 13, 1892, 523, 18311, 11, 810, 262, 2811, 6623, 16887, 691, 720, 1983, 11, 830, 583, 614, 13, 2688, 5283, 1547, 4341, 262, 845, 1551, 11, 1864, 284, 428, 2050, 11, 257, 5019, 720, 1731, 11, 4059, 13, 198, 198, 464, 3050, 25282, 6358, 25, 16328, 6641, 4581, 416, 1181, 198, 198, 21, 13, 43797, 11, 5650, 11, 24249, 11, 290, 10637, 6490, 4341, 2608, 1411, 517, 319, 9912, 13201, 621, 24026, 82, 13, 198, 198, 818, 257, 10026, 21365, 5526, 326, 373, 5952, 938, 614, 290, 3025, 39469, 389, 523, 14559, 355, 284, 4079, 6414, 7572, 11, 3261, 1411, 286, 9916, 6490, 20840, 531, 484, 6027, 284, 4341, 517, 326, 614, 621, 262, 2180, 614, 319, 9912, 13201, 329, 12772, 11, 3688, 284, 691, 642, 1411, 286, 24026, 6490, 20840, 13, 198, 198, 37701, 37306, 82, 42606, 3125, 30011, 2569, 546, 4463, 1817, 357, 10531, 828, 14102, 416, 10026, 21365, 290, 370, 578, 694, 12, 5377, 1443, 14620, 357, 12853, 2050, 407, 1695, 2691, 8, 198, 198, 22, 13, 3399, 9722, 2026, 284, 6135, 4341, 8915, 1411, 517, 1637, 583, 614, 621, 3399, 9722, 1248, 284, 1679, 13, 198, 198, 3581, 1315, 6705, 41835, 329, 2619, 3217, 464, 2811, 2026, 284, 6135, 614, 1468, 16887, 625, 5403, 644, 262, 2811, 1248, 284, 1679, 614, 1468, 16887, 319, 11409, 11, 290, 3016, 1115, 1661, 355, 881, 319, 5627, 13, 5501, 2479, 1448, 2659, 85, 444, 510, 663, 22895, 10338, 11, 351, 262, 7099, 900, 477, 27123, 2310, 1411, 286, 511, 10908, 4581, 284, 2057, 290, 4144, 11, 517, 621, 597, 584, 2479, 1448, 13, 2893, 477, 584, 2628, 31935, 546, 1987, 1411, 286, 511, 10908, 4581, 284, 9735, 357, 64, 6536, 326, 3407, 8242, 11, 10012, 11, 17075, 11, 45578, 11, 290, 584, 20813, 828, 23481, 31935, 262, 1551]
10.0.2.15: b'1. Men spend 19 percent more than women do.\n\nAnd they spend 3 percent more on holiday shopping than women, according to research by the personal-finance firm Bundle.com and the National Retail Federation. "It\'s interesting that men are generally not looked at as bigger spenders than women," says Avis Cardella, author of Spent: Memoirs of a Shopping Addict. "I think one of the things that keeps the myth about women spending more than men alive is that men often couch their shopping in functional items\xe2\x80\x94\'Oh, he bought a car!\'\xe2\x80\x94or hide their shopping habits in hobbies. It\'s harder for women to say that shoe shopping is a hobby, although I know a few who have tried."\n\nThe 2010 Bundle Report: Singles vs. married vs. parents. Who spends more? AND Ellen Davis: 10 Top Holiday Trends for 2010, a National Retail Federation survey.\n\n2. People in Austin, Texas spend four times as much money as people in Detroit.\n\nAustin is home to the biggest spenders in the country\xe2\x80\x94the average resident of Texas\' liberal enclave blows over $67,000 per year, and that doesn\'t include mortgage or rent. Austin is followed by Scottsdale, Arizona and San Jose, California, according to rankings compiled by Bundle.com based on household expenditures for shopping, eating, drinking, transportation, healthcare, and entertainment. Detroiters spend the least, around $16,400 per year, with New Yorkers and Angelenos right in the middle.\n\nThe 2010 Bundle Report: The 25 top-spending cities in the U.S.\n\n3. Conservatives donate 30.4 percent more money to charity than liberals do.\n\nAccording to Syracuse University business professor Arthur C. Brooks, whose book cites this figure, conservative American households give $1,600 a year on average, compared with the $1,227 given by liberal households. Brooks adds that Americans who believe the U.S. government spends "too little money on welfare" (e.g., liberals) are less likely than those who believe the government spends "too much money on welfare" (e.g., conservatives) to return extra change to cashiers or give money to homeless panhandlers.\n\nArthur C. Brooks. Who Really Cares? The Surprising Truth About Compassionate Conservatism. New York: Basic Books, 2006, page 36 and page 57.\n\n4. People who consistently shop online spend 24.6 percent more than people who shop in stores.\n\nAccording to a National Retail Federation survey, online shoppers also start their holiday shopping earlier than brick-and-mortar shoppers and are 4 percent more likely to buy things for themselves while holiday shopping on the Net. Among those who own smart phones, 27 percent will use them to make holiday purchases. "Online shopping is a magnet for compulsive buyers," warns psychologist April Benson, author of To Buy or Not to Buy: Why We Overshop and How to Stop. "Anonymity is a big factor, because when you shop online, no one has to see how much you\'re spending or what you\'re spending it on."\n\nNational Retail Federation 2010 Holiday Survey (link is to full study)\n\n5. People from Connecticut are the nation\'s biggest spenders, outspending Montanans by 112 percent.\n\nResidents of the District of Columbia, Hawaii, and California are the USA\'s second, third, and fourth biggest spenders\xe2\x80\x94all states known for encouraging a pricey lifestyle. Not so Montana, where the average resident spends only $27,000 per year. West Virginians spend the very least, according to this study, a mere $24,500.\n\nThe 2010 Bundle Report: Annual household spending by state\n\n6. Lesbian, gay, bisexual, and transgender adults spend 26 percent more on holiday gifts than heterosexuals.\n\nIn a Harris Interactive survey that was conducted last year and whose contrasts are so steep as to reflect consistent patterns, 31 percent of LGBT adults surveyed said they planned to spend more that year than the previous year on holiday gifts for relatives, compared to only 5 percent of heterosexual adults surveyed.\n\nLGBT Households Remain More Optimistic about Finances (2009), compiled by Harris Interactive and Witeck-Combs Communications (full study not available online)\n\n7. Americans aged 50 to 65 spend 74 percent more money per year than Americans aged 18 to 25.\n\n\xe2\x80\xa2 15 Best Stores for Black FridayThe average 50 to 65 year old spends over twice what the average 18 to 25 year old spends on healthcare, and nearly three times as much on housing. Each age group divvies up its expenditures differently, with the younger set allocating 21 percent of their everyday spending to food and drink, more than any other age group. While all other groups allocate about 24 percent of their everyday spending to shopping (a category that includes clothes, shoes, electronics, hobbies, and other merchandise), seniors allocate the least'
10.0.2.15: [7089, 1878, 5362, 4912, 198, 198, 464, 2930, 1878, 5362, 4912, 468, 734, 1688, 30485, 286, 1597, 17894, 13, 383, 717, 530, 318, 546, 1597, 8136, 543, 28800, 11087, 6128, 3341, 326, 389, 973, 416, 3756, 11394, 8688, 13, 383, 1218, 30042, 318, 2391, 329, 534, 1363, 13, 2312, 3186, 2291, 20897, 290, 11087, 11915, 3709, 13, 198, 198, 3198, 286, 2930, 1878, 5362, 338, 4493, 287, 2031, 198, 198, 818, 262, 2159, 286, 18162, 357, 817, 959, 12, 26552, 18287, 36888, 828, 262, 4031, 373, 284, 1577, 262, 1720, 10470, 517, 6382, 1042, 290, 7506, 284, 4079, 262, 1664, 338, 8876, 286, 5660, 2741, 6252, 772, 1365, 13, 198, 198, 2504, 318, 655, 530, 286, 1811, 4493, 326, 262, 4508, 1055, 1878, 5362, 468, 4166, 355, 257, 6308, 8705, 290, 1486, 1664, 13, 198, 198, 42419, 33728, 2930, 1878, 5362, 198, 198, 12041, 2930, 1878, 5362, 338, 3367, 33728, 2930, 1878, 5362, 468, 587, 11149, 262, 1597, 329, 1811, 4647, 1541, 13, 679, 925, 262, 2551, 284, 1011, 262, 1597, 284, 9764, 9071, 287, 3648, 329, 12948, 3349, 287, 262, 7229, 8211, 3814, 13, 198, 198, 5246, 13, 2930, 1878, 5362, 2957, 262, 1664, 284, 649, 1597, 17894, 351, 465, 11949, 290, 1597, 13572, 13, 198, 198, 7089, 1878, 5362, 379, 1898, 29917, 11, 4486, 198, 198, 14398, 29917, 318, 262, 2159, 338, 1400, 13, 352, 26702, 9601, 7011, 351, 7982, 11, 830, 285, 31185, 2010, 17127, 2272, 290, 362, 11, 4059, 7316, 6742, 422, 1088, 3126, 2678, 340, 318, 1111, 262, 5182, 2617, 353, 290, 281, 35669, 1321, 290, 6946, 3859, 329, 477, 2551, 12, 6620, 287, 6308, 13, 198, 198, 12041, 2930, 1878, 5362, 402, 2022, 39, 1222, 1766, 13, 42, 38, 468, 587, 46017, 379, 428, 995, 3800, 329, 1811, 4647, 13, 198, 198, 1135, 670, 351, 7097, 1827, 1083, 508, 7716, 257, 6937, 5202, 286, 13097, 10838, 290, 257, 845, 19701, 8705, 1074, 287, 4486, 13, 775, 477, 2648, 262, 976, 7506, 329, 4441, 3748, 3186, 329, 674, 1597, 4887, 290, 329, 345, 379, 1363, 13, 198, 198, 23067, 263, 3186, 329, 534, 1363, 416, 1055, 1878, 5362, 1222, 1597, 8136, 329, 534, 1664, 13]
10.0.2.15: b"Serafini Group\n\nThe Serafini Group has two major pillars of business segments. The first one is about business solutions which comprises interior shop systems that are used by leading brands worldwide. The second pillar is simply for your home. These products include exterior and interior designer items.\n\nOne of Serafini's projects in Europe\n\nIn the World of Running (Thier-Galerie Dortmund), the aim was to give the product presentation more dynamism and passion to reflect the company's philosophy of Runners Point even better.\n\nThat is just one of several projects that the brand serafini has developed as a retail engineering and design company.\n\nOwner Andreas Serafini\n\nPaul Serafini's son Andreas Serafini has been managing the business for several decades already. He made the decision to take the business to Hong Kong in 2008 for continuous growth in the Asia Pacific region.\n\nMr. Serafini led the company to new business segments with his engineer and business expertise.\n\nSerafini at EuroShop, Germany\n\nEuroShop is the World's No. 1 Retail Trade Fair with 120,000 m\xc2\xb2 net exhibition space and 2,500 exhibitors from around 60 countries it is both the trendsetter and an indispensable information and communication platform for all decision-makers in retail.\n\nPaul Serafini GmbH & Co.KG has been exhibiting at this world stage for several decades.\n\nWe work with external creatives who generate a constant flow of innovative concepts and a very competent engineering team in Germany. We all share the same passion for creating unique products for our business partners and for you at home.\n\nDesigner products for your home by serafini & business solutions for your company."
10.0.2.15: [464, 17884, 284, 2222, 33017, 736, 284, 262, 1353, 4477, 351, 1705, 326, 42952, 10018, 568, 338, 3700, 7383, 481, 4654, 262, 3517, 1074, 287, 262, 8308, 12, 25598, 2597, 286, 20671, 5890, 13, 198, 198, 32, 33017, 11821, 4999, 284, 19639, 16, 13, 785, 326, 7383, 318, 284, 1716, 636, 286, 262, 370, 5730, 8244, 13, 2102, 11, 340, 3793, 10061, 618, 262, 6337, 614, 1468, 481, 2221, 670, 13, 42952, 10018, 568, 423, 7392, 284, 2912, 13, 198, 198, 9218, 468, 734, 4647, 286, 1998, 287, 376, 16, 11, 1719, 9258, 465, 3451, 379, 8078, 287, 7795, 13, 679, 6150, 351, 262, 1074, 355, 340, 2627, 7215, 1044, 11, 1338, 2584, 263, 290, 5221, 3794, 11, 7396, 284, 262, 2597, 286, 20671, 5890, 13, 679, 5399, 23167, 527, 287, 3050, 878, 3867, 284, 42952, 10018, 568, 734, 812, 1568, 13, 198, 198, 10294, 11, 4705, 14433, 468, 1364, 465, 2597, 355, 33017, 338, 14044, 5890, 11, 996, 428, 318, 407, 5884, 284, 7383, 338, 27523, 10325, 13, 198, 198, 9742, 43, 5757, 389, 3058, 14024, 287, 262, 5678, 669, 6, 12184, 319, 4764, 2173, 11, 35404, 512, 35357, 286, 2766, 21279, 290, 2579, 1598, 286, 42952, 10018, 568, 13, 1119, 423, 407, 1839, 257, 3234, 1201, 262, 17036, 5675, 23832, 1622, 19523, 287, 2321, 13, 4619, 1946, 11, 484, 423, 5257, 655, 734, 27941, 82, 11, 981, 511, 938, 5678, 669, 6, 12184, 373, 734, 4647, 2084, 287, 7795, 13]
10.0.2.15: b"The rebuild to bring McLaren back to the top continues with news that Toro Rosso's James Key will join the British team in the newly-created role of Technical Director.\n\nA McLaren spokesperson confirmed to Formula1.com that Key is to become part of the Woking squad. However, it remains unclear when the 46 year old will begin work. Toro Rosso have declined to comment.\n\nKey has two decades of experience in F1, having begun his career at Jordan in 1998. He remained with the team as it became Midland, Spkyer and Force India, rising to the role of Technical Director. He joined Sauber in 2010 before moving to Toro Rosso two years later.\n\nMeanwhile, Matt Morris has left his role as McLaren's Engineering Director, though this is not connected to Key's impending arrival.\n\nMcLaren are currently seventh in the constructors' championship on 48 points, 262 adrift of leaders Mercedes and 28 clear of Toro Rosso. They have not won a race since the Brazilian Grand Prix season finale in 2012. Since 2014, they have managed just two podiums, while their last constructors' championship was two decades ago in 1998."
10.0.2.15: [40, 4398, 470, 1807, 428, 832, 198, 1537, 1312, 760, 644, 1312, 765, 284, 910, 198, 40, 1101, 7926, 986, 198, 12, 198, 1639, 6584, 470, 423, 284, 766, 502, 307, 588, 428, 198, 1639, 6584, 470, 423, 284, 3285, 502, 2740, 588, 428, 198, 1639, 6584, 470, 423, 284, 760, 1312, 1254, 588, 428, 198, 40, 691, 765, 345, 3772, 198, 40, 6044, 3360, 986, 198, 12, 198, 40, 4398, 470, 1807, 428, 832, 198, 1537, 1312, 760, 3446, 644, 1312, 765, 284, 1560, 345, 198, 40, 1101, 523, 7926, 986, 198, 12, 198, 1639, 6584, 470, 423, 284, 766, 502, 307, 588, 428, 198, 1639, 6584, 470, 423, 284, 3285, 502, 2740, 588, 428, 198, 1639, 6584, 470, 423, 284, 760, 1312, 1254, 588, 428, 198, 40, 691, 765, 345, 3772, 198, 40, 6044, 3360, 198, 2504, 502, 852, 19283, 1838, 345, 1254, 19283, 198, 2396, 1312, 1183, 1949, 284, 787, 428, 826, 198, 1639, 389, 616, 4202, 198, 1639, 389, 616, 761, 198, 5703, 1037, 502, 1949, 284, 787, 428, 826, 198, 8128, 1312, 836, 470, 760, 644, 1312, 423, 284, 466, 198, 8128, 1312, 4398, 470, 1807, 428, 832, 198, 12, 198, 40, 1101, 7926, 11, 1312, 1101, 7926, 11, 1312, 1101, 7926, 198, 40, 1101, 523, 7926, 986]
10.0.2.15: b"I haven't thought this through\nBut i know what i want to say\nI'm sorry...\n-\nYou shouldn't have to see me be like this\nYou shouldn't have to hear me speak like this\nYou shouldn't have to know i feel like this\nI only want you happy\nI forget sometimes...\n-\nI haven't thought this through\nBut i know exactly what i want to tell you\nI'm so sorry...\n-\nYou shouldn't have to see me be like this\nYou shouldn't have to hear me speak like this\nYou shouldn't have to know i feel like this\nI only want you happy\nI forget sometimes\nThat me being unhappy makes you feel unhappy\nSo i'll try to make this right\nYou are my strength\nYou are my need\nJust help me try to make this right\nBecause i don't know what i have to do\nBecause i haven't thought this through\n-\nI'm sorry, i'm sorry, i'm sorry\nI'm so sorry..."
10.0.2.15: [1722, 6693, 2961, 11, 356, 2098, 938, 614, 326, 10284, 849, 1531, 20211, 2957, 284, 281, 34660, 7532, 2882, 290, 8840, 14383, 5095, 326, 4120, 284, 307, 2233, 287, 636, 284, 257, 2994, 286, 1593, 39766, 15568, 355, 257, 1255, 286, 7532, 3797, 28426, 1042, 287, 262, 8889, 286, 7532, 21263, 13, 770, 614, 356, 5071, 326, 584, 1593, 19824, 15568, 389, 635, 2626, 422, 262, 14383, 416, 257, 2092, 9030, 11, 1390, 883, 2950, 287, 14383, 1960, 2522, 46671, 13, 44790, 25, 383, 4636, 500, 2746, 286, 10284, 849, 1531, 12, 17223, 14383, 5095, 4477, 284, 7766, 5337, 11701, 286, 360, 26761, 13, 3954, 6373, 428, 614, 1950, 326, 262, 34660, 7532, 2882, 18268, 416, 10284, 849, 1531, 460, 1085, 284, 14383, 5095, 416, 11062, 1780, 47585, 5549, 3846, 934, 1960, 2522, 46671, 11, 543, 468, 587, 2098, 284, 307, 281, 4075, 1429, 287, 10192, 32070, 30309, 422, 1918, 4073, 416, 257, 4996, 286, 2123, 72, 5823, 13, 632, 318, 2407, 1744, 326, 14383, 5095, 4073, 416, 584, 5010, 1244, 635, 307, 36631, 416, 2092, 22963, 7411, 1960, 2522, 46671, 30725, 13]
10.0.2.15: b'As discussed earlier, we reported last year that halothane metabolism led to an unfolded protein response and subsequent liver injury that appeared to be due in part to a loss of important antioxidant proteins as a result of protein catabolism in the absence of protein synthesis. This year we discovered that other important cellular proteins are also lost from the liver by a similar mechanism, including those involved in liver autophagy. Conclusion: The murine model of halothane-induced liver injury continues to reveal novel mechanisms of DILD. Our findings this year suggest that the unfolded protein response induced by halothane can lead to liver injury by inhibiting hepatocellular autophagy, which has been reported to be an active process in protecting hepatocytes from death caused by a variety of etiologies. It is quite possible that liver injury caused by other drugs might also be mediated by similar pathways involving autophagy inhibition.'
10.0.2.15: [46261, 11683, 3961, 25, 6350, 16889, 3533, 7119, 284, 22481, 6912, 263, 198, 198, 47, 2959, 1122, 5481, 591, 11, 257, 2266, 15600, 13430, 379, 14538, 12, 47, 500, 1086, 1648, 11, 9644, 257, 35093, 6246, 379, 5809, 626, 500, 17362, 11, 257, 1366, 12, 15808, 9283, 1430, 287, 8758, 11, 22892, 13, 198, 198, 13256, 35692, 329, 383, 968, 1971, 3782, 198, 198, 3886, 24412, 39878, 509, 8905, 21479, 198, 198, 17543, 1478, 11, 2177, 198, 198, 42, 3525, 11, 22892, 13, 851, 25389, 41971, 318, 262, 3367, 286, 257, 5931, 11949, 13, 5845, 389, 262, 10812, 339, 19552, 13, 1052, 11949, 11, 41971, 4893, 11, 481, 2074, 257, 1917, 11, 1205, 257, 1429, 284, 8494, 340, 11, 788, 3494, 11, 13446, 290, 4532, 1566, 4917, 257, 4610, 13, 10528, 318, 1418, 3201, 540, 13, 198, 198, 33, 16261, 750, 407, 765, 284, 307, 257, 5931, 11949, 11, 996, 13, 679, 2227, 284, 307, 11, 290, 991, 3382, 284, 307, 11, 262, 1266, 18086, 287, 9283, 13, 679, 14131, 319, 262, 31446, 355, 257, 2933, 287, 3442, 11, 1865, 465, 14495, 508, 9617, 7069, 11, 772, 416, 257, 1310, 1643, 11, 925, 262, 1266, 3067, 3466, 290, 12725, 517, 3241, 13, 198, 198, 1, 40, 373, 588, 11, 705, 46, 13, 42, 1539, 314, 1101, 10032, 286, 883, 661, 1972, 6443, 625, 502, 780, 314, 836, 470, 3714, 355, 1327, 16078, 41971, 531, 2904, 11, 287, 262, 10306, 12746, 6, 48018, 379, 19426, 1014, 3250, 13, 366, 5195, 836, 470, 314, 3714, 7069, 1701, 198, 198, 33, 16261, 9617, 8699, 4608, 281, 1711, 355, 257, 1029, 1524, 18621, 13, 383, 3931, 706, 326, 1622, 11, 339, 8672, 257, 3985, 287, 3936, 11, 6575, 8662, 1506, 11, 284, 466, 1223, 546, 340, 13, 554, 257, 1178, 812, 11, 41971, 2627, 257, 1327, 12, 400, 11577, 717, 12, 744, 4538, 2298, 290, 373, 2582, 287, 257, 1688, 4652, 13179, 13, 198, 198, 1890, 749, 25259, 11, 422, 6450, 75, 1747, 284, 37924, 11, 15432, 468, 1239, 587, 257, 2551, 13, 21035, 2073, 287, 21088, 460, 307, 4499, 851, 422, 13107, 11, 11070, 11, 772, 3835, 13, 887, 8168, 17324, 257, 38287, 11, 340, 318, 531, 26, 2035, 345, 3714, 1327, 393, 345, 836, 470, 13, 198, 198, 21174, 15195, 813, 11, 257, 14217, 329, 262, 8437, 38134, 11, 5610, 465, 22421, 319, 262, 7624, 878, 257, 2274, 983, 13, 317, 4152, 20052, 7817, 683, 465, 28982, 13, 317, 1688, 4652, 20052, 7817, 683, 465, 12133, 1894, 13, 317, 4159, 4652, 3985, 7817, 683, 465, 1487, 929, 13, 198, 198, 34, 5715, 392, 338, 25389, 41971, 21088, 1028, 6182, 938, 1227, 13, 1081, 881, 355, 597, 18086, 287, 262, 1688, 16861, 11, 339, 45370, 262, 1235, 284, 3714, 7069, 290, 517, 6840, 13, 198, 198, 11486, 41971, 290, 15195, 813, 423, 3402, 326, 262, 38287, 460, 307, 7817, 851, 416, 2615, 257, 1365, 530, 11, 287, 41971, 338, 1339, 11, 393, 25646, 257, 4814, 530, 11, 287, 15195, 813, 338, 13, 16126, 468, 587, 281, 1439, 12, 8248, 11, 475, 1111, 423, 1716, 20923, 11, 4745, 540, 19896, 11, 262, 1611, 2861, 5242, 284, 3466, 13, 9170, 262, 14805, 286, 257, 27721, 38287, 851, 10048, 4608, 281, 1711, 329, 41971, 11, 4101, 329, 15195, 813, 851, 484, 2192, 714, 407, 7866, 287, 262, 3660, 983, 13, 198, 198, 33, 16261, 290, 15195, 813, 423, 1111, 3111, 351, 14316, 347, 38553, 379, 5809, 626, 500, 17362, 338, 3047, 3641, 994, 11, 810, 41971, 9902, 319, 262, 11658, 339, 4499, 422, 8662, 1506, 379, 262, 3936, 17362, 24576, 287, 21532, 11, 3567, 13, 16126, 347, 38553, 4249, 8662, 1506, 21730, 28049, 11, 475, 484, 423, 3170, 27723, 5692, 416, 7743, 1123, 34047, 284, 20487, 851, 11512, 11, 484, 16361, 851, 465, 1767, 338, 5339, 329, 9644, 1327, 13, 198, 198, 2990, 6194, 1096, 11, 290, 423, 4193, 5252, 11, 262, 2866, 983, 9283, 468, 11835, 1716, 13, 4784, 284, 13836, 37065, 82, 11, 262, 2811, 38287, 287, 6244, 373, 9919, 285, 13, 79, 13, 71, 13, 632, 468, 47092, 2440, 287, 1123, 286, 262, 938, 3598, 7028, 11, 284, 10190, 13, 23, 285, 13, 79, 13, 71, 13, 17658, 15432, 318, 5609, 262, 6332, 11, 290, 477, 475, 25136, 503, 25259, 508, 460, 470, 1394, 510, 13, 198, 198, 1, 3198, 3470, 4608, 583, 1711, 318, 262, 649, 18335, 553, 531, 4186, 2097, 11, 257, 1966, 1688, 4652, 18086, 290, 3985, 508, 9393, 262, 2351, 350, 19811, 5396, 11, 543, 4539, 11665, 290, 21434, 13673, 13, 366, 40, 892, 287, 262, 1306, 1936, 284, 3624, 812, 11, 749, 25259, 11, 284, 1051, 257, 386, 2775, 11, 389, 1016, 284, 423, 284, 905, 10111, 11, 9661, 11, 290, 3638, 8949, 11, 15143, 13, 1320, 338, 810, 262, 2267, 318, 1016, 526, 198, 198, 2504, 318, 1521, 11, 739, 3329, 15114, 287, 2739, 2795, 11, 9264, 286, 1862, 25259, 851, 4632, 319, 2270, 422, 4152, 4056, 851, 9660, 2934, 45136, 1088, 262, 7647, 6041, 286, 5809, 626, 500, 338, 12949, 1363, 379, 281, 7593, 3952, 1474, 6896, 12, 51, 330, 12690, 11, 4769, 40231, 7923, 19590, 625, 511, 6665, 393, 266, 6950, 1359, 890, 16461, 357, 7174, 8163, 21103, 11, 329, 5814, 12, 929, 290, 7628, 8, 287, 2166, 286, 511, 34572, 13, 1119, 547, 617, 286, 262, 867, 31483, 10360, 508, 670, 351, 2097, 11, 8662, 1506, 11, 347, 38553, 290, 584, 11070, 508, 460, 1037, 606, 3714, 1327, 1576, 284, 307, 6810, 13, 198, 198, 1870, 1865, 11, 611, 484, 466, 407, 760, 340, 1541, 11, 484, 481, 2582, 2193, 326, 15432, 3436, 318, 407, 1576, 284, 6758, 379, 262, 4511, 1241, 13, 383, 983, 318, 1165, 28746, 290, 5901, 351, 1165, 867, 34113, 508, 460, 4532, 13, 887, 1231, 326, 15432, 11, 262, 1334, 743, 1239, 1282, 656, 711, 13, 198, 198, 1, 1858, 338, 257, 4314, 11, 588, 25, 705, 1639, 423, 284, 3714, 428, 1327, 11, 290, 611, 345, 836, 470, 11, 788, 345, 821, 407, 257, 1263, 443, 11433, 263, 16078, 347, 38553, 531, 13, 366, 1537, 661, 1011, 340, 835, 1165, 1290, 13, 632, 338, 691, 1593, 1035, 296, 794, 326, 345, 761]
10.0.2.15: b'Velocity School: Where Pitchers Pay to Throw Harder\n\nPeyton Burks, a redshirt junior at Arkansas-Pine Bluff, throwing a bullpen session at Driveline Baseball, a data-driven baseball program in Kent, Wash.\n\nMichael Hanson for The New York Times\n\nBy TYLER KEPNER\n\nSeptember 14, 2017\n\nKENT, Wash. \xe2\x80\x94 Trevor Bauer is the son of a chemical engineer. Those are the genes he inherited. An engineer, Bauer explained, will consider a problem, develop a process to solve it, then implement, evaluate and adjust until finding a solution. Nothing is untrainable.\n\nBauer did not want to be a chemical engineer, though. He wanted to be, and still wants to be, the best pitcher in baseball. He succeeded on the mound as a boy in California, yet his peers who threw harder, even by a little bit, made the best travel teams and attracted more attention.\n\n"I was like, \'O.K., I\'m tired of those people getting opportunities over me because I don\'t throw as hard,\'" Bauer said recently, in the Cleveland Indians\' clubhouse at Fenway Park. "Why don\'t I throw harder?"\n\nBauer threw 78 miles an hour as a high school freshman. The summer after that season, he visited a coach in Texas, Ron Wolforth, to do something about it. In a few years, Bauer became a hard-throwing first-round draft pick and was soon in a major league rotation.\n\nFor most pitchers, from sandlots to stadiums, velocity has never been a decision. Anything else in pitching can be learned \xe2\x80\x94 from teammates, coaches, even books. But nobody teaches a fastball, it is said; either you throw hard or you don\'t.\n\nDan Straily, a starter for the Miami Marlins, listed his pitches on the bench before a recent game. A college teammate taught him his slider. A major league teammate taught him his curveball. A minor league coach taught him his changeup.\n\nCleveland\'s Trevor Bauer pitching against Boston last month. As much as any pitcher in the major leagues, he embodies the quest to throw harder and more effectively.\n\nYet Bauer and Straily have shown that the fastball can be taught \xe2\x80\x94 by building a better one, in Bauer\'s case, or restoring a missing one, in Straily\'s. Neither has been an All-Star, but both have become durable, dependable starters, the kind worth millions to teams. Without the baseline of a respectable fastball \xe2\x80\x94 94 miles an hour for Bauer, 90 for Straily \xe2\x80\x94 they probably could not survive in the modern game.\n\nBauer and Straily have both worked with Kyle Boddy at Driveline Baseball\'s training center here, where Bauer expanded on the lessons he learned from Wolforth at the Texas Baseball Ranch in Montgomery, Tex. Neither Boddy nor Wolforth pitched professionally, but they have built thriving businesses by teaching each pupil to maximize \xe2\x80\x94 safely, they insist \xe2\x80\x94 his body\'s capacity for throwing hard.\n\nThey symbolize, and have helped fuel, the speed game baseball has gradually become. According to FanGraphs, the average fastball in 2002 was 89 m.p.h. It has crept higher in each of the last seven seasons, to 92.8 m.p.h. Rising velocity is changing the sport, and all but shutting out pitchers who can\'t keep up.\n\n"One hundred miles per hour is the new benchmark," said Tom House, a former major league pitcher and coach who founded the National Pitching Association, which runs camps and clinics nationwide. "I think in the next five to eight years, most pitchers, to sign a pro contract, are going to have to show 97, 98, and touch 101, 102. That\'s where the research is going."\n\nThat is why, under morning clouds in late June, dozens of young pitchers \xe2\x80\x94 mostly on break from college programs \xe2\x80\x94 strode purposefully around the parking lots of Driveline\'s modest home at an industrial park near Sea-Tac Airport, holding kettlebell weights over their heads or wiggling long sticks (called shoulder tubes, for warm-up and recovery) in front of their chests. They were some of the many aspiring pros who work with House, Wolforth, Boddy and other coaches who can help them throw hard enough to be noticed.\n\nAnd yet, if they do not know it already, they will soon learn that velocity alone is not enough to succeed at the highest level. The game is too intricate and filled with too many hitters who can adjust. But without that velocity, the rest may never come into play.\n\n"There\'s a floor, like: \'You have to throw this hard, and if you don\'t, then you\'re not a big leaguer,\'" Boddy said. "But people take it way too far. It\'s only important insomuch that you need'
10.0.2.15: [13056, 774, 1737, 520, 5593, 198, 198, 1, 32, 1588, 1641, 286, 607, 898, 508, 423, 5281, 319, 607, 1708, 40550, 82, 11097, 198, 198, 2061, 481, 345, 3505, 749, 546, 606, 30, 198, 198, 1, 9360, 2461, 290, 34935, 284, 607, 27495, 1, 198, 198, 10449, 345, 329, 10013, 616, 21208, 286, 13408, 17547, 1814, 13, 198, 198, 1212, 2041, 1814, 481, 1037, 5298, 9204, 5153, 329, 262, 3517, 8894, 5693, 13, 383, 347, 29567, 318, 262, 3277, 338, 2612, 11016, 11, 7256, 284, 8914, 3160, 832, 36304, 2267, 11, 5827, 1337, 290, 416, 4955, 9204, 1321, 13, 632, 561, 307, 4998, 611, 356, 714, 5298, 6041, 286, 1637, 284, 1037, 606, 2555, 511, 6393, 670, 13, 1002, 345, 389, 257, 3482, 14776, 345, 460, 751, 1679, 4, 284, 534, 13784, 379, 645, 3131, 1575, 284, 345, 416, 2391, 40212, 262, 21208, 22225, 3091, 618, 345, 16565, 13, 198, 198, 10449, 345, 757, 13, 198, 198, 3987, 602, 352, 198, 198, 14988, 3064, 13, 405, 198, 198, 5246, 290, 9074, 1757, 30223, 367, 382, 198, 198, 1415, 14, 3070, 14, 6999, 198, 198, 1135, 423, 890, 9880, 11, 523, 307, 14431, 290, 7074, 262, 2496, 356, 423, 900, 11, 673, 373, 257, 10457, 38367, 11, 18008, 290, 3878, 18008, 290, 815, 1464, 307, 12086, 13, 198, 18565, 422, 38074, 3208, 1222, 23169, 1757, 198, 79, 13, 82, 13, 5845, 351, 3644, 3387, 1208, 428, 3275, 4291, 1854, 508, 389, 407, 2691, 13]
10.0.2.15: b'Betty May St Louis\n\n"A large family of her own who have carried on her following precepts:"\n\nWhat will you remember most about them?\n\n"Her respect and humility to her elders"\n\nThank you for visiting my Gift of Hope tribute fund.\n\nThis special fund will help raise vital funds for the British Heart Foundation. The BHF is the nation\'s heart charity, dedicated to saving lives through pioneering research, patient care and by providing vital information. It would be amazing if we could raise lots of money to help them continue their essential work. If you are a UK taxpayer you can add 25% to your donation at no extra cost to you by simply ticking the Gift Aid box when you donate.\n\nThank you again.\n\nDonations 1\n\n\xc2\xa3100.00\n\nMr and Mrs John Patricia Hore\n\n14/03/2012\n\nWe have long retired, so be generous and exceed the target we have set, she was a brilliant Mum, Nan and Great Nan and should always be remembered.\nLove from Aunt Pat & Uncle John\np.s. Those with computer please pass this message onto others who are not online.'
10.0.2.15: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.15: Load language detection model
10.0.2.15: Dataset document count 2509150, token count 3431998805, non sentence start0.0
10.0.2.15: BERT prob 0.5, gap sent prob 0.3, GPT prob 0.2, infill prob 0.5
10.0.2.15: generation min ratio 0.25, block ratio 0.15, gap sent ratio 0.15
10.0.2.15: block length distribution [0.14936120510359185, 0.22404180765538775, 0.22404180765538775, 0.16803135574154085, 0.10081881344492458, 0.05040940672246224, 0.02160403145248382, 0.008101511794681432, 0.002700503931560479, 0.0008101511794681439, 0.00022095032167312998, 5.523758041828258e-05, 1.2747133942680616e-05, 2.7315287020029775e-06, 5.463057404005949e-07, 1.0243232632511163e-07, 1.807629288090209e-08, 3.0127154801503496e-09, 4.756919179184761e-10, 7.135378768777153e-11, 1.01933982411102e-11, 1.3900088510604783e-12, 1.8130550231223652e-13, 2.26631877890296e-14, 2.719582534683569e-15, 3.1379798477117986e-16, 3.486644275235373e-17, 3.735690294894986e-18, 3.8645072016155465e-19, 3.864507201615528e-20, 3.739845678982777e-21, 3.5061053240463625e-22, 3.18736847640571e-23, 2.8123839497698246e-24, 2.410614814088421e-25, 2.0088456784069862e-26, 1.6287937933030137e-27, 1.285889836818143e-28, 9.89146028321656e-30]
10.0.2.15: block mask prob 0.1, context mask ratio 0.0
10.0.2.15: building GPT2 model ...
10.0.2.15:  > number of parameters on model parallel rank 1: 2477359104
10.0.2.15:  > number of parameters on model parallel rank 3: 2477359104
10.0.2.15:  > number of parameters on model parallel rank 2: 2477359104
10.0.2.15:  > number of parameters on model parallel rank 0: 2477359104
10.0.2.15: NCCL version 2.8.4+cuda11.2
10.0.2.15: NCCL version 2.8.4+cuda11.2
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139531 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139530 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53500 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53501 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139531 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139530 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53500 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53501 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO Setting affinity for GPU 7 to 3fff,fff00000,03ffffff
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO Setting affinity for GPU 7 to ff,ffff0000,00ffffff
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Channel 00/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Channel 01/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Setting affinity for GPU 3 to 3fff,fff00000,03ffffff
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Channel 00 : 1[41000] -> 2[1e000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO Channel 00 : 1[41000] -> 2[1e000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Channel 00 : 3[41000] -> 0[1e000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO Channel 00 : 3[41000] -> 0[1e000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Channel 01 : 1[41000] -> 2[1e000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Channel 00 : 2[1e000] -> 3[41000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Channel 01 : 2[1e000] -> 3[41000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO Channel 01 : 1[41000] -> 2[1e000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO Channel 01 : 3[41000] -> 0[1e000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Channel 01 : 3[41000] -> 0[1e000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Channel 00 : 0[1e000] -> 1[41000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Channel 01 : 0[1e000] -> 1[41000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO Channel 00 : 3[41000] -> 2[1e000] via P2P/IPC
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO Channel 01 : 3[41000] -> 2[1e000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Channel 00 : 0[1e000] -> 2[1e000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO Channel 00 : 1[41000] -> 0[1e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO Channel 01 : 1[41000] -> 0[1e000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Channel 01 : 0[1e000] -> 2[1e000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Channel 00 : 2[1e000] -> 0[1e000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Channel 00 : 2[1e000] -> 0[1e000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Channel 01 : 2[1e000] -> 0[1e000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Channel 01 : 2[1e000] -> 0[1e000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Channel 00 : 0[1e000] -> 2[1e000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Channel 01 : 0[1e000] -> 2[1e000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139194:139531 [3] NCCL INFO comm 0x7f68b40df4b0 rank 2 nranks 4 cudaDev 3 busId 1e000 - Init COMPLETE
10.0.2.16: 94f182076445:139198:139530 [7] NCCL INFO comm 0x7ff0a40dfaf0 rank 3 nranks 4 cudaDev 7 busId 41000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53332:53500 [3] NCCL INFO comm 0x7fc8c4008e10 rank 0 nranks 4 cudaDev 3 busId 1e000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53337:53501 [7] NCCL INFO comm 0x7f6e900df5f0 rank 1 nranks 4 cudaDev 7 busId 41000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53332:53332 [3] NCCL INFO Launch mode Parallel
10.0.2.15: NCCL version 2.8.4+cuda11.2
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139537 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139536 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53508 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53507 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139537 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139536 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53508 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53507 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO Setting affinity for GPU 6 to ff,ffff0000,00ffffff
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO Setting affinity for GPU 6 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Channel 00/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Channel 01/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Setting affinity for GPU 2 to 3fff,fff00000,03ffffff
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Channel 00 : 1[40000] -> 2[1d000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO Channel 00 : 3[40000] -> 0[1d000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO Channel 00 : 1[40000] -> 2[1d000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Channel 00 : 3[40000] -> 0[1d000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Channel 01 : 1[40000] -> 2[1d000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Channel 00 : 2[1d000] -> 3[40000] via P2P/IPC
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Channel 01 : 2[1d000] -> 3[40000] via P2P/IPC
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO Channel 01 : 3[40000] -> 0[1d000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO Channel 01 : 1[40000] -> 2[1d000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Channel 01 : 3[40000] -> 0[1d000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Channel 00 : 0[1d000] -> 1[40000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Channel 01 : 0[1d000] -> 1[40000] via P2P/IPC
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO Channel 00 : 3[40000] -> 2[1d000] via P2P/IPC
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO Channel 01 : 3[40000] -> 2[1d000] via P2P/IPC
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Channel 00 : 0[1d000] -> 2[1d000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO Channel 00 : 1[40000] -> 0[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO Channel 01 : 1[40000] -> 0[1d000] via P2P/IPC
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Channel 01 : 0[1d000] -> 2[1d000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Channel 00 : 2[1d000] -> 0[1d000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Channel 00 : 2[1d000] -> 0[1d000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Channel 01 : 2[1d000] -> 0[1d000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Channel 01 : 2[1d000] -> 0[1d000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Channel 00 : 0[1d000] -> 2[1d000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Channel 01 : 0[1d000] -> 2[1d000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139197:139536 [6] NCCL INFO comm 0x7f3eec0e1f50 rank 3 nranks 4 cudaDev 6 busId 40000 - Init COMPLETE
10.0.2.16: 94f182076445:139193:139537 [2] NCCL INFO comm 0x7ff51c0e1f50 rank 2 nranks 4 cudaDev 2 busId 1d000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53335:53508 [6] NCCL INFO comm 0x7f12900e2680 rank 1 nranks 4 cudaDev 6 busId 40000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53331:53507 [2] NCCL INFO comm 0x7f29c8008e10 rank 0 nranks 4 cudaDev 2 busId 1d000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53331:53331 [2] NCCL INFO Launch mode Parallel
10.0.2.15: DeepSpeed is enabled.
10.0.2.15: [2021-12-31 18:37:49,986] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.3.16, git-hash=unknown, git-branch=unknown
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139528 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139529 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53515 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53498 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139528 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139529 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53515 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53498 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO Setting affinity for GPU 5 to 3fff,fff00000,03ffffff
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO Setting affinity for GPU 5 to ff,ffff0000,00ffffff
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Channel 00/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Channel 01/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Setting affinity for GPU 1 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO Channel 00 : 1[3e000] -> 2[1b000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO Channel 00 : 3[3e000] -> 0[1b000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Channel 00 : 1[3e000] -> 2[1b000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO Channel 01 : 1[3e000] -> 2[1b000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO Channel 01 : 3[3e000] -> 0[1b000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Channel 00 : 3[3e000] -> 0[1b000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Channel 01 : 1[3e000] -> 2[1b000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Channel 00 : 2[1b000] -> 3[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Channel 01 : 2[1b000] -> 3[3e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Channel 01 : 3[3e000] -> 0[1b000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Channel 00 : 0[1b000] -> 1[3e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Channel 01 : 0[1b000] -> 1[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO Channel 00 : 3[3e000] -> 2[1b000] via P2P/IPC
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO Channel 01 : 3[3e000] -> 2[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO Channel 00 : 1[3e000] -> 0[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO Channel 01 : 1[3e000] -> 0[1b000] via P2P/IPC
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Channel 00 : 0[1b000] -> 2[1b000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Channel 00 : 2[1b000] -> 0[1b000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Channel 01 : 0[1b000] -> 2[1b000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Channel 01 : 2[1b000] -> 0[1b000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Channel 00 : 0[1b000] -> 2[1b000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Channel 00 : 2[1b000] -> 0[1b000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Channel 01 : 0[1b000] -> 2[1b000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Channel 01 : 2[1b000] -> 0[1b000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53334:53515 [5] NCCL INFO comm 0x7f8d900e2780 rank 1 nranks 4 cudaDev 5 busId 3e000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53330:53498 [1] NCCL INFO comm 0x7f09d4008e10 rank 0 nranks 4 cudaDev 1 busId 1b000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53330:53330 [1] NCCL INFO Launch mode Parallel
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139192:139528 [1] NCCL INFO comm 0x7f71440e1f50 rank 2 nranks 4 cudaDev 1 busId 1b000 - Init COMPLETE
10.0.2.16: 94f182076445:139196:139529 [5] NCCL INFO comm 0x7fb33c0e21c0 rank 3 nranks 4 cudaDev 5 busId 3e000 - Init COMPLETE
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53520 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139542 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139543 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53514 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53520 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139542 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139543 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53514 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO Setting affinity for GPU 4 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Channel 00/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO Setting affinity for GPU 4 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Channel 01/02 :    0   1   2   3
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Setting affinity for GPU 0 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO Channel 00 : 1[3d000] -> 2[1a000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Channel 00 : 1[3d000] -> 2[1a000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO Channel 00 : 3[3d000] -> 0[1a000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Channel 01 : 1[3d000] -> 2[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO Channel 01 : 1[3d000] -> 2[1a000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Channel 00 : 2[1a000] -> 3[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO Channel 01 : 3[3d000] -> 0[1a000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Channel 01 : 2[1a000] -> 3[3d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Channel 00 : 3[3d000] -> 0[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Channel 01 : 3[3d000] -> 0[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Channel 00 : 0[1a000] -> 1[3d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Channel 01 : 0[1a000] -> 1[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO Channel 00 : 3[3d000] -> 2[1a000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO Channel 01 : 3[3d000] -> 2[1a000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO Channel 00 : 1[3d000] -> 0[1a000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO Channel 01 : 1[3d000] -> 0[1a000] via P2P/IPC
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Channel 00 : 0[1a000] -> 2[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Channel 00 : 2[1a000] -> 0[1a000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Channel 01 : 0[1a000] -> 2[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Channel 01 : 2[1a000] -> 0[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Channel 00 : 0[1a000] -> 2[1a000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Channel 01 : 0[1a000] -> 2[1a000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Channel 00 : 2[1a000] -> 0[1a000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Channel 01 : 2[1a000] -> 0[1a000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139195:139542 [4] NCCL INFO comm 0x7fcf880df4b0 rank 3 nranks 4 cudaDev 4 busId 3d000 - Init COMPLETE
10.0.2.16: 94f182076445:139191:139543 [0] NCCL INFO comm 0x7f93b00dfde0 rank 2 nranks 4 cudaDev 0 busId 1a000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53333:53520 [4] NCCL INFO comm 0x7f8a500df4b0 rank 1 nranks 4 cudaDev 4 busId 3d000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53329:53514 [0] NCCL INFO comm 0x7f42f8008e10 rank 0 nranks 4 cudaDev 0 busId 1a000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53329:53329 [0] NCCL INFO Launch mode Parallel
10.0.2.15: [2021-12-31 18:38:06,409] [INFO] [engine.py:610:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer
10.0.2.15: [2021-12-31 18:38:06,409] [INFO] [engine.py:615:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam
10.0.2.15: Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
10.0.2.15: [2021-12-31 18:38:06,410] [INFO] [logging.py:60:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer
10.0.2.15: [2021-12-31 18:38:06,410] [INFO] [stage2.py:102:__init__] Reduce bucket size 50000000
10.0.2.15: [2021-12-31 18:38:06,410] [INFO] [stage2.py:103:__init__] Allgather bucket size 500000000
10.0.2.15: [2021-12-31 18:38:06,410] [INFO] [stage2.py:104:__init__] CPU Offload: False
10.0.2.15: [2021-12-31 18:38:13,813] [INFO] [stage2.py:381:__init__] optimizer state initialized
10.0.2.15: [2021-12-31 18:38:13,813] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
10.0.2.15: [2021-12-31 18:38:13,813] [INFO] [engine.py:444:_configure_lr_scheduler] DeepSpeed using client LR scheduler
10.0.2.15: [2021-12-31 18:38:13,813] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
10.0.2.15: [2021-12-31 18:38:13,813] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 18:38:13,813] [INFO] [config.py:747:print] DeepSpeedEngine configuration:
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   activation_checkpointing_config  {
10.0.2.15:     "partition_activations": false, 
10.0.2.15:     "contiguous_memory_optimization": false, 
10.0.2.15:     "cpu_checkpointing": false, 
10.0.2.15:     "number_checkpoints": null, 
10.0.2.15:     "synchronize_checkpoint_boundary": false, 
10.0.2.15:     "profile": false
10.0.2.15: }
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   allreduce_always_fp32 ........ False
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   amp_enabled .................. False
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   amp_params ................... False
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   checkpoint_tag_validation_enabled  True
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   checkpoint_tag_validation_fail  False
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   disable_allgather ............ False
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   dump_state ................... False
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
10.0.2.15: [2021-12-31 18:38:13,814] [INFO] [config.py:751:print]   elasticity_enabled ........... False
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   flops_profiler_config ........ {
10.0.2.15:     "enabled": false, 
10.0.2.15:     "profile_step": 1, 
10.0.2.15:     "module_depth": -1, 
10.0.2.15:     "top_modules": 3, 
10.0.2.15:     "detailed": true
10.0.2.15: }
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   fp16_enabled ................. True
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   global_rank .................. 0
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   gradient_accumulation_steps .. 1
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   gradient_clipping ............ 1.0
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   gradient_predivide_factor .... 1.0
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   initial_dynamic_scale ........ 4294967296
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   loss_scale ................... 0
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   memory_breakdown ............. False
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   optimizer_legacy_fusion ...... False
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   optimizer_name ............... adam
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.95], 'eps': 1e-08, 'weight_decay': 0.1}
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   pld_enabled .................. False
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   pld_params ................... False
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   prescale_gradients ........... False
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   scheduler_name ............... None
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   scheduler_params ............. None
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   sparse_attention ............. None
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   sparse_gradients_enabled ..... False
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   steps_per_print .............. 50
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   tensorboard_enabled .......... False
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   tensorboard_job_name ......... DeepSpeedJobName
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   tensorboard_output_path ...... 
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   train_batch_size ............. 84
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   train_micro_batch_size_per_gpu  21
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   wall_clock_breakdown ......... False
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   world_size ................... 4
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   zero_allow_untested_optimizer  True
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   zero_config .................. {
10.0.2.15:     "stage": 2, 
10.0.2.15:     "contiguous_gradients": false, 
10.0.2.15:     "reduce_scatter": true, 
10.0.2.15:     "reduce_bucket_size": 5.000000e+07, 
10.0.2.15:     "allgather_partitions": true, 
10.0.2.15:     "allgather_bucket_size": 5.000000e+08, 
10.0.2.15:     "overlap_comm": true, 
10.0.2.15:     "load_from_fp32_weights": true, 
10.0.2.15:     "elastic_checkpoint": true, 
10.0.2.15:     "offload_param": null, 
10.0.2.15:     "offload_optimizer": null, 
10.0.2.15:     "sub_group_size": 1.000000e+12, 
10.0.2.15:     "prefetch_bucket_size": 5.000000e+07, 
10.0.2.15:     "param_persistence_threshold": 1.000000e+05, 
10.0.2.15:     "max_live_parameters": 1.000000e+09, 
10.0.2.15:     "max_reuse_distance": 1.000000e+09, 
10.0.2.15:     "gather_fp16_weights_on_model_save": false, 
10.0.2.15:     "find_unused_parameters": false
10.0.2.15: }
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   zero_enabled ................. True
10.0.2.15: [2021-12-31 18:38:13,815] [INFO] [config.py:751:print]   zero_optimization_stage ...... 2
10.0.2.15: [2021-12-31 18:38:13,816] [INFO] [config.py:753:print]   json = {
10.0.2.15:     "train_micro_batch_size_per_gpu": 21, 
10.0.2.15:     "gradient_accumulation_steps": 1, 
10.0.2.15:     "steps_per_print": 50, 
10.0.2.15:     "gradient_clipping": 1.0, 
10.0.2.15:     "zero_optimization": {
10.0.2.15:         "stage": 2, 
10.0.2.15:         "contiguous_gradients": false, 
10.0.2.15:         "overlap_comm": true, 
10.0.2.15:         "reduce_scatter": true, 
10.0.2.15:         "reduce_bucket_size": 5.000000e+07, 
10.0.2.15:         "allgather_bucket_size": 5.000000e+08
10.0.2.15:     }, 
10.0.2.15:     "zero_allow_untested_optimizer": true, 
10.0.2.15:     "fp16": {
10.0.2.15:         "enabled": true, 
10.0.2.15:         "loss_scale": 0, 
10.0.2.15:         "loss_scale_window": 1000, 
10.0.2.15:         "hysteresis": 2, 
10.0.2.15:         "min_loss_scale": 1
10.0.2.15:     }, 
10.0.2.15:     "optimizer": {
10.0.2.15:         "type": "Adam", 
10.0.2.15:         "params": {
10.0.2.15:             "lr": 0.0001, 
10.0.2.15:             "betas": [0.9, 0.95], 
10.0.2.15:             "eps": 1e-08, 
10.0.2.15:             "weight_decay": 0.1
10.0.2.15:         }
10.0.2.15:     }, 
10.0.2.15:     "activation_checkpointing": {
10.0.2.15:         "partition_activations": false, 
10.0.2.15:         "contiguous_memory_optimization": false
10.0.2.15:     }, 
10.0.2.15:     "wall_clock_breakdown": false
10.0.2.15: }
10.0.2.15: learning rate decaying style cosine, ratio 10.0
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53539 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53534 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53538 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53533 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139571 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139571 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139571 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53539 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53534 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53538 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53533 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139573 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139577 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139571 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139570 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139574 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139576 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53539 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53534 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139572 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139575 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53538 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53533 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53540 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53537 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53536 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53535 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139573 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139577 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139571 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139570 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139574 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139576 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139572 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139575 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53539 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53534 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53538 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53533 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53540 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53537 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53536 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53535 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139573 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139577 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139571 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139570 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139574 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139576 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139572 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139575 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53539 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53534 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53538 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53533 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53540 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139573 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139577 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53537 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53536 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139571 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139570 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139574 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139576 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53535 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139572 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139575 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53539 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53534 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139573 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139577 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53538 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139196:139571 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139570 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53533 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139574 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139576 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53540 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53537 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139572 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139575 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53536 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53535 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139573 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139577 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139570 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139574 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139576 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53539 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139572 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139575 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53534 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53538 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53533 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53540 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53537 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53536 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53535 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139573 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139577 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139570 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139574 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139576 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139572 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139575 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53331:53539 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53332:53534 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53330:53538 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53329:53533 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53540 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53537 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53536 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53535 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139195:139573 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139197:139577 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139198:139570 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139192:139574 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139193:139576 [2] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139194:139572 [3] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.16: 
10.0.2.16: 94f182076445:139191:139575 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53540 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53537 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53536 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53535 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53335:53540 [6] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53334:53537 [5] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53337:53536 [7] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 
10.0.2.15: 2be7fa6883db:53333:53535 [4] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO Setting affinity for GPU 6 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO Setting affinity for GPU 7 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO Setting affinity for GPU 4 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO Setting affinity for GPU 3 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO Setting affinity for GPU 5 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO Setting affinity for GPU 1 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO Setting affinity for GPU 2 to 3fff,fff00000,03ffffff
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->8
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Setting affinity for GPU 0 to 3fff,fff00000,03ffffff
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO Setting affinity for GPU 5 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO Setting affinity for GPU 6 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO Setting affinity for GPU 7 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO Setting affinity for GPU 4 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/0/-1->8->-1
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Channel 00 : 15[41000] -> 0[1a000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO Channel 00 : 14[40000] -> 15[41000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO Channel 00 : 11[1e000] -> 12[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Channel 00 : 7[41000] -> 8[1a000] [receive] via NET/Socket/0
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO Channel 00 : 13[3e000] -> 14[40000] via P2P/IPC
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO Channel 00 : 9[1b000] -> 10[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO Channel 00 : 4[3d000] -> 5[3e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO Channel 00 : 6[40000] -> 7[41000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO Channel 00 : 12[3d000] -> 13[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO Channel 00 : 10[1d000] -> 11[1e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO Channel 00 : 1[1b000] -> 2[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO Channel 00 : 3[1e000] -> 4[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO Channel 01 : 14[40000] -> 15[41000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO Channel 00 : 7[41000] -> 8[1a000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO Channel 01 : 11[1e000] -> 12[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO Channel 01 : 13[3e000] -> 14[40000] via P2P/IPC
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO Channel 01 : 9[1b000] -> 10[1d000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO Channel 01 : 12[3d000] -> 13[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO Channel 01 : 10[1d000] -> 11[1e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO Channel 00 : 5[3e000] -> 6[40000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO Channel 00 : 2[1d000] -> 3[1e000] via P2P/IPC
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO Channel 00 : 15[41000] -> 0[1a000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Channel 01 : 15[41000] -> 0[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO Channel 01 : 4[3d000] -> 5[3e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO Channel 01 : 6[40000] -> 7[41000] via P2P/IPC
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Channel 01 : 7[41000] -> 8[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO Channel 01 : 1[1b000] -> 2[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO Channel 01 : 3[1e000] -> 4[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Channel 00 : 8[1a000] -> 9[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO Channel 01 : 5[3e000] -> 6[40000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO Channel 01 : 2[1d000] -> 3[1e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Channel 00 : 0[1a000] -> 1[1b000] via P2P/IPC
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO Channel 01 : 7[41000] -> 8[1a000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Channel 01 : 8[1a000] -> 9[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Channel 01 : 0[1a000] -> 1[1b000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO Channel 01 : 15[41000] -> 0[1a000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO Channel 00 : 13[3e000] -> 12[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO Channel 00 : 10[1d000] -> 9[1b000] via P2P/IPC
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO Channel 01 : 13[3e000] -> 12[3d000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO Channel 00 : 12[3d000] -> 11[1e000] via P2P/IPC
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO Channel 01 : 10[1d000] -> 9[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO Channel 01 : 12[3d000] -> 11[1e000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO Channel 00 : 11[1e000] -> 10[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO Channel 00 : 5[3e000] -> 4[3d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO Channel 00 : 2[1d000] -> 1[1b000] via P2P/IPC
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO Channel 00 : 9[1b000] -> 8[1a000] via P2P/IPC
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO Channel 01 : 11[1e000] -> 10[1d000] via P2P/IPC
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO Channel 01 : 9[1b000] -> 8[1a000] via P2P/IPC
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139195:139573 [4] NCCL INFO comm 0x7fcf882e3140 rank 12 nranks 16 cudaDev 4 busId 3d000 - Init COMPLETE
10.0.2.16: 94f182076445:139194:139572 [3] NCCL INFO comm 0x7f68b42efcd0 rank 11 nranks 16 cudaDev 3 busId 1e000 - Init COMPLETE
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139193:139576 [2] NCCL INFO comm 0x7ff51c2f27e0 rank 10 nranks 16 cudaDev 2 busId 1d000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO Channel 01 : 5[3e000] -> 4[3d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO Channel 01 : 2[1d000] -> 1[1b000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO Channel 00 : 1[1b000] -> 0[1a000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO Channel 01 : 1[1b000] -> 0[1a000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO Channel 00 : 4[3d000] -> 3[1e000] via P2P/IPC
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Channel 00 : 0[1a000] -> 8[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO Channel 00 : 7[41000] -> 6[40000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO Channel 01 : 4[3d000] -> 3[1e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO Channel 00 : 3[1e000] -> 2[1d000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO Channel 01 : 7[41000] -> 6[40000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO Channel 01 : 3[1e000] -> 2[1d000] via P2P/IPC
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Channel 01 : 0[1a000] -> 8[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53333:53535 [4] NCCL INFO comm 0x7f8a502e3140 rank 4 nranks 16 cudaDev 4 busId 3d000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53331:53539 [2] NCCL INFO comm 0x7f29c80d56e0 rank 2 nranks 16 cudaDev 2 busId 1d000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53332:53534 [3] NCCL INFO comm 0x7fc8c40d56e0 rank 3 nranks 16 cudaDev 3 busId 1e000 - Init COMPLETE
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Channel 00 : 8[1a000] -> 0[1a000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO Connected all rings
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO Channel 00 : 6[40000] -> 5[3e000] via P2P/IPC
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO Channel 01 : 6[40000] -> 5[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Channel 01 : 8[1a000] -> 0[1a000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53337:53536 [7] NCCL INFO comm 0x7f6e902e3280 rank 7 nranks 16 cudaDev 7 busId 41000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53335:53540 [6] NCCL INFO comm 0x7f12902e6070 rank 6 nranks 16 cudaDev 6 busId 40000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53334:53537 [5] NCCL INFO comm 0x7f8d902e6180 rank 5 nranks 16 cudaDev 5 busId 3e000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO Connected all rings
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO Channel 00 : 15[41000] -> 14[40000] via P2P/IPC
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO Channel 01 : 15[41000] -> 14[40000] via P2P/IPC
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO Channel 00 : 14[40000] -> 13[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO Channel 01 : 14[40000] -> 13[3e000] via P2P/IPC
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139198:139570 [7] NCCL INFO comm 0x7ff0a42e3820 rank 15 nranks 16 cudaDev 7 busId 41000 - Init COMPLETE
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139196:139571 [5] NCCL INFO comm 0x7fb33c2e5e50 rank 13 nranks 16 cudaDev 5 busId 3e000 - Init COMPLETE
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139197:139577 [6] NCCL INFO comm 0x7f3eec2e5ba0 rank 14 nranks 16 cudaDev 6 busId 40000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Channel 00 : 8[1a000] -> 0[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Channel 01 : 8[1a000] -> 0[1a000] [receive] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Channel 00 : 0[1a000] -> 8[1a000] [send] via NET/Socket/0
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Channel 01 : 0[1a000] -> 8[1a000] [send] via NET/Socket/0
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.16: 94f182076445:139191:139575 [0] NCCL INFO comm 0x7f93b02f04c0 rank 8 nranks 16 cudaDev 0 busId 1a000 - Init COMPLETE
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO Connected all trees
10.0.2.16: 94f182076445:139192:139574 [1] NCCL INFO comm 0x7f71442f27e0 rank 9 nranks 16 cudaDev 1 busId 1b000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53329:53533 [0] NCCL INFO comm 0x7f41fc008e10 rank 0 nranks 16 cudaDev 0 busId 1a000 - Init COMPLETE
10.0.2.15: 2be7fa6883db:53329:53329 [0] NCCL INFO Launch mode Parallel
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO Connected all trees
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
10.0.2.15: 2be7fa6883db:53330:53538 [1] NCCL INFO comm 0x7f09d40d56e0 rank 1 nranks 16 cudaDev 1 busId 1b000 - Init COMPLETE
10.0.2.15: Pretrain GPT2 model
10.0.2.15: arguments:
10.0.2.15:   transformer_xl ............... False
10.0.2.15:   pretrained_bert .............. False
10.0.2.15:   encoder_decoder .............. False
10.0.2.15:   attention_dropout ............ 0.1
10.0.2.15:   num_attention_heads .......... 64
10.0.2.15:   hidden_size .................. 4096
10.0.2.15:   intermediate_size ............ None
10.0.2.15:   num_layers ................... 48
10.0.2.15:   layernorm_epsilon ............ 1e-05
10.0.2.15:   hidden_dropout ............... 0.1
10.0.2.15:   output_dropout ............... 0.1
10.0.2.15:   max_position_embeddings ...... 1024
10.0.2.15:   vocab_size ................... 50304
10.0.2.15:   deep_init .................... False
10.0.2.15:   make_vocab_size_divisible_by . 128
10.0.2.15:   cpu_optimizer ................ False
10.0.2.15:   cpu_torch_adam ............... False
10.0.2.15:   fp16 ......................... True
10.0.2.15:   fp32_embedding ............... False
10.0.2.15:   fp32_layernorm ............... False
10.0.2.15:   fp32_tokentypes .............. False
10.0.2.15:   fp32_allreduce ............... False
10.0.2.15:   hysteresis ................... 2
10.0.2.15:   loss_scale ................... None
10.0.2.15:   loss_scale_window ............ 1000
10.0.2.15:   min_scale .................... 1
10.0.2.15:   attention_scale .............. 1.0
10.0.2.15:   experiment_name .............. blocklm-10b12-31-18-36
10.0.2.15:   batch_size ................... 21
10.0.2.15:   gradient_accumulation_steps .. 1
10.0.2.15:   weight_decay ................. 0.1
10.0.2.15:   checkpoint_activations ....... True
10.0.2.15:   checkpoint_num_layers ........ 1
10.0.2.15:   deepspeed_activation_checkpointing  True
10.0.2.15:   epochs ....................... None
10.0.2.15:   clip_grad .................... 1.0
10.0.2.15:   train_iters .................. 1000
10.0.2.15:   label_smoothing .............. 0.0
10.0.2.15:   log_interval ................. 1
10.0.2.15:   summary_dir .................. 
10.0.2.15:   seed ......................... 1234
10.0.2.15:   reset_position_ids ........... False
10.0.2.15:   reset_attention_mask ......... False
10.0.2.15:   lr_decay_iters ............... 175000
10.0.2.15:   lr_decay_style ............... cosine
10.0.2.15:   lr_decay_ratio ............... 0.1
10.0.2.15:   lr ........................... 0.0001
10.0.2.15:   warmup ....................... 0.04
10.0.2.15:   switch_linear ................ False
10.0.2.15:   save ......................... None
10.0.2.15:   new_save_directory ........... False
10.0.2.15:   save_epoch ................... 1
10.0.2.15:   save_interval ................ 5000
10.0.2.15:   no_save_optim ................ False
10.0.2.15:   no_save_rng .................. False
10.0.2.15:   load ......................... None
10.0.2.15:   no_load_optim ................ False
10.0.2.15:   no_load_rng .................. False
10.0.2.15:   no_load_lr_scheduler ......... False
10.0.2.15:   no_deepspeed_load ............ False
10.0.2.15:   finetune ..................... False
10.0.2.15:   resume_dataloader ............ False
10.0.2.15:   distributed_backend .......... nccl
10.0.2.15:   DDP_impl ..................... torch
10.0.2.15:   local_rank ................... 0
10.0.2.15:   block_lm ..................... True
10.0.2.15:   masked_lm .................... False
10.0.2.15:   bert_prob .................... 0.5
10.0.2.15:   gpt_infill_prob .............. 0.5
10.0.2.15:   gpt_min_ratio ................ 0.25
10.0.2.15:   gap_sentence_prob ............ 0.3
10.0.2.15:   gap_sentence_ratio ........... 0.15
10.0.2.15:   avg_block_length ............. 3
10.0.2.15:   short_seq_prob ............... 0.02
10.0.2.15:   single_span_prob ............. 0.0
10.0.2.15:   task_mask .................... True
10.0.2.15:   no_shuffle_block ............. False
10.0.2.15:   no_block_position ............ False
10.0.2.15:   sentinel_token ............... False
10.0.2.15:   block_mask_prob .............. 0.1
10.0.2.15:   context_mask_ratio ........... 0.0
10.0.2.15:   random_position .............. False
10.0.2.15:   eval_batch_size .............. None
10.0.2.15:   eval_iters ................... 100
10.0.2.15:   eval_interval ................ 1000
10.0.2.15:   eval_epoch ................... 1
10.0.2.15:   eval_seq_length .............. None
10.0.2.15:   eval_max_preds_per_seq ....... None
10.0.2.15:   overlapping_eval ............. 32
10.0.2.15:   temperature .................. 1.0
10.0.2.15:   top_p ........................ 0.0
10.0.2.15:   top_k ........................ 0
10.0.2.15:   out_seq_length ............... 256
10.0.2.15:   num_beams .................... 1
10.0.2.15:   length_penalty ............... 0.0
10.0.2.15:   no_repeat_ngram_size ......... 0
10.0.2.15:   min_tgt_length ............... 0
10.0.2.15:   select_topk .................. False
10.0.2.15:   blank_maskratio .............. 0.1
10.0.2.15:   model_parallel_size .......... 4
10.0.2.15:   shuffle ...................... False
10.0.2.15:   filter_english ............... True
10.0.2.15:   train_data ................... ['pile']
10.0.2.15:   valid_data ................... None
10.0.2.15:   test_data .................... None
10.0.2.15:   data_dir ..................... None
10.0.2.15:   input_data_sizes_file ........ sizes.txt
10.0.2.15:   delim ........................ ,
10.0.2.15:   text_key ..................... sentence
10.0.2.15:   eval_text_key ................ None
10.0.2.15:   split ........................ 1000,0,0
10.0.2.15:   no_lazy_loader ............... False
10.0.2.15:   half_lazy_loader ............. False
10.0.2.15:   loader_scatter ............... 2
10.0.2.15:   loose_json ................... False
10.0.2.15:   presplit_sentences ........... False
10.0.2.15:   num_workers .................. 2
10.0.2.15:   tokenizer_model_type ......... None
10.0.2.15:   tokenizer_path ............... tokenizer.model
10.0.2.15:   tokenizer_type ............... GPT2BPETokenizer
10.0.2.15:   no_pre_tokenize .............. False
10.0.2.15:   cache_dir .................... None
10.0.2.15:   use_tfrecords ................ False
10.0.2.15:   seq_length ................... 512
10.0.2.15:   mem_length ................... 0
10.0.2.15:   max_preds_per_seq ............ None
10.0.2.15:   non_sentence_start ........... 0.0
10.0.2.15:   sample_one_document .......... False
10.0.2.15:   load_splits .................. None
10.0.2.15:   save_splits .................. None
10.0.2.15:   save_test_data ............... None
10.0.2.15:   multi_task_data .............. None
10.0.2.15:   multi_task_ratio ............. 0.0
10.0.2.15:   multi_seq_length ............. None
10.0.2.15:   multi_batch_size ............. None
10.0.2.15:   task ......................... None
10.0.2.15:   load_pretrained .............. None
10.0.2.15:   pool_token ................... cls
10.0.2.15:   cloze_eval ................... False
10.0.2.15:   multi_token .................. False
10.0.2.15:   segment_length ............... 0
10.0.2.15:   loss_func .................... cross_entropy
10.0.2.15:   block_lm_ratio ............... 0.0
10.0.2.15:   adapet ....................... False
10.0.2.15:   pattern_id ................... 0
10.0.2.15:   fast_decode .................. False
10.0.2.15:   few_superglue ................ False
10.0.2.15:   eval_valid ................... False
10.0.2.15:   validation_metric ............ None
10.0.2.15:   unidirectional ............... False
10.0.2.15:   src_seq_length ............... None
10.0.2.15:   tgt_seq_length ............... None
10.0.2.15:   adam_beta1 ................... 0.9
10.0.2.15:   adam_beta2 ................... 0.999
10.0.2.15:   adam_eps ..................... 1e-08
10.0.2.15:   optimizer .................... adam
10.0.2.15:   wsc_negative ................. False
10.0.2.15:   overwrite .................... False
10.0.2.15:   no_validation ................ False
10.0.2.15:   continuous_prompt ............ False
10.0.2.15:   num_prompt_tokens ............ 0
10.0.2.15:   prompt_func .................. lstm
10.0.2.15:   freeze_transformer ........... False
10.0.2.15:   tune_prefix_layers ........... None
10.0.2.15:   prefix_prompt ................ 0
10.0.2.15:   prompt_init .................. False
10.0.2.15:   deepspeed .................... True
10.0.2.15:   deepspeed_config ............. /home/zhangchi1/glm_acc/config/config_block_10B_acc_ac1.json
10.0.2.15:   deepscale .................... False
10.0.2.15:   deepscale_config ............. None
10.0.2.15:   deepspeed_mpi ................ False
10.0.2.15:   cuda ......................... True
10.0.2.15:   rank ......................... 0
10.0.2.15:   world_size ................... 16
10.0.2.15:   dynamic_loss_scale ........... True
10.0.2.15:   master_ip .................... 10.0.2.15
10.0.2.15:   master_port .................. 40991
10.0.2.15:   eod_token .................... 50256
10.0.2.15:   persist_state ................ 0
10.0.2.15:   lazy ......................... False
10.0.2.15:   transpose .................... False
10.0.2.15:   data_set_type ................ Block
10.0.2.15:   samples_per_shard ............ 100
10.0.2.15:   do_train ..................... 1
10.0.2.15:   do_valid ..................... 0
10.0.2.15:   do_test ...................... 0
10.0.2.15:   iteration .................... 0
10.0.2.15:   log_dir ...................... runs/blocklm-10b12-31-18-36
10.0.2.15: [2021-12-31 18:38:16,074] [INFO] [checkpointing.py:400:forward] Activation Checkpointing Information
10.0.2.15: [2021-12-31 18:38:16,074] [INFO] [checkpointing.py:401:forward] ----Partition Activations False, CPU CHECKPOINTING False
10.0.2.15: [2021-12-31 18:38:16,074] [INFO] [checkpointing.py:404:forward] ----contiguous Memory Checkpointing False with 48 total layers
10.0.2.15: [2021-12-31 18:38:16,074] [INFO] [checkpointing.py:407:forward] ----Synchronization False
10.0.2.15: [2021-12-31 18:38:16,074] [INFO] [checkpointing.py:408:forward] ----Profiling False
10.0.2.15: [2021-12-31 18:38:43,384] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.15: [2021-12-31 18:38:43,384] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.15: [2021-12-31 18:38:43,384] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.15: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.15: [2021-12-31 18:38:43,384] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.16: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.15: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.16: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.16: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.16: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.16: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.15: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.16: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.15:  iteration        1/    1000 | elapsed time per iteration (ms): 28246.7 | learning rate 0.000E+00 | lm loss 1.186496E+01 | loss scale 4294967296.0 |
10.0.2.16: [2021-12-31 18:38:43,386] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.16: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.15: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.15:   warnings.warn(
10.0.2.15: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.15:   warnings.warn(
10.0.2.15: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.15:   warnings.warn(
10.0.2.16: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.16:   warnings.warn(
10.0.2.15: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.15:   warnings.warn(
10.0.2.16: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.16:   warnings.warn(
10.0.2.15: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.15:   warnings.warn(
10.0.2.16: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.16:   warnings.warn(
10.0.2.16: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.16:   warnings.warn(
10.0.2.16: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.16:   warnings.warn(
10.0.2.15: [2021-12-31 18:38:43,385] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.16: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.16:   warnings.warn(
10.0.2.15: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.15:   warnings.warn(
10.0.2.15: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.15:   warnings.warn(
10.0.2.15: after 1 iterations memory (MB) | allocated: 11815.8447265625 | max allocated: 18863.68603515625 | cached: 24040.0 | max cached: 24040.0
10.0.2.15: time (ms) | forward: 4878.71 | backward: 23297.45 | optimizer: 69.28 | batch generator: 928.49 | data loader: 924.50
10.0.2.16: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.16:   warnings.warn(
10.0.2.16: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.16:   warnings.warn(
10.0.2.15: /opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
10.0.2.15:   warnings.warn(
10.0.2.15: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.15: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.16: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.16: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.16: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.16: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.15: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.15: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.15: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.15: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.15: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.15: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.16: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.16: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.16: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.16: [2021-12-31 18:39:09,834] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.15:  iteration        2/    1000 | elapsed time per iteration (ms): 26449.4 | learning rate 0.000E+00 | lm loss 1.182608E+01 | loss scale 2147483648.0 |
10.0.2.15: time (ms) | forward: 2746.26 | backward: 23182.66 | optimizer: 34.39 | batch generator: 2.45 | data loader: 1.13
10.0.2.15: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.15: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.16: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.15: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.16: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.16: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.15: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.16: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.15: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.16: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.15: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.15: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.15: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.16: [2021-12-31 18:39:36,472] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.16: [2021-12-31 18:39:36,473] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.16: [2021-12-31 18:39:36,473] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.15:  iteration        3/    1000 | elapsed time per iteration (ms): 26638.3 | learning rate 0.000E+00 | lm loss 1.181565E+01 | loss scale 1073741824.0 |
10.0.2.15: time (ms) | forward: 2712.67 | backward: 23473.07 | optimizer: 126.63 | batch generator: 2.37 | data loader: 1.06
10.0.2.16: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.16: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.16: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.15: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.15: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.16: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.15: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.15: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.15: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.16: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.16: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.16: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.16: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.15: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.15: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.15: [2021-12-31 18:40:02,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.15:  iteration        4/    1000 | elapsed time per iteration (ms): 26470.5 | learning rate 0.000E+00 | lm loss 1.173506E+01 | loss scale 536870912.0 |
10.0.2.15: time (ms) | forward: 2770.34 | backward: 23688.56 | optimizer: 9.61 | batch generator: 1.60 | data loader: 0.38
10.0.2.15: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.15: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.16: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.16: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.15: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.16: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.16: [2021-12-31 18:40:28,787] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.15: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.16: [2021-12-31 18:40:28,787] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.16: [2021-12-31 18:40:28,787] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.15: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.15: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.15: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.15: [2021-12-31 18:40:28,786] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.16: [2021-12-31 18:40:28,787] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.16: [2021-12-31 18:40:28,787] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.15:  iteration        5/    1000 | elapsed time per iteration (ms): 25843.5 | learning rate 0.000E+00 | lm loss 1.179348E+01 | loss scale 268435456.0 |
10.0.2.15: time (ms) | forward: 2736.68 | backward: 23067.39 | optimizer: 38.10 | batch generator: 2.21 | data loader: 0.96
10.0.2.15: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.15: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.15: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.15: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.15: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.16: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.16: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.16: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.16: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.16: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.15: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.15: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.16: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.15: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.16: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.16: [2021-12-31 18:40:54,223] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.15:  iteration        6/    1000 | elapsed time per iteration (ms): 25436.8 | learning rate 0.000E+00 | lm loss 1.176296E+01 | loss scale 134217728.0 |
10.0.2.15: time (ms) | forward: 2569.72 | backward: 22532.50 | optimizer: 259.29 | batch generator: 2.25 | data loader: 0.99
10.0.2.15: [2021-12-31 18:41:20,185] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.15: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.15: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.15: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.16: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.16: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.16: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.16: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.15: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.15: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.15: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.16: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.15: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.16: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.16: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.16: [2021-12-31 18:41:20,186] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.15:  iteration        7/    1000 | elapsed time per iteration (ms): 25962.6 | learning rate 0.000E+00 | lm loss 1.177627E+01 | loss scale 67108864.0 |
10.0.2.15: time (ms) | forward: 2713.84 | backward: 23069.74 | optimizer: 177.88 | batch generator: 1.86 | data loader: 0.75
10.0.2.15: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.16: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.15: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.16: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.15: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.15: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.16: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.15: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.16: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.15: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.16: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.16: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.16: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.15: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.15: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.16: [2021-12-31 18:41:46,329] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.15:  iteration        8/    1000 | elapsed time per iteration (ms): 26143.3 | learning rate 0.000E+00 | lm loss 1.177440E+01 | loss scale 33554432.0 |
10.0.2.15: time (ms) | forward: 2526.51 | backward: 23056.02 | optimizer: 149.66 | batch generator: 1.57 | data loader: 0.34
10.0.2.15: b' relevant organizing of these trips for you too Hunting rifles are available on hire if required The rate will be specified on your quotation Package Includes Services of licensed professional hunters Expert skinners and trackers Hospitality services Accommodation All meals Snacks Daily laundry services Refreshments Transportation during the hunt Rifle and ammunition if required Crossbow and Compound Bow available on request Package Excludes Trophy fees of animals shot or wounded Hotel accommodation before and or after the safari Commercial and charter flights between hunting areas Gratuities Government licenses fees Transport of trophies to taxidermist Taxidermy costs shipping and dipping Personal effects All additional off ranch excursions Translation Quick Search Staff Gallery Facilities Gallery Guest Feedback Fantastic Experience by far my best hunting trip Adriaan Roohtman Wristwatch Dealer Sandton South Africa Friendly People fantastic accomodation and great safari experiences Lodewyk Loubser Multimedia Designer Pretoria South Africa Contact Centre For further information Address FM Safaris Pty Ltd Farms Onderplaatz 401 Zonderhuis 402 Zwartkop 403 P O Box 168 Grootdrink 8822 Northern Cape South Africa Telephone 27 0 83 264 8725 E Mail kalahari fmsafaris co za Associations FM Safaris is a member of the following associations Safari Club International Professional Hunter Association<|endoftext|>'
10.0.2.15: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.16: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.15: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.16: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.15: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.16: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.16: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.15: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.16: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.16: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.15: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.15: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.15: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.15: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.16: [2021-12-31 18:42:11,988] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.16: [2021-12-31 18:42:11,987] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.15:  iteration        9/    1000 | elapsed time per iteration (ms): 25658.3 | learning rate 0.000E+00 | lm loss 1.174193E+01 | loss scale 16777216.0 |
10.0.2.15: time (ms) | forward: 2643.43 | backward: 22722.08 | optimizer: 183.12 | batch generator: 1.56 | data loader: 0.36
10.0.2.15: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.15: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.15: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.15: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.15: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.16: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.16: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.16: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.16: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.16: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.16: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.16: [2021-12-31 18:42:37,943] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.15: [2021-12-31 18:42:37,944] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.15: [2021-12-31 18:42:37,944] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.15: [2021-12-31 18:42:37,944] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.16: [2021-12-31 18:42:37,944] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.15:  iteration       10/    1000 | elapsed time per iteration (ms): 25956.0 | learning rate 0.000E+00 | lm loss 1.173808E+01 | loss scale 8388608.0 |
10.0.2.15: time (ms) | forward: 2471.03 | backward: 22857.37 | optimizer: 183.79 | batch generator: 1.60 | data loader: 0.40
10.0.2.15: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.15: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.15: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.15: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.16: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.16: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.16: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.16: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.16: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.16: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.16: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.16: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.15: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.15: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.15: [2021-12-31 18:43:03,349] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.15: [2021-12-31 18:43:03,350] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.15:  iteration       11/    1000 | elapsed time per iteration (ms): 25405.8 | learning rate 0.000E+00 | lm loss 1.189436E+01 | loss scale 4194304.0 |
10.0.2.15: time (ms) | forward: 2737.38 | backward: 22658.72 | optimizer: 8.64 | batch generator: 1.80 | data loader: 0.69
10.0.2.15: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.15: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.15: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.15: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.16: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.15: [2021-12-31 18:43:29,688] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.16: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.15: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.16: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.15: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.16: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.16: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.15: [2021-12-31 18:43:29,688] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.16: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.16: [2021-12-31 18:43:29,687] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.16: [2021-12-31 18:43:29,688] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.15:  iteration       12/    1000 | elapsed time per iteration (ms): 26338.1 | learning rate 0.000E+00 | lm loss 1.178531E+01 | loss scale 2097152.0 |
10.0.2.15: time (ms) | forward: 2691.00 | backward: 23270.53 | optimizer: 17.81 | batch generator: 1.54 | data loader: 0.37
10.0.2.15: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.15: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.15: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.15: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.16: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.16: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.15: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.16: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.16: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.15: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.16: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.16: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.16: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.15: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.15: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.16: [2021-12-31 18:43:55,853] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.15:  iteration       13/    1000 | elapsed time per iteration (ms): 26165.8 | learning rate 0.000E+00 | lm loss 1.176471E+01 | loss scale 1048576.0 |
10.0.2.15: time (ms) | forward: 2508.60 | backward: 23401.69 | optimizer: 23.86 | batch generator: 1.49 | data loader: 0.35
10.0.2.15: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.15: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.16: [2021-12-31 18:44:22,254] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.15: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.16: [2021-12-31 18:44:22,254] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.15: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.15: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.16: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.16: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.16: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.16: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.15: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.15: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.15: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.16: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.16: [2021-12-31 18:44:22,255] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.15:  iteration       14/    1000 | elapsed time per iteration (ms): 26401.6 | learning rate 0.000E+00 | lm loss 1.175068E+01 | loss scale 524288.0 |
10.0.2.15: time (ms) | forward: 2818.74 | backward: 23455.76 | optimizer: 125.99 | batch generator: 1.83 | data loader: 0.71
10.0.2.15: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.15: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.15: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.15: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.16: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.15: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.16: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.16: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.16: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.16: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.16: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.15: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.16: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.15: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.15: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.16: [2021-12-31 18:44:49,567] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.15:  iteration       15/    1000 | elapsed time per iteration (ms): 27312.3 | learning rate 0.000E+00 | lm loss 1.177785E+01 | loss scale 262144.0 |
10.0.2.15: time (ms) | forward: 2969.19 | backward: 23960.60 | optimizer: 340.82 | batch generator: 2.10 | data loader: 0.87
10.0.2.15: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.15: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.16: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.16: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.15: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.15: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.16: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.16: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.16: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.16: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.16: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.15: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.15: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.15: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.16: [2021-12-31 18:45:15,595] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.15: [2021-12-31 18:45:15,596] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.15:  iteration       16/    1000 | elapsed time per iteration (ms): 26028.5 | learning rate 0.000E+00 | lm loss 1.173304E+01 | loss scale 131072.0 |
10.0.2.15: time (ms) | forward: 2658.25 | backward: 23051.14 | optimizer: 318.03 | batch generator: 1.81 | data loader: 0.70
10.0.2.15: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.16: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.16: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.16: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.16: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.15: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.15: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.15: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.15: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.15: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.15: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.15: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.16: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.16: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.16: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.16: [2021-12-31 18:45:42,545] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.15:  iteration       17/    1000 | elapsed time per iteration (ms): 26949.3 | learning rate 0.000E+00 | lm loss 1.178822E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 2934.96 | backward: 23984.66 | optimizer: 28.40 | batch generator: 2.20 | data loader: 0.98
10.0.2.15:  iteration       18/    1000 | elapsed time per iteration (ms): 38340.0 | learning rate 1.429E-08 | lm loss 1.184158E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 2767.17 | backward: 23029.34 | optimizer: 12541.22 | batch generator: 1.57 | data loader: 0.36
10.0.2.15:  iteration       19/    1000 | elapsed time per iteration (ms): 40013.9 | learning rate 2.857E-08 | lm loss 1.178018E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 3553.75 | backward: 23301.94 | optimizer: 12717.13 | batch generator: 1.53 | data loader: 0.39
10.0.2.15:  iteration       20/    1000 | elapsed time per iteration (ms): 40039.8 | learning rate 4.286E-08 | lm loss 1.181200E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 4175.37 | backward: 23225.11 | optimizer: 12504.45 | batch generator: 1.83 | data loader: 0.45
10.0.2.15:  iteration       21/    1000 | elapsed time per iteration (ms): 39519.4 | learning rate 5.714E-08 | lm loss 1.177392E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 3719.72 | backward: 23041.64 | optimizer: 12319.46 | batch generator: 1.84 | data loader: 0.74
10.0.2.15:  iteration       22/    1000 | elapsed time per iteration (ms): 39760.8 | learning rate 7.143E-08 | lm loss 1.178064E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 4175.07 | backward: 23020.16 | optimizer: 12563.87 | batch generator: 2.49 | data loader: 1.10
10.0.2.15:  iteration       23/    1000 | elapsed time per iteration (ms): 39328.8 | learning rate 8.571E-08 | lm loss 1.163725E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 3755.00 | backward: 23391.41 | optimizer: 12092.32 | batch generator: 1.64 | data loader: 0.36
10.0.2.15:  iteration       24/    1000 | elapsed time per iteration (ms): 39351.4 | learning rate 1.000E-07 | lm loss 1.161667E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 3910.19 | backward: 22964.66 | optimizer: 12139.59 | batch generator: 1.67 | data loader: 0.36
10.0.2.15:  iteration       25/    1000 | elapsed time per iteration (ms): 39450.8 | learning rate 1.143E-07 | lm loss 1.100135E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 4336.50 | backward: 22838.08 | optimizer: 12078.03 | batch generator: 1.51 | data loader: 0.32
10.0.2.15:  iteration       26/    1000 | elapsed time per iteration (ms): 39823.1 | learning rate 1.286E-07 | lm loss 1.122771E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 4018.93 | backward: 23451.98 | optimizer: 12007.00 | batch generator: 1.89 | data loader: 0.59
10.0.2.15:  iteration       27/    1000 | elapsed time per iteration (ms): 38989.8 | learning rate 1.429E-07 | lm loss 1.082856E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 4231.52 | backward: 23114.89 | optimizer: 11466.85 | batch generator: 1.77 | data loader: 0.56
10.0.2.15:  iteration       28/    1000 | elapsed time per iteration (ms): 38729.9 | learning rate 1.571E-07 | lm loss 1.113859E+01 | loss scale 65536.0 |
10.0.2.15: time (ms) | forward: 4772.60 | backward: 22724.96 | optimizer: 11230.46 | batch generator: 1.90 | data loader: 0.57
10.0.2.15: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.15: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.15: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.15: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.16: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.16: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.16: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.16: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.15: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.15: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.15: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.16: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.15: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.16: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.16: [2021-12-31 18:53:24,391] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.15:  iteration       29/    1000 | elapsed time per iteration (ms): 28498.5 | learning rate 1.571E-07 | lm loss 1.031809E+01 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4775.52 | backward: 23324.27 | optimizer: 63.68 | batch generator: 1.83 | data loader: 0.56
10.0.2.16: [2021-12-31 18:53:24,392] [INFO] [stage2.py:1407:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.15:  iteration       30/    1000 | elapsed time per iteration (ms): 36391.8 | learning rate 1.714E-07 | lm loss 9.901514E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2678.16 | backward: 22960.28 | optimizer: 10600.46 | batch generator: 1.53 | data loader: 0.31
10.0.2.15:  iteration       31/    1000 | elapsed time per iteration (ms): 40053.8 | learning rate 1.857E-07 | lm loss 9.466600E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5282.71 | backward: 23212.87 | optimizer: 11206.58 | batch generator: 1.68 | data loader: 0.50
10.0.2.15:  iteration       32/    1000 | elapsed time per iteration (ms): 38975.2 | learning rate 2.000E-07 | lm loss 9.606087E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5518.26 | backward: 22956.63 | optimizer: 10498.48 | batch generator: 1.79 | data loader: 0.37
10.0.2.15:  iteration       33/    1000 | elapsed time per iteration (ms): 40766.4 | learning rate 2.143E-07 | lm loss 9.289608E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5511.79 | backward: 22753.77 | optimizer: 11983.90 | batch generator: 1.75 | data loader: 0.55
10.0.2.15:  iteration       34/    1000 | elapsed time per iteration (ms): 39189.4 | learning rate 2.286E-07 | lm loss 9.249472E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4097.41 | backward: 22763.21 | optimizer: 11888.89 | batch generator: 1.67 | data loader: 0.36
10.0.2.15:  iteration       35/    1000 | elapsed time per iteration (ms): 39737.3 | learning rate 2.429E-07 | lm loss 9.441105E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5050.74 | backward: 23108.15 | optimizer: 11576.94 | batch generator: 1.48 | data loader: 0.27
10.0.2.15:  iteration       36/    1000 | elapsed time per iteration (ms): 40146.1 | learning rate 2.571E-07 | lm loss 9.083718E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4999.79 | backward: 23353.40 | optimizer: 11751.11 | batch generator: 1.82 | data loader: 0.55
10.0.2.15:  iteration       37/    1000 | elapsed time per iteration (ms): 39367.8 | learning rate 2.714E-07 | lm loss 9.719864E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4365.49 | backward: 23057.09 | optimizer: 11923.62 | batch generator: 1.48 | data loader: 0.31
10.0.2.15:  iteration       38/    1000 | elapsed time per iteration (ms): 39491.4 | learning rate 2.857E-07 | lm loss 9.053646E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4211.68 | backward: 23424.87 | optimizer: 11406.44 | batch generator: 1.94 | data loader: 0.56
10.0.2.15:  iteration       39/    1000 | elapsed time per iteration (ms): 39336.8 | learning rate 3.000E-07 | lm loss 8.846729E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4941.27 | backward: 22473.12 | optimizer: 11920.80 | batch generator: 1.63 | data loader: 0.47
10.0.2.15:  iteration       40/    1000 | elapsed time per iteration (ms): 39930.9 | learning rate 3.143E-07 | lm loss 8.660440E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4900.31 | backward: 22846.97 | optimizer: 12181.83 | batch generator: 1.98 | data loader: 0.60
10.0.2.15:  iteration       41/    1000 | elapsed time per iteration (ms): 39214.3 | learning rate 3.286E-07 | lm loss 8.317438E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4190.34 | backward: 22693.10 | optimizer: 11877.01 | batch generator: 1.67 | data loader: 0.48
10.0.2.15:  iteration       42/    1000 | elapsed time per iteration (ms): 38802.9 | learning rate 3.429E-07 | lm loss 8.422792E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4423.61 | backward: 22874.75 | optimizer: 11503.27 | batch generator: 1.73 | data loader: 0.55
10.0.2.15:  iteration       43/    1000 | elapsed time per iteration (ms): 39643.2 | learning rate 3.571E-07 | lm loss 9.157135E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4861.53 | backward: 22375.69 | optimizer: 12404.11 | batch generator: 1.68 | data loader: 0.35
10.0.2.15:  iteration       44/    1000 | elapsed time per iteration (ms): 39177.9 | learning rate 3.714E-07 | lm loss 9.179625E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4032.48 | backward: 22856.72 | optimizer: 12208.21 | batch generator: 1.62 | data loader: 0.49
10.0.2.15:  iteration       45/    1000 | elapsed time per iteration (ms): 40353.0 | learning rate 3.857E-07 | lm loss 9.389544E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4043.37 | backward: 22696.12 | optimizer: 13611.74 | batch generator: 1.87 | data loader: 0.57
10.0.2.15:  iteration       46/    1000 | elapsed time per iteration (ms): 39840.9 | learning rate 4.000E-07 | lm loss 8.625862E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2672.09 | backward: 23329.13 | optimizer: 13465.99 | batch generator: 1.40 | data loader: 0.29
10.0.2.15:  iteration       47/    1000 | elapsed time per iteration (ms): 38482.5 | learning rate 4.143E-07 | lm loss 8.266363E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2588.05 | backward: 22548.98 | optimizer: 12920.37 | batch generator: 1.33 | data loader: 0.26
10.0.2.15:  iteration       48/    1000 | elapsed time per iteration (ms): 39507.6 | learning rate 4.286E-07 | lm loss 8.772473E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3459.96 | backward: 22733.16 | optimizer: 13301.98 | batch generator: 1.82 | data loader: 0.37
10.0.2.15:  iteration       49/    1000 | elapsed time per iteration (ms): 39213.4 | learning rate 4.429E-07 | lm loss 8.113410E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3232.55 | backward: 22550.12 | optimizer: 13429.53 | batch generator: 1.80 | data loader: 0.69
10.0.2.15: [2021-12-31 19:06:58,080] [INFO] [logging.py:60:log_dist] [Rank 0] step=50, skipped=18, lr=[4.428571428571429e-07, 4.428571428571429e-07], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 19:07:11,048] [INFO] [timer.py:154:stop] 0/50, SamplesPerSec=2.398503844781557
10.0.2.15:  iteration       50/    1000 | elapsed time per iteration (ms): 39034.0 | learning rate 4.571E-07 | lm loss 7.797390E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3017.57 | backward: 22474.15 | optimizer: 13261.74 | batch generator: 1.79 | data loader: 0.65
10.0.2.15:  iteration       51/    1000 | elapsed time per iteration (ms): 39483.9 | learning rate 4.714E-07 | lm loss 8.439663E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3269.16 | backward: 22907.60 | optimizer: 13095.69 | batch generator: 1.89 | data loader: 0.72
10.0.2.15:  iteration       52/    1000 | elapsed time per iteration (ms): 39179.4 | learning rate 4.857E-07 | lm loss 9.818301E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2982.50 | backward: 22941.48 | optimizer: 13089.34 | batch generator: 2.40 | data loader: 1.17
10.0.2.15:  iteration       53/    1000 | elapsed time per iteration (ms): 40338.6 | learning rate 5.000E-07 | lm loss 8.828419E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3552.32 | backward: 23701.97 | optimizer: 13082.78 | batch generator: 1.98 | data loader: 0.75
10.0.2.15:  iteration       54/    1000 | elapsed time per iteration (ms): 39295.5 | learning rate 5.143E-07 | lm loss 7.988008E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3318.41 | backward: 22583.14 | optimizer: 13002.36 | batch generator: 2.53 | data loader: 1.18
10.0.2.15:  iteration       55/    1000 | elapsed time per iteration (ms): 38616.7 | learning rate 5.286E-07 | lm loss 8.377117E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3094.65 | backward: 22029.64 | optimizer: 12970.77 | batch generator: 2.13 | data loader: 0.96
10.0.2.15:  iteration       56/    1000 | elapsed time per iteration (ms): 40034.1 | learning rate 5.429E-07 | lm loss 7.732873E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3940.85 | backward: 23009.22 | optimizer: 13082.28 | batch generator: 2.39 | data loader: 0.99
10.0.2.15:  iteration       57/    1000 | elapsed time per iteration (ms): 38759.2 | learning rate 5.571E-07 | lm loss 8.486593E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3203.30 | backward: 22833.06 | optimizer: 12560.27 | batch generator: 2.29 | data loader: 1.12
10.0.2.15:  iteration       58/    1000 | elapsed time per iteration (ms): 38342.2 | learning rate 5.714E-07 | lm loss 8.714685E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3675.76 | backward: 22660.13 | optimizer: 12004.86 | batch generator: 2.45 | data loader: 1.15
10.0.2.15:  iteration       59/    1000 | elapsed time per iteration (ms): 39774.9 | learning rate 5.857E-07 | lm loss 7.611493E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4333.62 | backward: 23246.79 | optimizer: 12103.99 | batch generator: 1.83 | data loader: 0.72
10.0.2.15:  iteration       60/    1000 | elapsed time per iteration (ms): 40109.3 | learning rate 6.000E-07 | lm loss 8.573498E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4431.09 | backward: 23231.18 | optimizer: 12233.49 | batch generator: 1.79 | data loader: 0.40
10.0.2.15:  iteration       61/    1000 | elapsed time per iteration (ms): 40159.5 | learning rate 6.143E-07 | lm loss 8.059191E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4176.33 | backward: 22984.75 | optimizer: 12502.99 | batch generator: 1.98 | data loader: 0.89
10.0.2.15:  iteration       62/    1000 | elapsed time per iteration (ms): 39028.3 | learning rate 6.286E-07 | lm loss 8.029762E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4128.51 | backward: 22627.75 | optimizer: 12101.65 | batch generator: 1.92 | data loader: 0.74
10.0.2.15:  iteration       63/    1000 | elapsed time per iteration (ms): 40654.3 | learning rate 6.429E-07 | lm loss 7.771704E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3963.07 | backward: 23283.20 | optimizer: 13176.89 | batch generator: 1.80 | data loader: 0.54
10.0.2.15:  iteration       64/    1000 | elapsed time per iteration (ms): 38805.7 | learning rate 6.571E-07 | lm loss 9.016338E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2873.34 | backward: 22421.93 | optimizer: 13295.91 | batch generator: 2.37 | data loader: 0.98
10.0.2.15:  iteration       65/    1000 | elapsed time per iteration (ms): 39171.1 | learning rate 6.714E-07 | lm loss 8.019903E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3202.99 | backward: 22610.63 | optimizer: 13037.07 | batch generator: 1.76 | data loader: 0.38
10.0.2.15:  iteration       66/    1000 | elapsed time per iteration (ms): 39842.6 | learning rate 6.857E-07 | lm loss 7.792210E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3175.86 | backward: 23279.95 | optimizer: 13146.91 | batch generator: 1.83 | data loader: 0.39
10.0.2.15:  iteration       67/    1000 | elapsed time per iteration (ms): 39552.2 | learning rate 7.000E-07 | lm loss 7.223746E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3214.39 | backward: 23081.08 | optimizer: 13094.81 | batch generator: 1.90 | data loader: 0.56
10.0.2.15:  iteration       68/    1000 | elapsed time per iteration (ms): 39711.8 | learning rate 7.143E-07 | lm loss 7.833541E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3262.15 | backward: 23219.49 | optimizer: 13224.01 | batch generator: 2.57 | data loader: 1.21
10.0.2.15:  iteration       69/    1000 | elapsed time per iteration (ms): 39112.3 | learning rate 7.286E-07 | lm loss 7.814915E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3151.67 | backward: 22619.55 | optimizer: 13339.71 | batch generator: 1.93 | data loader: 0.75
10.0.2.15:  iteration       70/    1000 | elapsed time per iteration (ms): 38251.1 | learning rate 7.429E-07 | lm loss 7.393993E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3155.97 | backward: 22896.32 | optimizer: 12096.91 | batch generator: 2.53 | data loader: 1.18
10.0.2.15:  iteration       71/    1000 | elapsed time per iteration (ms): 39074.3 | learning rate 7.571E-07 | lm loss 8.281199E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4155.03 | backward: 22524.98 | optimizer: 12081.56 | batch generator: 1.60 | data loader: 0.47
10.0.2.15:  iteration       72/    1000 | elapsed time per iteration (ms): 40833.7 | learning rate 7.714E-07 | lm loss 7.332567E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4714.41 | backward: 23209.68 | optimizer: 12844.16 | batch generator: 1.88 | data loader: 0.86
10.0.2.15:  iteration       73/    1000 | elapsed time per iteration (ms): 39401.8 | learning rate 7.857E-07 | lm loss 7.723161E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3657.77 | backward: 22682.68 | optimizer: 12808.07 | batch generator: 2.00 | data loader: 0.48
10.0.2.15:  iteration       74/    1000 | elapsed time per iteration (ms): 39869.0 | learning rate 8.000E-07 | lm loss 7.248054E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3640.04 | backward: 22985.77 | optimizer: 13241.13 | batch generator: 1.72 | data loader: 0.40
10.0.2.15:  iteration       75/    1000 | elapsed time per iteration (ms): 38916.2 | learning rate 8.143E-07 | lm loss 7.539786E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3188.28 | backward: 22587.56 | optimizer: 13090.22 | batch generator: 1.88 | data loader: 0.71
10.0.2.15:  iteration       76/    1000 | elapsed time per iteration (ms): 39573.5 | learning rate 8.286E-07 | lm loss 7.015267E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3569.39 | backward: 23328.17 | optimizer: 12673.78 | batch generator: 2.16 | data loader: 0.78
10.0.2.15:  iteration       77/    1000 | elapsed time per iteration (ms): 39758.8 | learning rate 8.429E-07 | lm loss 8.177653E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3560.16 | backward: 23075.35 | optimizer: 12881.29 | batch generator: 2.06 | data loader: 0.92
10.0.2.15:  iteration       78/    1000 | elapsed time per iteration (ms): 39014.3 | learning rate 8.571E-07 | lm loss 7.185265E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3375.05 | backward: 22562.72 | optimizer: 12874.59 | batch generator: 2.31 | data loader: 0.96
10.0.2.15:  iteration       79/    1000 | elapsed time per iteration (ms): 39560.2 | learning rate 8.714E-07 | lm loss 7.151803E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3589.05 | backward: 22861.17 | optimizer: 13036.61 | batch generator: 2.19 | data loader: 0.89
10.0.2.15:  iteration       80/    1000 | elapsed time per iteration (ms): 39515.8 | learning rate 8.857E-07 | lm loss 7.390525E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3814.21 | backward: 22704.19 | optimizer: 12995.62 | batch generator: 2.39 | data loader: 1.00
10.0.2.15:  iteration       81/    1000 | elapsed time per iteration (ms): 40396.5 | learning rate 9.000E-07 | lm loss 7.052075E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3934.17 | backward: 23682.88 | optimizer: 12778.07 | batch generator: 2.02 | data loader: 0.74
10.0.2.15:  iteration       82/    1000 | elapsed time per iteration (ms): 38851.0 | learning rate 9.143E-07 | lm loss 7.867494E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3245.18 | backward: 22520.12 | optimizer: 12815.96 | batch generator: 2.85 | data loader: 1.33
10.0.2.15:  iteration       83/    1000 | elapsed time per iteration (ms): 40051.2 | learning rate 9.286E-07 | lm loss 7.367122E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3489.64 | backward: 23001.75 | optimizer: 13090.16 | batch generator: 2.02 | data loader: 0.79
10.0.2.15:  iteration       84/    1000 | elapsed time per iteration (ms): 38975.8 | learning rate 9.429E-07 | lm loss 8.648334E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3017.97 | backward: 22618.81 | optimizer: 13161.59 | batch generator: 2.59 | data loader: 1.09
10.0.2.15:  iteration       85/    1000 | elapsed time per iteration (ms): 39117.9 | learning rate 9.571E-07 | lm loss 7.249007E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2836.27 | backward: 22917.73 | optimizer: 12913.70 | batch generator: 2.28 | data loader: 1.10
10.0.2.15:  iteration       86/    1000 | elapsed time per iteration (ms): 39062.4 | learning rate 9.714E-07 | lm loss 7.702666E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3520.99 | backward: 22592.14 | optimizer: 12947.99 | batch generator: 2.16 | data loader: 0.97
10.0.2.15:  iteration       87/    1000 | elapsed time per iteration (ms): 38934.2 | learning rate 9.857E-07 | lm loss 8.574126E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3091.99 | backward: 22478.23 | optimizer: 13105.65 | batch generator: 2.03 | data loader: 0.92
10.0.2.15:  iteration       88/    1000 | elapsed time per iteration (ms): 39211.0 | learning rate 1.000E-06 | lm loss 7.471737E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3325.29 | backward: 22626.61 | optimizer: 13050.81 | batch generator: 2.93 | data loader: 1.16
10.0.2.15:  iteration       89/    1000 | elapsed time per iteration (ms): 39541.6 | learning rate 1.014E-06 | lm loss 7.568968E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3459.99 | backward: 22793.70 | optimizer: 13282.43 | batch generator: 2.48 | data loader: 1.08
10.0.2.15:  iteration       90/    1000 | elapsed time per iteration (ms): 39830.3 | learning rate 1.029E-06 | lm loss 7.275494E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3125.66 | backward: 23111.67 | optimizer: 13400.10 | batch generator: 2.52 | data loader: 1.18
10.0.2.15:  iteration       91/    1000 | elapsed time per iteration (ms): 39213.5 | learning rate 1.043E-06 | lm loss 8.576802E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2986.39 | backward: 22897.03 | optimizer: 13328.52 | batch generator: 2.42 | data loader: 1.01
10.0.2.15:  iteration       92/    1000 | elapsed time per iteration (ms): 39122.2 | learning rate 1.057E-06 | lm loss 8.551196E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2653.67 | backward: 22903.05 | optimizer: 13250.84 | batch generator: 2.17 | data loader: 0.98
10.0.2.15:  iteration       93/    1000 | elapsed time per iteration (ms): 39156.1 | learning rate 1.071E-06 | lm loss 8.096256E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3292.30 | backward: 22450.13 | optimizer: 13411.45 | batch generator: 1.67 | data loader: 0.36
10.0.2.15:  iteration       94/    1000 | elapsed time per iteration (ms): 39228.7 | learning rate 1.086E-06 | lm loss 8.060737E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3088.23 | backward: 22596.92 | optimizer: 13295.10 | batch generator: 2.38 | data loader: 1.20
10.0.2.15:  iteration       95/    1000 | elapsed time per iteration (ms): 39740.8 | learning rate 1.100E-06 | lm loss 7.849505E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3428.47 | backward: 22791.65 | optimizer: 13519.36 | batch generator: 2.11 | data loader: 0.96
10.0.2.15:  iteration       96/    1000 | elapsed time per iteration (ms): 39999.7 | learning rate 1.114E-06 | lm loss 7.401388E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3376.35 | backward: 23368.86 | optimizer: 13252.38 | batch generator: 1.88 | data loader: 0.71
10.0.2.15:  iteration       97/    1000 | elapsed time per iteration (ms): 39352.9 | learning rate 1.129E-06 | lm loss 8.035574E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2955.37 | backward: 23170.77 | optimizer: 13181.92 | batch generator: 1.63 | data loader: 0.49
10.0.2.15:  iteration       98/    1000 | elapsed time per iteration (ms): 39513.7 | learning rate 1.143E-06 | lm loss 8.048319E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3370.80 | backward: 22897.85 | optimizer: 13243.58 | batch generator: 2.33 | data loader: 1.14
10.0.2.15:  iteration       99/    1000 | elapsed time per iteration (ms): 39650.2 | learning rate 1.157E-06 | lm loss 7.567081E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2895.29 | backward: 23010.69 | optimizer: 13408.34 | batch generator: 1.48 | data loader: 0.29
10.0.2.15: [2021-12-31 19:39:49,960] [INFO] [logging.py:60:log_dist] [Rank 0] step=100, skipped=18, lr=[1.157142857142857e-06, 1.157142857142857e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 19:40:03,126] [INFO] [timer.py:154:stop] 0/100, SamplesPerSec=2.253530382018587
10.0.2.15:  iteration      100/    1000 | elapsed time per iteration (ms): 39387.2 | learning rate 1.171E-06 | lm loss 8.096313E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2740.49 | backward: 23159.31 | optimizer: 13297.90 | batch generator: 1.97 | data loader: 0.87
10.0.2.15:  iteration      101/    1000 | elapsed time per iteration (ms): 39052.6 | learning rate 1.186E-06 | lm loss 8.021739E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2951.36 | backward: 22607.30 | optimizer: 13376.66 | batch generator: 2.19 | data loader: 0.92
10.0.2.15:  iteration      102/    1000 | elapsed time per iteration (ms): 39099.0 | learning rate 1.200E-06 | lm loss 7.756098E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3141.76 | backward: 22524.44 | optimizer: 13215.29 | batch generator: 1.82 | data loader: 0.71
10.0.2.15:  iteration      103/    1000 | elapsed time per iteration (ms): 39293.8 | learning rate 1.214E-06 | lm loss 7.716681E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3077.64 | backward: 22857.23 | optimizer: 13272.26 | batch generator: 1.88 | data loader: 0.73
10.0.2.15:  iteration      104/    1000 | elapsed time per iteration (ms): 38176.8 | learning rate 1.229E-06 | lm loss 7.450987E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3104.93 | backward: 21912.54 | optimizer: 13140.71 | batch generator: 2.16 | data loader: 0.97
10.0.2.15:  iteration      105/    1000 | elapsed time per iteration (ms): 39768.0 | learning rate 1.243E-06 | lm loss 7.400351E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3366.45 | backward: 23154.49 | optimizer: 13168.10 | batch generator: 1.88 | data loader: 0.74
10.0.2.15:  iteration      106/    1000 | elapsed time per iteration (ms): 39157.6 | learning rate 1.257E-06 | lm loss 8.088944E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3291.96 | backward: 22679.72 | optimizer: 13184.09 | batch generator: 1.56 | data loader: 0.50
10.0.2.15:  iteration      107/    1000 | elapsed time per iteration (ms): 39078.8 | learning rate 1.271E-06 | lm loss 7.777413E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3042.07 | backward: 22923.60 | optimizer: 12874.99 | batch generator: 1.85 | data loader: 0.69
10.0.2.15:  iteration      108/    1000 | elapsed time per iteration (ms): 39969.1 | learning rate 1.286E-06 | lm loss 7.255541E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3354.91 | backward: 22775.92 | optimizer: 13526.97 | batch generator: 1.51 | data loader: 0.35
10.0.2.15:  iteration      109/    1000 | elapsed time per iteration (ms): 39143.6 | learning rate 1.300E-06 | lm loss 7.199183E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3008.68 | backward: 22987.29 | optimizer: 13074.73 | batch generator: 1.91 | data loader: 0.74
10.0.2.15:  iteration      110/    1000 | elapsed time per iteration (ms): 39452.4 | learning rate 1.314E-06 | lm loss 7.338552E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3692.15 | backward: 22809.36 | optimizer: 12949.50 | batch generator: 1.84 | data loader: 0.76
10.0.2.15:  iteration      111/    1000 | elapsed time per iteration (ms): 39308.1 | learning rate 1.329E-06 | lm loss 7.644656E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3361.22 | backward: 22984.53 | optimizer: 12960.84 | batch generator: 2.55 | data loader: 1.20
10.0.2.15:  iteration      112/    1000 | elapsed time per iteration (ms): 39941.4 | learning rate 1.343E-06 | lm loss 7.066953E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3561.19 | backward: 23256.01 | optimizer: 13122.91 | batch generator: 1.88 | data loader: 0.75
10.0.2.15:  iteration      113/    1000 | elapsed time per iteration (ms): 38840.6 | learning rate 1.357E-06 | lm loss 7.739310E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3185.70 | backward: 22479.78 | optimizer: 13022.29 | batch generator: 2.39 | data loader: 0.99
10.0.2.15:  iteration      114/    1000 | elapsed time per iteration (ms): 38790.0 | learning rate 1.371E-06 | lm loss 7.886819E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3115.58 | backward: 22244.46 | optimizer: 12987.99 | batch generator: 2.28 | data loader: 1.04
10.0.2.15:  iteration      115/    1000 | elapsed time per iteration (ms): 40117.2 | learning rate 1.386E-06 | lm loss 7.222031E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3492.99 | backward: 23614.17 | optimizer: 13008.36 | batch generator: 2.52 | data loader: 1.02
10.0.2.15:  iteration      116/    1000 | elapsed time per iteration (ms): 38668.9 | learning rate 1.400E-06 | lm loss 8.500029E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3035.18 | backward: 22752.84 | optimizer: 12586.02 | batch generator: 2.36 | data loader: 1.15
10.0.2.15:  iteration      117/    1000 | elapsed time per iteration (ms): 38949.1 | learning rate 1.414E-06 | lm loss 7.552702E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3565.84 | backward: 22719.21 | optimizer: 12314.95 | batch generator: 2.55 | data loader: 1.09
10.0.2.15:  iteration      118/    1000 | elapsed time per iteration (ms): 38333.1 | learning rate 1.429E-06 | lm loss 7.222624E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3892.77 | backward: 22573.84 | optimizer: 11455.11 | batch generator: 2.12 | data loader: 0.91
10.0.2.15:  iteration      119/    1000 | elapsed time per iteration (ms): 38704.7 | learning rate 1.443E-06 | lm loss 8.086788E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4880.49 | backward: 22570.61 | optimizer: 11202.97 | batch generator: 2.23 | data loader: 1.01
10.0.2.15:  iteration      120/    1000 | elapsed time per iteration (ms): 39265.9 | learning rate 1.457E-06 | lm loss 7.556685E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4928.80 | backward: 22406.25 | optimizer: 11506.84 | batch generator: 1.88 | data loader: 0.73
10.0.2.15:  iteration      121/    1000 | elapsed time per iteration (ms): 39542.8 | learning rate 1.471E-06 | lm loss 6.808226E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4879.70 | backward: 23280.63 | optimizer: 11270.69 | batch generator: 2.24 | data loader: 0.91
10.0.2.15:  iteration      122/    1000 | elapsed time per iteration (ms): 40064.4 | learning rate 1.486E-06 | lm loss 6.922222E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5006.33 | backward: 22763.93 | optimizer: 12200.89 | batch generator: 1.88 | data loader: 0.83
10.0.2.15:  iteration      123/    1000 | elapsed time per iteration (ms): 38776.7 | learning rate 1.500E-06 | lm loss 7.347537E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4070.01 | backward: 23081.98 | optimizer: 11353.94 | batch generator: 2.51 | data loader: 1.05
10.0.2.15:  iteration      124/    1000 | elapsed time per iteration (ms): 39316.1 | learning rate 1.514E-06 | lm loss 8.269702E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4701.97 | backward: 22759.73 | optimizer: 11700.04 | batch generator: 1.89 | data loader: 0.74
10.0.2.15:  iteration      125/    1000 | elapsed time per iteration (ms): 39689.9 | learning rate 1.529E-06 | lm loss 7.430875E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4919.92 | backward: 22907.27 | optimizer: 11723.24 | batch generator: 1.84 | data loader: 0.60
10.0.2.15:  iteration      126/    1000 | elapsed time per iteration (ms): 40340.0 | learning rate 1.543E-06 | lm loss 7.170249E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4576.29 | backward: 22488.96 | optimizer: 12976.20 | batch generator: 1.48 | data loader: 0.30
10.0.2.15:  iteration      127/    1000 | elapsed time per iteration (ms): 39135.5 | learning rate 1.557E-06 | lm loss 7.579503E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3027.06 | backward: 22729.24 | optimizer: 12783.84 | batch generator: 2.23 | data loader: 1.09
10.0.2.15:  iteration      128/    1000 | elapsed time per iteration (ms): 39366.6 | learning rate 1.571E-06 | lm loss 6.660192E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3441.31 | backward: 22855.14 | optimizer: 13029.03 | batch generator: 1.73 | data loader: 0.40
10.0.2.15:  iteration      129/    1000 | elapsed time per iteration (ms): 38294.8 | learning rate 1.586E-06 | lm loss 7.580313E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2007.57 | backward: 22451.67 | optimizer: 12567.44 | batch generator: 1.35 | data loader: 0.25
10.0.2.15:  iteration      130/    1000 | elapsed time per iteration (ms): 39884.1 | learning rate 1.600E-06 | lm loss 7.813098E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3807.08 | backward: 23357.37 | optimizer: 12717.47 | batch generator: 1.82 | data loader: 0.64
10.0.2.15:  iteration      131/    1000 | elapsed time per iteration (ms): 38486.9 | learning rate 1.614E-06 | lm loss 7.548136E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3289.21 | backward: 21999.58 | optimizer: 12606.81 | batch generator: 2.28 | data loader: 0.99
10.0.2.15:  iteration      132/    1000 | elapsed time per iteration (ms): 39459.6 | learning rate 1.629E-06 | lm loss 7.122108E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3633.64 | backward: 22743.49 | optimizer: 13016.66 | batch generator: 2.13 | data loader: 0.94
10.0.2.15:  iteration      133/    1000 | elapsed time per iteration (ms): 39272.2 | learning rate 1.643E-06 | lm loss 8.236539E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3508.58 | backward: 23058.80 | optimizer: 12703.41 | batch generator: 1.85 | data loader: 0.72
10.0.2.15:  iteration      134/    1000 | elapsed time per iteration (ms): 39754.0 | learning rate 1.657E-06 | lm loss 7.267499E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3788.84 | backward: 22827.66 | optimizer: 13136.32 | batch generator: 1.91 | data loader: 0.71
10.0.2.15:  iteration      135/    1000 | elapsed time per iteration (ms): 38858.4 | learning rate 1.671E-06 | lm loss 7.504300E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3069.05 | backward: 22750.75 | optimizer: 12837.22 | batch generator: 2.27 | data loader: 0.96
10.0.2.15:  iteration      136/    1000 | elapsed time per iteration (ms): 39478.4 | learning rate 1.686E-06 | lm loss 7.359369E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3822.64 | backward: 23091.76 | optimizer: 12562.71 | batch generator: 1.97 | data loader: 0.77
10.0.2.15:  iteration      137/    1000 | elapsed time per iteration (ms): 39739.5 | learning rate 1.700E-06 | lm loss 7.078090E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3865.49 | backward: 22998.90 | optimizer: 12873.15 | batch generator: 2.34 | data loader: 0.95
10.0.2.15:  iteration      138/    1000 | elapsed time per iteration (ms): 38252.4 | learning rate 1.714E-06 | lm loss 7.369334E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3394.12 | backward: 22748.17 | optimizer: 11447.06 | batch generator: 2.05 | data loader: 0.82
10.0.2.15:  iteration      139/    1000 | elapsed time per iteration (ms): 39554.1 | learning rate 1.729E-06 | lm loss 6.998054E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5155.55 | backward: 22889.23 | optimizer: 11507.90 | batch generator: 2.23 | data loader: 0.95
10.0.2.15:  iteration      140/    1000 | elapsed time per iteration (ms): 39465.8 | learning rate 1.743E-06 | lm loss 7.241307E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4761.03 | backward: 22741.41 | optimizer: 11652.52 | batch generator: 2.01 | data loader: 0.92
10.0.2.15:  iteration      141/    1000 | elapsed time per iteration (ms): 39314.5 | learning rate 1.757E-06 | lm loss 7.769958E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4441.11 | backward: 23140.56 | optimizer: 11426.74 | batch generator: 2.45 | data loader: 1.12
10.0.2.15:  iteration      142/    1000 | elapsed time per iteration (ms): 38822.9 | learning rate 1.771E-06 | lm loss 7.526791E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4948.62 | backward: 22630.46 | optimizer: 11241.63 | batch generator: 2.75 | data loader: 1.18
10.0.2.15:  iteration      143/    1000 | elapsed time per iteration (ms): 39397.0 | learning rate 1.786E-06 | lm loss 7.393136E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4849.41 | backward: 22550.44 | optimizer: 11734.75 | batch generator: 2.31 | data loader: 1.03
10.0.2.15:  iteration      144/    1000 | elapsed time per iteration (ms): 39580.8 | learning rate 1.800E-06 | lm loss 7.488141E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4672.65 | backward: 23350.54 | optimizer: 11328.27 | batch generator: 1.99 | data loader: 0.77
10.0.2.15:  iteration      145/    1000 | elapsed time per iteration (ms): 39231.4 | learning rate 1.814E-06 | lm loss 7.056025E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5059.45 | backward: 22881.76 | optimizer: 11285.42 | batch generator: 2.35 | data loader: 1.02
10.0.2.15:  iteration      146/    1000 | elapsed time per iteration (ms): 40435.9 | learning rate 1.829E-06 | lm loss 7.898133E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5119.77 | backward: 23104.90 | optimizer: 11773.29 | batch generator: 2.08 | data loader: 0.85
10.0.2.15:  iteration      147/    1000 | elapsed time per iteration (ms): 39729.4 | learning rate 1.843E-06 | lm loss 7.319417E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5093.16 | backward: 22673.97 | optimizer: 11960.59 | batch generator: 2.32 | data loader: 1.04
10.0.2.15:  iteration      148/    1000 | elapsed time per iteration (ms): 39431.1 | learning rate 1.857E-06 | lm loss 6.599011E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4833.75 | backward: 22928.96 | optimizer: 11667.10 | batch generator: 1.95 | data loader: 0.74
10.0.2.15:  iteration      149/    1000 | elapsed time per iteration (ms): 39082.2 | learning rate 1.871E-06 | lm loss 7.449797E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4682.22 | backward: 22808.22 | optimizer: 11545.54 | batch generator: 2.10 | data loader: 0.93
10.0.2.15: [2021-12-31 20:12:36,747] [INFO] [logging.py:60:log_dist] [Rank 0] step=150, skipped=18, lr=[1.8714285714285715e-06, 1.8714285714285715e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 20:12:48,096] [INFO] [timer.py:154:stop] 0/150, SamplesPerSec=2.212996508154802
10.0.2.15:  iteration      150/    1000 | elapsed time per iteration (ms): 40101.7 | learning rate 1.886E-06 | lm loss 6.820699E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4891.86 | backward: 23510.63 | optimizer: 11420.64 | batch generator: 1.91 | data loader: 0.75
10.0.2.15:  iteration      151/    1000 | elapsed time per iteration (ms): 39202.2 | learning rate 1.900E-06 | lm loss 7.522670E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4854.86 | backward: 22866.83 | optimizer: 11306.47 | batch generator: 2.18 | data loader: 0.94
10.0.2.15:  iteration      152/    1000 | elapsed time per iteration (ms): 39401.0 | learning rate 1.914E-06 | lm loss 7.287697E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4725.83 | backward: 22815.99 | optimizer: 11300.71 | batch generator: 2.10 | data loader: 0.92
10.0.2.15:  iteration      153/    1000 | elapsed time per iteration (ms): 40509.6 | learning rate 1.929E-06 | lm loss 7.635767E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5185.76 | backward: 23226.95 | optimizer: 12008.59 | batch generator: 1.87 | data loader: 0.80
10.0.2.15:  iteration      154/    1000 | elapsed time per iteration (ms): 39704.1 | learning rate 1.943E-06 | lm loss 7.784545E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4465.38 | backward: 23047.42 | optimizer: 12016.13 | batch generator: 2.05 | data loader: 0.90
10.0.2.15:  iteration      155/    1000 | elapsed time per iteration (ms): 39633.0 | learning rate 1.957E-06 | lm loss 6.907174E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4290.89 | backward: 22642.23 | optimizer: 12373.33 | batch generator: 2.21 | data loader: 0.91
10.0.2.15:  iteration      156/    1000 | elapsed time per iteration (ms): 40194.0 | learning rate 1.971E-06 | lm loss 6.912255E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3940.63 | backward: 23348.07 | optimizer: 12699.06 | batch generator: 1.83 | data loader: 0.71
10.0.2.15:  iteration      157/    1000 | elapsed time per iteration (ms): 39562.0 | learning rate 1.986E-06 | lm loss 7.052755E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3699.91 | backward: 22877.81 | optimizer: 12782.32 | batch generator: 2.00 | data loader: 0.79
10.0.2.15:  iteration      158/    1000 | elapsed time per iteration (ms): 40068.9 | learning rate 2.000E-06 | lm loss 7.700947E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3797.68 | backward: 23139.99 | optimizer: 13013.48 | batch generator: 1.65 | data loader: 0.35
10.0.2.15:  iteration      159/    1000 | elapsed time per iteration (ms): 38637.6 | learning rate 2.014E-06 | lm loss 7.106518E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3376.61 | backward: 22439.86 | optimizer: 12819.70 | batch generator: 2.06 | data loader: 0.93
10.0.2.15:  iteration      160/    1000 | elapsed time per iteration (ms): 39736.8 | learning rate 2.029E-06 | lm loss 7.197867E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3675.03 | backward: 22983.21 | optimizer: 12947.19 | batch generator: 1.94 | data loader: 0.75
10.0.2.15:  iteration      161/    1000 | elapsed time per iteration (ms): 39320.2 | learning rate 2.043E-06 | lm loss 7.176245E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2064.33 | backward: 22542.16 | optimizer: 13032.99 | batch generator: 2.18 | data loader: 0.92
10.0.2.15:  iteration      162/    1000 | elapsed time per iteration (ms): 39537.2 | learning rate 2.057E-06 | lm loss 7.226854E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3491.02 | backward: 22934.36 | optimizer: 12918.88 | batch generator: 1.85 | data loader: 0.73
10.0.2.15:  iteration      163/    1000 | elapsed time per iteration (ms): 39430.6 | learning rate 2.071E-06 | lm loss 7.432646E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3386.81 | backward: 22796.82 | optimizer: 13131.16 | batch generator: 1.97 | data loader: 0.75
10.0.2.15:  iteration      164/    1000 | elapsed time per iteration (ms): 39325.4 | learning rate 2.086E-06 | lm loss 7.545172E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3527.73 | backward: 23234.89 | optimizer: 12561.31 | batch generator: 2.20 | data loader: 0.92
10.0.2.15:  iteration      165/    1000 | elapsed time per iteration (ms): 39412.6 | learning rate 2.100E-06 | lm loss 7.049994E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3542.07 | backward: 22492.58 | optimizer: 12859.62 | batch generator: 1.87 | data loader: 0.75
10.0.2.15:  iteration      166/    1000 | elapsed time per iteration (ms): 39715.8 | learning rate 2.114E-06 | lm loss 6.916137E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2966.04 | backward: 22929.46 | optimizer: 13130.94 | batch generator: 2.04 | data loader: 0.93
10.0.2.15:  iteration      167/    1000 | elapsed time per iteration (ms): 38733.0 | learning rate 2.129E-06 | lm loss 7.090048E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3142.35 | backward: 22752.00 | optimizer: 12698.89 | batch generator: 2.50 | data loader: 1.09
10.0.2.15:  iteration      168/    1000 | elapsed time per iteration (ms): 40071.9 | learning rate 2.143E-06 | lm loss 7.050281E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3873.90 | backward: 23365.43 | optimizer: 12820.63 | batch generator: 2.05 | data loader: 0.94
10.0.2.15:  iteration      169/    1000 | elapsed time per iteration (ms): 39345.0 | learning rate 2.157E-06 | lm loss 7.338111E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2985.12 | backward: 22506.91 | optimizer: 13419.75 | batch generator: 2.80 | data loader: 1.43
10.0.2.15:  iteration      170/    1000 | elapsed time per iteration (ms): 39718.1 | learning rate 2.171E-06 | lm loss 7.687905E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2643.20 | backward: 23383.92 | optimizer: 13160.11 | batch generator: 2.50 | data loader: 1.04
10.0.2.15:  iteration      171/    1000 | elapsed time per iteration (ms): 40593.9 | learning rate 2.186E-06 | lm loss 7.137344E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3684.58 | backward: 23950.90 | optimizer: 12957.23 | batch generator: 2.01 | data loader: 0.83
10.0.2.15:  iteration      172/    1000 | elapsed time per iteration (ms): 39604.5 | learning rate 2.200E-06 | lm loss 7.239039E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3272.09 | backward: 23106.05 | optimizer: 12867.31 | batch generator: 1.58 | data loader: 0.35
10.0.2.15:  iteration      173/    1000 | elapsed time per iteration (ms): 39927.7 | learning rate 2.214E-06 | lm loss 6.647537E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3871.05 | backward: 22748.23 | optimizer: 13174.36 | batch generator: 2.00 | data loader: 0.92
10.0.2.15:  iteration      174/    1000 | elapsed time per iteration (ms): 39523.6 | learning rate 2.229E-06 | lm loss 7.898684E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3158.76 | backward: 22893.04 | optimizer: 13370.61 | batch generator: 2.39 | data loader: 1.10
10.0.2.15:  iteration      175/    1000 | elapsed time per iteration (ms): 39518.4 | learning rate 2.243E-06 | lm loss 7.598117E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3299.37 | backward: 23411.86 | optimizer: 12805.92 | batch generator: 1.90 | data loader: 0.72
10.0.2.15:  iteration      176/    1000 | elapsed time per iteration (ms): 38971.8 | learning rate 2.257E-06 | lm loss 7.040105E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3430.09 | backward: 22387.25 | optimizer: 13099.17 | batch generator: 1.81 | data loader: 0.72
10.0.2.15:  iteration      177/    1000 | elapsed time per iteration (ms): 38932.4 | learning rate 2.271E-06 | lm loss 7.192249E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3336.16 | backward: 22404.29 | optimizer: 13068.99 | batch generator: 2.00 | data loader: 0.86
10.0.2.15:  iteration      178/    1000 | elapsed time per iteration (ms): 39403.9 | learning rate 2.286E-06 | lm loss 7.308411E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3112.61 | backward: 22588.14 | optimizer: 13340.04 | batch generator: 1.64 | data loader: 0.36
10.0.2.15:  iteration      179/    1000 | elapsed time per iteration (ms): 39816.9 | learning rate 2.300E-06 | lm loss 6.824021E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3019.75 | backward: 23113.33 | optimizer: 13276.96 | batch generator: 1.79 | data loader: 0.65
10.0.2.15:  iteration      180/    1000 | elapsed time per iteration (ms): 39228.5 | learning rate 2.314E-06 | lm loss 7.667928E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3044.24 | backward: 22832.81 | optimizer: 13327.17 | batch generator: 2.52 | data loader: 1.16
10.0.2.15:  iteration      181/    1000 | elapsed time per iteration (ms): 38720.4 | learning rate 2.329E-06 | lm loss 7.231331E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3210.60 | backward: 22640.85 | optimizer: 12867.13 | batch generator: 2.39 | data loader: 0.95
10.0.2.15:  iteration      182/    1000 | elapsed time per iteration (ms): 39197.5 | learning rate 2.343E-06 | lm loss 7.553208E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3338.10 | backward: 22875.40 | optimizer: 12795.08 | batch generator: 2.11 | data loader: 0.87
10.0.2.15:  iteration      183/    1000 | elapsed time per iteration (ms): 39848.6 | learning rate 2.357E-06 | lm loss 7.679120E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3375.45 | backward: 23236.58 | optimizer: 12882.65 | batch generator: 1.77 | data loader: 0.40
10.0.2.15:  iteration      184/    1000 | elapsed time per iteration (ms): 39761.9 | learning rate 2.371E-06 | lm loss 7.456634E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3241.96 | backward: 23108.76 | optimizer: 13212.73 | batch generator: 1.72 | data loader: 0.38
10.0.2.15:  iteration      185/    1000 | elapsed time per iteration (ms): 39439.1 | learning rate 2.386E-06 | lm loss 7.327968E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2710.92 | backward: 23150.86 | optimizer: 13361.97 | batch generator: 1.73 | data loader: 0.37
10.0.2.15:  iteration      186/    1000 | elapsed time per iteration (ms): 39054.8 | learning rate 2.400E-06 | lm loss 8.051175E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2892.53 | backward: 23039.41 | optimizer: 13121.26 | batch generator: 2.25 | data loader: 0.97
10.0.2.15:  iteration      187/    1000 | elapsed time per iteration (ms): 39599.3 | learning rate 2.414E-06 | lm loss 7.186295E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3273.62 | backward: 22967.59 | optimizer: 12910.20 | batch generator: 2.00 | data loader: 0.58
10.0.2.15:  iteration      188/    1000 | elapsed time per iteration (ms): 39266.4 | learning rate 2.429E-06 | lm loss 7.080354E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3306.24 | backward: 22840.86 | optimizer: 13099.86 | batch generator: 1.95 | data loader: 0.71
10.0.2.15:  iteration      189/    1000 | elapsed time per iteration (ms): 39248.9 | learning rate 2.443E-06 | lm loss 7.230827E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2934.55 | backward: 22888.90 | optimizer: 12882.74 | batch generator: 2.22 | data loader: 0.95
10.0.2.15:  iteration      190/    1000 | elapsed time per iteration (ms): 39282.4 | learning rate 2.457E-06 | lm loss 7.467260E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3300.23 | backward: 23090.69 | optimizer: 12865.47 | batch generator: 1.87 | data loader: 0.73
10.0.2.15:  iteration      191/    1000 | elapsed time per iteration (ms): 39511.2 | learning rate 2.471E-06 | lm loss 6.614046E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3395.48 | backward: 22833.27 | optimizer: 13170.41 | batch generator: 2.29 | data loader: 0.93
10.0.2.15:  iteration      192/    1000 | elapsed time per iteration (ms): 40382.3 | learning rate 2.486E-06 | lm loss 6.490077E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3621.77 | backward: 23163.01 | optimizer: 13596.09 | batch generator: 2.00 | data loader: 0.75
10.0.2.15:  iteration      193/    1000 | elapsed time per iteration (ms): 39534.5 | learning rate 2.500E-06 | lm loss 7.188428E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2622.31 | backward: 23276.99 | optimizer: 13427.57 | batch generator: 2.07 | data loader: 0.89
10.0.2.15:  iteration      194/    1000 | elapsed time per iteration (ms): 39733.5 | learning rate 2.514E-06 | lm loss 7.196381E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2892.37 | backward: 23545.32 | optimizer: 13043.47 | batch generator: 2.52 | data loader: 1.24
10.0.2.15:  iteration      195/    1000 | elapsed time per iteration (ms): 39069.4 | learning rate 2.529E-06 | lm loss 6.830883E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3216.74 | backward: 22716.39 | optimizer: 13028.09 | batch generator: 2.30 | data loader: 0.98
10.0.2.15:  iteration      196/    1000 | elapsed time per iteration (ms): 38869.2 | learning rate 2.543E-06 | lm loss 7.780596E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2979.62 | backward: 22397.25 | optimizer: 13010.11 | batch generator: 1.75 | data loader: 0.39
10.0.2.15:  iteration      197/    1000 | elapsed time per iteration (ms): 39172.9 | learning rate 2.557E-06 | lm loss 6.907947E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3228.68 | backward: 22454.80 | optimizer: 13020.41 | batch generator: 1.73 | data loader: 0.38
10.0.2.15:  iteration      198/    1000 | elapsed time per iteration (ms): 38855.6 | learning rate 2.571E-06 | lm loss 6.692096E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3188.51 | backward: 22528.09 | optimizer: 12848.88 | batch generator: 2.24 | data loader: 0.90
10.0.2.15:  iteration      199/    1000 | elapsed time per iteration (ms): 39809.8 | learning rate 2.586E-06 | lm loss 7.128369E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3194.88 | backward: 23058.93 | optimizer: 12999.57 | batch generator: 2.49 | data loader: 1.16
10.0.2.15: [2021-12-31 20:45:29,286] [INFO] [logging.py:60:log_dist] [Rank 0] step=200, skipped=18, lr=[2.585714285714286e-06, 2.585714285714286e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 20:45:42,188] [INFO] [timer.py:154:stop] 0/200, SamplesPerSec=2.190832557619522
10.0.2.15:  iteration      200/    1000 | elapsed time per iteration (ms): 38952.9 | learning rate 2.600E-06 | lm loss 7.269228E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3078.55 | backward: 22323.10 | optimizer: 13076.70 | batch generator: 1.94 | data loader: 0.72
10.0.2.15:  iteration      201/    1000 | elapsed time per iteration (ms): 39576.7 | learning rate 2.614E-06 | lm loss 6.930975E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3707.35 | backward: 22858.31 | optimizer: 13009.43 | batch generator: 2.16 | data loader: 0.86
10.0.2.15:  iteration      202/    1000 | elapsed time per iteration (ms): 39823.1 | learning rate 2.629E-06 | lm loss 7.070251E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3281.63 | backward: 23351.10 | optimizer: 13111.83 | batch generator: 1.90 | data loader: 0.75
10.0.2.15:  iteration      203/    1000 | elapsed time per iteration (ms): 39288.3 | learning rate 2.643E-06 | lm loss 8.146214E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3245.43 | backward: 22862.02 | optimizer: 13179.18 | batch generator: 2.46 | data loader: 1.16
10.0.2.15:  iteration      204/    1000 | elapsed time per iteration (ms): 38330.9 | learning rate 2.657E-06 | lm loss 6.750302E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3003.77 | backward: 22532.79 | optimizer: 12757.32 | batch generator: 1.98 | data loader: 0.77
10.0.2.15:  iteration      205/    1000 | elapsed time per iteration (ms): 39559.9 | learning rate 2.671E-06 | lm loss 7.171329E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3819.29 | backward: 23010.21 | optimizer: 12727.90 | batch generator: 1.76 | data loader: 0.39
10.0.2.15:  iteration      206/    1000 | elapsed time per iteration (ms): 39456.4 | learning rate 2.686E-06 | lm loss 6.563300E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3538.85 | backward: 22660.32 | optimizer: 12796.18 | batch generator: 2.26 | data loader: 1.07
10.0.2.15:  iteration      207/    1000 | elapsed time per iteration (ms): 40012.8 | learning rate 2.700E-06 | lm loss 6.996168E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3553.34 | backward: 23267.06 | optimizer: 12839.68 | batch generator: 2.43 | data loader: 1.09
10.0.2.15:  iteration      208/    1000 | elapsed time per iteration (ms): 39484.3 | learning rate 2.714E-06 | lm loss 6.544353E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3340.58 | backward: 23017.60 | optimizer: 12810.58 | batch generator: 2.36 | data loader: 0.98
10.0.2.15:  iteration      209/    1000 | elapsed time per iteration (ms): 39043.4 | learning rate 2.729E-06 | lm loss 7.636754E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3606.27 | backward: 22775.26 | optimizer: 12659.89 | batch generator: 1.96 | data loader: 0.78
10.0.2.15:  iteration      210/    1000 | elapsed time per iteration (ms): 39287.7 | learning rate 2.743E-06 | lm loss 7.389601E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3309.41 | backward: 23014.42 | optimizer: 12762.36 | batch generator: 2.32 | data loader: 0.99
10.0.2.15:  iteration      211/    1000 | elapsed time per iteration (ms): 38976.1 | learning rate 2.757E-06 | lm loss 7.709682E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3331.61 | backward: 22449.69 | optimizer: 12954.18 | batch generator: 2.10 | data loader: 0.92
10.0.2.15:  iteration      212/    1000 | elapsed time per iteration (ms): 40251.1 | learning rate 2.771E-06 | lm loss 7.105483E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3530.21 | backward: 23538.28 | optimizer: 13032.95 | batch generator: 1.97 | data loader: 0.73
10.0.2.15:  iteration      213/    1000 | elapsed time per iteration (ms): 39449.4 | learning rate 2.786E-06 | lm loss 7.264626E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3399.02 | backward: 22987.26 | optimizer: 12696.45 | batch generator: 1.65 | data loader: 0.37
10.0.2.15:  iteration      214/    1000 | elapsed time per iteration (ms): 39083.5 | learning rate 2.800E-06 | lm loss 6.488129E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3511.06 | backward: 22043.72 | optimizer: 13297.64 | batch generator: 2.01 | data loader: 0.81
10.0.2.15:  iteration      215/    1000 | elapsed time per iteration (ms): 39498.4 | learning rate 2.814E-06 | lm loss 6.670483E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2859.93 | backward: 22811.53 | optimizer: 13673.63 | batch generator: 2.45 | data loader: 1.06
10.0.2.15:  iteration      216/    1000 | elapsed time per iteration (ms): 40321.6 | learning rate 2.829E-06 | lm loss 7.257792E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3321.37 | backward: 23350.76 | optimizer: 13648.02 | batch generator: 2.00 | data loader: 0.88
10.0.2.15:  iteration      217/    1000 | elapsed time per iteration (ms): 39045.3 | learning rate 2.843E-06 | lm loss 7.085079E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2695.65 | backward: 22467.92 | optimizer: 13695.44 | batch generator: 1.90 | data loader: 0.71
10.0.2.15:  iteration      218/    1000 | elapsed time per iteration (ms): 39389.3 | learning rate 2.857E-06 | lm loss 6.931507E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2716.84 | backward: 23025.02 | optimizer: 13553.07 | batch generator: 2.09 | data loader: 0.90
10.0.2.15:  iteration      219/    1000 | elapsed time per iteration (ms): 39209.6 | learning rate 2.871E-06 | lm loss 6.725578E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2646.70 | backward: 22586.96 | optimizer: 13556.10 | batch generator: 1.78 | data loader: 0.72
10.0.2.15:  iteration      220/    1000 | elapsed time per iteration (ms): 39806.5 | learning rate 2.886E-06 | lm loss 7.200150E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2769.67 | backward: 23240.99 | optimizer: 13502.31 | batch generator: 2.34 | data loader: 0.97
10.0.2.15:  iteration      221/    1000 | elapsed time per iteration (ms): 47716.1 | learning rate 2.900E-06 | lm loss 7.064340E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2938.48 | backward: 31790.36 | optimizer: 12985.78 | batch generator: 1.32 | data loader: 0.27
10.0.2.15:  iteration      222/    1000 | elapsed time per iteration (ms): 36674.9 | learning rate 2.914E-06 | lm loss 7.192604E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2974.22 | backward: 20968.53 | optimizer: 12712.90 | batch generator: 1.93 | data loader: 0.87
10.0.2.15:  iteration      223/    1000 | elapsed time per iteration (ms): 34964.3 | learning rate 2.929E-06 | lm loss 7.585360E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3262.87 | backward: 19760.62 | optimizer: 11897.14 | batch generator: 1.34 | data loader: 0.23
10.0.2.15:  iteration      224/    1000 | elapsed time per iteration (ms): 33630.0 | learning rate 2.943E-06 | lm loss 6.900052E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3903.21 | backward: 19414.50 | optimizer: 9800.28 | batch generator: 1.59 | data loader: 0.32
10.0.2.15:  iteration      225/    1000 | elapsed time per iteration (ms): 37292.5 | learning rate 2.957E-06 | lm loss 6.990830E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6217.82 | backward: 19573.42 | optimizer: 11476.45 | batch generator: 1.84 | data loader: 0.67
10.0.2.15:  iteration      226/    1000 | elapsed time per iteration (ms): 35929.1 | learning rate 2.971E-06 | lm loss 6.899258E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4631.45 | backward: 19839.67 | optimizer: 11456.84 | batch generator: 1.77 | data loader: 0.68
10.0.2.15:  iteration      227/    1000 | elapsed time per iteration (ms): 35494.2 | learning rate 2.986E-06 | lm loss 7.120358E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4555.69 | backward: 19604.40 | optimizer: 11332.77 | batch generator: 1.82 | data loader: 0.81
10.0.2.15:  iteration      228/    1000 | elapsed time per iteration (ms): 35484.7 | learning rate 3.000E-06 | lm loss 7.297858E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4698.68 | backward: 19332.00 | optimizer: 11452.72 | batch generator: 1.95 | data loader: 0.70
10.0.2.15:  iteration      229/    1000 | elapsed time per iteration (ms): 35265.0 | learning rate 3.014E-06 | lm loss 6.473675E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4845.42 | backward: 19560.11 | optimizer: 10858.09 | batch generator: 2.08 | data loader: 0.85
10.0.2.15:  iteration      230/    1000 | elapsed time per iteration (ms): 34877.9 | learning rate 3.029E-06 | lm loss 7.369572E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5134.85 | backward: 19601.12 | optimizer: 10137.06 | batch generator: 1.81 | data loader: 0.51
10.0.2.15:  iteration      231/    1000 | elapsed time per iteration (ms): 36366.8 | learning rate 3.043E-06 | lm loss 6.581487E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5913.58 | backward: 20075.88 | optimizer: 10093.86 | batch generator: 1.78 | data loader: 0.80
10.0.2.15:  iteration      232/    1000 | elapsed time per iteration (ms): 35949.9 | learning rate 3.057E-06 | lm loss 7.336921E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5920.48 | backward: 19683.89 | optimizer: 10343.92 | batch generator: 1.60 | data loader: 0.48
10.0.2.15:  iteration      233/    1000 | elapsed time per iteration (ms): 35765.8 | learning rate 3.071E-06 | lm loss 7.006739E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5661.42 | backward: 19978.64 | optimizer: 9883.58 | batch generator: 1.82 | data loader: 0.69
10.0.2.15:  iteration      234/    1000 | elapsed time per iteration (ms): 35723.8 | learning rate 3.086E-06 | lm loss 7.010897E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5865.84 | backward: 19676.67 | optimizer: 9911.76 | batch generator: 2.21 | data loader: 0.87
10.0.2.15:  iteration      235/    1000 | elapsed time per iteration (ms): 37143.5 | learning rate 3.100E-06 | lm loss 7.352038E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6057.21 | backward: 20306.86 | optimizer: 10579.74 | batch generator: 1.92 | data loader: 0.78
10.0.2.15:  iteration      236/    1000 | elapsed time per iteration (ms): 36790.0 | learning rate 3.114E-06 | lm loss 6.737250E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5384.86 | backward: 20708.26 | optimizer: 10469.33 | batch generator: 1.41 | data loader: 0.27
10.0.2.15:  iteration      237/    1000 | elapsed time per iteration (ms): 37204.4 | learning rate 3.129E-06 | lm loss 6.758906E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5589.02 | backward: 19815.89 | optimizer: 11729.32 | batch generator: 2.16 | data loader: 0.98
10.0.2.15:  iteration      238/    1000 | elapsed time per iteration (ms): 34913.7 | learning rate 3.143E-06 | lm loss 7.626138E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4234.00 | backward: 19379.78 | optimizer: 11256.79 | batch generator: 2.45 | data loader: 1.12
10.0.2.15:  iteration      239/    1000 | elapsed time per iteration (ms): 34541.4 | learning rate 3.157E-06 | lm loss 6.874976E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4740.60 | backward: 19927.37 | optimizer: 9850.93 | batch generator: 1.42 | data loader: 0.28
10.0.2.15:  iteration      240/    1000 | elapsed time per iteration (ms): 36119.9 | learning rate 3.171E-06 | lm loss 7.211220E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6133.11 | backward: 19745.01 | optimizer: 9972.28 | batch generator: 1.46 | data loader: 0.28
10.0.2.15:  iteration      241/    1000 | elapsed time per iteration (ms): 37481.5 | learning rate 3.186E-06 | lm loss 6.679927E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5736.00 | backward: 20749.66 | optimizer: 10577.02 | batch generator: 1.62 | data loader: 0.47
10.0.2.15:  iteration      242/    1000 | elapsed time per iteration (ms): 36830.4 | learning rate 3.200E-06 | lm loss 6.854326E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5175.06 | backward: 19535.93 | optimizer: 11617.51 | batch generator: 1.80 | data loader: 0.40
10.0.2.15:  iteration      243/    1000 | elapsed time per iteration (ms): 34189.9 | learning rate 3.214E-06 | lm loss 7.374965E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4324.68 | backward: 19525.45 | optimizer: 10266.07 | batch generator: 1.72 | data loader: 0.38
10.0.2.15:  iteration      244/    1000 | elapsed time per iteration (ms): 35286.3 | learning rate 3.229E-06 | lm loss 6.665008E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5689.73 | backward: 19421.66 | optimizer: 9999.53 | batch generator: 1.66 | data loader: 0.35
10.0.2.15:  iteration      245/    1000 | elapsed time per iteration (ms): 36625.0 | learning rate 3.243E-06 | lm loss 6.503138E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5983.48 | backward: 20179.26 | optimizer: 10384.79 | batch generator: 1.65 | data loader: 0.29
10.0.2.15:  iteration      246/    1000 | elapsed time per iteration (ms): 35807.5 | learning rate 3.257E-06 | lm loss 6.401042E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5633.07 | backward: 19652.30 | optimizer: 10179.77 | batch generator: 2.42 | data loader: 1.00
10.0.2.15:  iteration      247/    1000 | elapsed time per iteration (ms): 36918.6 | learning rate 3.271E-06 | lm loss 6.547390E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5793.66 | backward: 20348.28 | optimizer: 10413.78 | batch generator: 1.89 | data loader: 0.55
10.0.2.15:  iteration      248/    1000 | elapsed time per iteration (ms): 37221.3 | learning rate 3.286E-06 | lm loss 6.578221E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5771.36 | backward: 21540.25 | optimizer: 9819.22 | batch generator: 2.09 | data loader: 0.90
10.0.2.15:  iteration      249/    1000 | elapsed time per iteration (ms): 37335.0 | learning rate 3.300E-06 | lm loss 7.379612E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6193.76 | backward: 20472.67 | optimizer: 10667.00 | batch generator: 2.35 | data loader: 0.98
10.0.2.15: [2021-12-31 21:16:52,368] [INFO] [logging.py:60:log_dist] [Rank 0] step=250, skipped=18, lr=[3.3e-06, 3.3e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 21:17:02,851] [INFO] [timer.py:154:stop] 0/250, SamplesPerSec=2.199295430996393
10.0.2.15:  iteration      250/    1000 | elapsed time per iteration (ms): 36224.8 | learning rate 3.314E-06 | lm loss 7.212246E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5108.02 | backward: 19927.62 | optimizer: 10536.71 | batch generator: 2.35 | data loader: 0.94
10.0.2.15:  iteration      251/    1000 | elapsed time per iteration (ms): 36366.3 | learning rate 3.329E-06 | lm loss 6.390989E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5777.80 | backward: 19620.51 | optimizer: 10966.37 | batch generator: 2.19 | data loader: 0.90
10.0.2.15:  iteration      252/    1000 | elapsed time per iteration (ms): 35729.5 | learning rate 3.343E-06 | lm loss 7.364433E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5196.67 | backward: 19797.10 | optimizer: 10734.42 | batch generator: 1.99 | data loader: 0.75
10.0.2.15:  iteration      253/    1000 | elapsed time per iteration (ms): 36357.7 | learning rate 3.357E-06 | lm loss 6.223126E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5338.49 | backward: 19549.16 | optimizer: 11146.31 | batch generator: 2.27 | data loader: 0.96
10.0.2.15:  iteration      254/    1000 | elapsed time per iteration (ms): 35248.8 | learning rate 3.371E-06 | lm loss 7.012071E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4825.12 | backward: 19876.99 | optimizer: 10263.74 | batch generator: 1.90 | data loader: 0.73
10.0.2.15:  iteration      255/    1000 | elapsed time per iteration (ms): 36372.3 | learning rate 3.386E-06 | lm loss 6.345977E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5977.75 | backward: 19988.28 | optimizer: 10404.89 | batch generator: 2.01 | data loader: 0.79
10.0.2.15:  iteration      256/    1000 | elapsed time per iteration (ms): 35988.4 | learning rate 3.400E-06 | lm loss 6.715583E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6051.91 | backward: 20139.30 | optimizer: 9791.62 | batch generator: 2.00 | data loader: 0.79
10.0.2.15:  iteration      257/    1000 | elapsed time per iteration (ms): 37306.8 | learning rate 3.414E-06 | lm loss 6.604379E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6160.42 | backward: 20907.39 | optimizer: 10225.13 | batch generator: 2.18 | data loader: 1.04
10.0.2.15:  iteration      258/    1000 | elapsed time per iteration (ms): 34960.7 | learning rate 3.429E-06 | lm loss 7.507291E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5972.62 | backward: 19153.11 | optimizer: 9833.52 | batch generator: 2.29 | data loader: 1.08
10.0.2.15:  iteration      259/    1000 | elapsed time per iteration (ms): 37037.5 | learning rate 3.443E-06 | lm loss 6.525136E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6358.42 | backward: 19835.50 | optimizer: 10542.08 | batch generator: 1.89 | data loader: 0.72
10.0.2.15:  iteration      260/    1000 | elapsed time per iteration (ms): 36538.8 | learning rate 3.457E-06 | lm loss 7.120152E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5237.41 | backward: 19644.23 | optimizer: 11147.31 | batch generator: 1.72 | data loader: 0.61
10.0.2.15:  iteration      261/    1000 | elapsed time per iteration (ms): 35776.0 | learning rate 3.471E-06 | lm loss 6.602639E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4802.12 | backward: 19556.64 | optimizer: 11118.02 | batch generator: 1.50 | data loader: 0.31
10.0.2.15:  iteration      262/    1000 | elapsed time per iteration (ms): 35512.4 | learning rate 3.486E-06 | lm loss 6.846748E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4671.29 | backward: 19690.90 | optimizer: 10984.00 | batch generator: 1.74 | data loader: 0.67
10.0.2.15:  iteration      263/    1000 | elapsed time per iteration (ms): 36024.7 | learning rate 3.500E-06 | lm loss 6.947308E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4722.79 | backward: 20560.89 | optimizer: 10258.27 | batch generator: 2.28 | data loader: 1.07
10.0.2.15:  iteration      264/    1000 | elapsed time per iteration (ms): 35858.1 | learning rate 3.514E-06 | lm loss 6.933971E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5725.59 | backward: 19946.42 | optimizer: 9921.77 | batch generator: 1.89 | data loader: 0.71
10.0.2.15:  iteration      265/    1000 | elapsed time per iteration (ms): 37892.8 | learning rate 3.529E-06 | lm loss 6.631628E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6090.82 | backward: 19861.78 | optimizer: 11664.80 | batch generator: 1.35 | data loader: 0.38
10.0.2.15:  iteration      266/    1000 | elapsed time per iteration (ms): 35814.4 | learning rate 3.543E-06 | lm loss 6.981998E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4115.28 | backward: 19240.29 | optimizer: 12236.88 | batch generator: 1.97 | data loader: 0.90
10.0.2.15:  iteration      267/    1000 | elapsed time per iteration (ms): 35184.7 | learning rate 3.557E-06 | lm loss 7.077888E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3696.54 | backward: 19621.02 | optimizer: 11407.70 | batch generator: 1.87 | data loader: 0.75
10.0.2.15:  iteration      268/    1000 | elapsed time per iteration (ms): 35574.8 | learning rate 3.571E-06 | lm loss 7.015011E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4395.15 | backward: 19213.24 | optimizer: 11483.27 | batch generator: 1.40 | data loader: 0.30
10.0.2.15:  iteration      269/    1000 | elapsed time per iteration (ms): 36518.7 | learning rate 3.586E-06 | lm loss 6.553844E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4520.24 | backward: 20096.31 | optimizer: 11566.73 | batch generator: 1.90 | data loader: 0.86
10.0.2.15:  iteration      270/    1000 | elapsed time per iteration (ms): 36178.2 | learning rate 3.600E-06 | lm loss 6.431219E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4184.15 | backward: 20115.00 | optimizer: 11494.30 | batch generator: 1.70 | data loader: 0.69
10.0.2.15:  iteration      271/    1000 | elapsed time per iteration (ms): 34343.7 | learning rate 3.614E-06 | lm loss 6.910863E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4225.57 | backward: 19843.70 | optimizer: 9924.78 | batch generator: 2.16 | data loader: 1.08
10.0.2.15:  iteration      272/    1000 | elapsed time per iteration (ms): 35920.7 | learning rate 3.629E-06 | lm loss 7.299559E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6350.89 | backward: 19590.84 | optimizer: 9977.76 | batch generator: 1.73 | data loader: 0.69
10.0.2.15:  iteration      273/    1000 | elapsed time per iteration (ms): 36723.9 | learning rate 3.643E-06 | lm loss 6.876650E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6160.82 | backward: 19414.00 | optimizer: 11147.68 | batch generator: 1.99 | data loader: 0.87
10.0.2.15:  iteration      274/    1000 | elapsed time per iteration (ms): 36163.0 | learning rate 3.657E-06 | lm loss 6.576982E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4826.83 | backward: 19467.73 | optimizer: 11638.91 | batch generator: 1.45 | data loader: 0.29
10.0.2.15:  iteration      275/    1000 | elapsed time per iteration (ms): 37008.1 | learning rate 3.671E-06 | lm loss 6.928061E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4785.97 | backward: 20862.34 | optimizer: 11358.22 | batch generator: 1.57 | data loader: 0.46
10.0.2.15:  iteration      276/    1000 | elapsed time per iteration (ms): 36726.9 | learning rate 3.686E-06 | lm loss 6.489118E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4886.23 | backward: 20214.08 | optimizer: 11625.36 | batch generator: 1.77 | data loader: 0.70
10.0.2.15:  iteration      277/    1000 | elapsed time per iteration (ms): 35654.7 | learning rate 3.700E-06 | lm loss 6.891376E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4368.47 | backward: 19883.40 | optimizer: 11196.63 | batch generator: 2.03 | data loader: 0.86
10.0.2.15:  iteration      278/    1000 | elapsed time per iteration (ms): 35238.3 | learning rate 3.714E-06 | lm loss 6.913249E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4758.12 | backward: 19869.29 | optimizer: 10420.84 | batch generator: 1.95 | data loader: 0.89
10.0.2.15:  iteration      279/    1000 | elapsed time per iteration (ms): 35588.8 | learning rate 3.729E-06 | lm loss 7.051840E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5519.60 | backward: 19782.74 | optimizer: 9929.79 | batch generator: 1.65 | data loader: 0.35
10.0.2.15:  iteration      280/    1000 | elapsed time per iteration (ms): 36315.6 | learning rate 3.743E-06 | lm loss 6.979157E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5981.35 | backward: 19453.21 | optimizer: 10531.64 | batch generator: 1.81 | data loader: 0.78
10.0.2.15:  iteration      281/    1000 | elapsed time per iteration (ms): 36109.4 | learning rate 3.757E-06 | lm loss 7.294128E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5415.18 | backward: 19532.25 | optimizer: 10918.84 | batch generator: 1.94 | data loader: 0.88
10.0.2.15:  iteration      282/    1000 | elapsed time per iteration (ms): 35448.8 | learning rate 3.771E-06 | lm loss 6.946625E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4868.40 | backward: 19771.57 | optimizer: 10306.98 | batch generator: 1.82 | data loader: 0.69
10.0.2.15:  iteration      283/    1000 | elapsed time per iteration (ms): 36582.6 | learning rate 3.786E-06 | lm loss 6.551939E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6102.11 | backward: 20075.70 | optimizer: 10402.70 | batch generator: 1.84 | data loader: 0.58
10.0.2.15:  iteration      284/    1000 | elapsed time per iteration (ms): 36727.7 | learning rate 3.800E-06 | lm loss 6.790504E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5986.86 | backward: 19902.04 | optimizer: 10837.07 | batch generator: 1.66 | data loader: 0.63
10.0.2.15:  iteration      285/    1000 | elapsed time per iteration (ms): 36042.3 | learning rate 3.814E-06 | lm loss 6.756339E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5104.09 | backward: 19755.60 | optimizer: 10945.65 | batch generator: 1.32 | data loader: 0.26
10.0.2.15:  iteration      286/    1000 | elapsed time per iteration (ms): 35978.1 | learning rate 3.829E-06 | lm loss 6.973805E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5006.58 | backward: 19821.50 | optimizer: 10902.67 | batch generator: 1.93 | data loader: 0.89
10.0.2.15:  iteration      287/    1000 | elapsed time per iteration (ms): 37296.7 | learning rate 3.843E-06 | lm loss 6.960005E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4959.23 | backward: 19888.91 | optimizer: 12172.90 | batch generator: 1.80 | data loader: 0.65
10.0.2.15:  iteration      288/    1000 | elapsed time per iteration (ms): 37085.1 | learning rate 3.857E-06 | lm loss 6.199143E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4244.92 | backward: 20386.10 | optimizer: 12452.79 | batch generator: 1.80 | data loader: 0.70
10.0.2.15:  iteration      289/    1000 | elapsed time per iteration (ms): 36286.0 | learning rate 3.871E-06 | lm loss 7.030659E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3262.79 | backward: 20642.60 | optimizer: 12071.00 | batch generator: 1.86 | data loader: 0.77
10.0.2.15:  iteration      290/    1000 | elapsed time per iteration (ms): 36890.3 | learning rate 3.886E-06 | lm loss 6.141216E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4177.78 | backward: 20339.92 | optimizer: 12371.25 | batch generator: 1.94 | data loader: 0.86
10.0.2.15:  iteration      291/    1000 | elapsed time per iteration (ms): 34724.5 | learning rate 3.900E-06 | lm loss 6.270473E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3681.51 | backward: 20182.03 | optimizer: 10858.82 | batch generator: 1.86 | data loader: 0.58
10.0.2.15:  iteration      292/    1000 | elapsed time per iteration (ms): 35905.9 | learning rate 3.914E-06 | lm loss 6.759384E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5250.05 | backward: 19376.54 | optimizer: 11132.89 | batch generator: 2.14 | data loader: 0.91
10.0.2.15:  iteration      293/    1000 | elapsed time per iteration (ms): 36283.9 | learning rate 3.929E-06 | lm loss 6.498090E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4853.06 | backward: 19522.29 | optimizer: 11650.23 | batch generator: 1.67 | data loader: 0.38
10.0.2.15:  iteration      294/    1000 | elapsed time per iteration (ms): 34202.6 | learning rate 3.943E-06 | lm loss 7.008779E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4067.41 | backward: 19260.49 | optimizer: 10279.30 | batch generator: 1.64 | data loader: 0.49
10.0.2.15:  iteration      295/    1000 | elapsed time per iteration (ms): 37419.9 | learning rate 3.957E-06 | lm loss 6.211887E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5464.77 | backward: 21248.17 | optimizer: 10031.50 | batch generator: 1.74 | data loader: 0.38
10.0.2.15:  iteration      296/    1000 | elapsed time per iteration (ms): 37883.2 | learning rate 3.971E-06 | lm loss 6.600735E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5928.62 | backward: 21343.03 | optimizer: 10317.28 | batch generator: 1.85 | data loader: 0.77
10.0.2.15:  iteration      297/    1000 | elapsed time per iteration (ms): 37457.4 | learning rate 3.986E-06 | lm loss 6.734906E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6148.23 | backward: 20205.29 | optimizer: 11101.98 | batch generator: 2.61 | data loader: 1.24
10.0.2.15:  iteration      298/    1000 | elapsed time per iteration (ms): 36851.8 | learning rate 4.000E-06 | lm loss 6.632574E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5312.19 | backward: 19799.62 | optimizer: 11703.73 | batch generator: 1.82 | data loader: 0.43
10.0.2.15:  iteration      299/    1000 | elapsed time per iteration (ms): 37349.2 | learning rate 4.014E-06 | lm loss 6.415247E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4250.48 | backward: 20676.84 | optimizer: 12250.15 | batch generator: 2.50 | data loader: 1.13
10.0.2.15: [2021-12-31 21:47:00,719] [INFO] [logging.py:60:log_dist] [Rank 0] step=300, skipped=18, lr=[4.014285714285715e-06, 4.014285714285715e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 21:47:12,908] [INFO] [timer.py:154:stop] 0/300, SamplesPerSec=2.2187530606345973
10.0.2.15:  iteration      300/    1000 | elapsed time per iteration (ms): 35605.8 | learning rate 4.029E-06 | lm loss 6.644984E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3753.65 | backward: 19502.78 | optimizer: 12258.01 | batch generator: 2.08 | data loader: 0.73
10.0.2.15:  iteration      301/    1000 | elapsed time per iteration (ms): 35351.4 | learning rate 4.043E-06 | lm loss 6.412721E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3885.92 | backward: 19913.85 | optimizer: 11401.80 | batch generator: 2.31 | data loader: 0.96
10.0.2.15:  iteration      302/    1000 | elapsed time per iteration (ms): 35044.0 | learning rate 4.057E-06 | lm loss 6.879942E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4748.10 | backward: 19361.39 | optimizer: 10825.74 | batch generator: 1.97 | data loader: 0.76
10.0.2.15:  iteration      303/    1000 | elapsed time per iteration (ms): 35859.4 | learning rate 4.071E-06 | lm loss 6.522442E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4935.66 | backward: 20296.39 | optimizer: 10066.17 | batch generator: 1.60 | data loader: 0.32
10.0.2.15:  iteration      304/    1000 | elapsed time per iteration (ms): 37279.1 | learning rate 4.086E-06 | lm loss 6.313196E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6186.31 | backward: 20723.67 | optimizer: 10331.25 | batch generator: 1.49 | data loader: 0.29
10.0.2.15:  iteration      305/    1000 | elapsed time per iteration (ms): 37492.1 | learning rate 4.100E-06 | lm loss 6.557601E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6080.78 | backward: 20590.39 | optimizer: 10818.86 | batch generator: 1.85 | data loader: 0.54
10.0.2.15:  iteration      306/    1000 | elapsed time per iteration (ms): 35873.2 | learning rate 4.114E-06 | lm loss 6.733339E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5186.60 | backward: 19687.96 | optimizer: 10997.41 | batch generator: 1.97 | data loader: 0.75
10.0.2.15:  iteration      307/    1000 | elapsed time per iteration (ms): 37109.1 | learning rate 4.129E-06 | lm loss 6.521245E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4975.20 | backward: 21206.59 | optimizer: 10605.69 | batch generator: 2.45 | data loader: 1.15
10.0.2.15:  iteration      308/    1000 | elapsed time per iteration (ms): 36553.8 | learning rate 4.143E-06 | lm loss 6.571436E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5352.49 | backward: 19586.05 | optimizer: 11297.58 | batch generator: 1.55 | data loader: 0.33
10.0.2.15:  iteration      309/    1000 | elapsed time per iteration (ms): 34817.8 | learning rate 4.157E-06 | lm loss 6.842382E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4514.08 | backward: 19505.50 | optimizer: 10424.93 | batch generator: 2.53 | data loader: 1.17
10.0.2.15:  iteration      310/    1000 | elapsed time per iteration (ms): 36919.1 | learning rate 4.171E-06 | lm loss 6.482182E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5376.47 | backward: 19580.28 | optimizer: 11579.35 | batch generator: 1.94 | data loader: 0.77
10.0.2.15:  iteration      311/    1000 | elapsed time per iteration (ms): 36334.8 | learning rate 4.186E-06 | lm loss 7.039141E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4534.58 | backward: 19423.78 | optimizer: 12341.06 | batch generator: 2.27 | data loader: 0.95
10.0.2.15:  iteration      312/    1000 | elapsed time per iteration (ms): 34610.5 | learning rate 4.200E-06 | lm loss 6.722984E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3736.55 | backward: 19340.96 | optimizer: 11530.42 | batch generator: 1.94 | data loader: 0.43
10.0.2.15:  iteration      313/    1000 | elapsed time per iteration (ms): 35821.9 | learning rate 4.214E-06 | lm loss 6.698735E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4188.59 | backward: 19471.99 | optimizer: 11801.67 | batch generator: 1.90 | data loader: 0.75
10.0.2.15:  iteration      314/    1000 | elapsed time per iteration (ms): 33869.2 | learning rate 4.229E-06 | lm loss 6.824229E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4205.77 | backward: 19370.18 | optimizer: 10146.40 | batch generator: 1.72 | data loader: 0.37
10.0.2.15:  iteration      315/    1000 | elapsed time per iteration (ms): 36958.4 | learning rate 4.243E-06 | lm loss 6.499064E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5798.01 | backward: 19537.00 | optimizer: 11546.19 | batch generator: 1.70 | data loader: 0.38
10.0.2.15:  iteration      316/    1000 | elapsed time per iteration (ms): 36393.3 | learning rate 4.257E-06 | lm loss 6.270308E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4423.39 | backward: 19395.07 | optimizer: 12294.02 | batch generator: 1.77 | data loader: 0.37
10.0.2.15:  iteration      317/    1000 | elapsed time per iteration (ms): 35468.4 | learning rate 4.271E-06 | lm loss 6.840364E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4119.05 | backward: 19772.56 | optimizer: 11575.03 | batch generator: 1.98 | data loader: 0.81
10.0.2.15:  iteration      318/    1000 | elapsed time per iteration (ms): 35923.4 | learning rate 4.286E-06 | lm loss 6.822732E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4322.50 | backward: 19623.90 | optimizer: 11875.54 | batch generator: 1.69 | data loader: 0.37
10.0.2.15:  iteration      319/    1000 | elapsed time per iteration (ms): 35680.2 | learning rate 4.300E-06 | lm loss 7.136057E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3900.18 | backward: 20337.67 | optimizer: 11143.35 | batch generator: 1.55 | data loader: 0.33
10.0.2.15:  iteration      320/    1000 | elapsed time per iteration (ms): 34521.9 | learning rate 4.314E-06 | lm loss 6.375446E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4817.77 | backward: 19157.38 | optimizer: 10209.84 | batch generator: 2.68 | data loader: 1.22
10.0.2.15:  iteration      321/    1000 | elapsed time per iteration (ms): 36894.9 | learning rate 4.329E-06 | lm loss 6.877701E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6171.77 | backward: 20025.20 | optimizer: 10695.48 | batch generator: 1.97 | data loader: 0.43
10.0.2.15:  iteration      322/    1000 | elapsed time per iteration (ms): 35610.3 | learning rate 4.343E-06 | lm loss 6.520474E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5364.74 | backward: 19933.67 | optimizer: 10001.96 | batch generator: 2.48 | data loader: 1.00
10.0.2.15:  iteration      323/    1000 | elapsed time per iteration (ms): 39423.9 | learning rate 4.357E-06 | lm loss 6.596390E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5954.47 | backward: 21696.73 | optimizer: 11491.43 | batch generator: 2.52 | data loader: 1.08
10.0.2.15:  iteration      324/    1000 | elapsed time per iteration (ms): 36060.8 | learning rate 4.371E-06 | lm loss 7.105824E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4703.81 | backward: 21497.41 | optimizer: 9849.09 | batch generator: 2.09 | data loader: 0.62
10.0.2.15:  iteration      325/    1000 | elapsed time per iteration (ms): 39197.9 | learning rate 4.386E-06 | lm loss 6.313556E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6524.66 | backward: 22087.32 | optimizer: 10584.27 | batch generator: 2.50 | data loader: 1.08
10.0.2.15:  iteration      326/    1000 | elapsed time per iteration (ms): 35278.4 | learning rate 4.400E-06 | lm loss 6.776028E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5302.98 | backward: 19689.32 | optimizer: 10189.97 | batch generator: 2.66 | data loader: 1.28
10.0.2.15:  iteration      327/    1000 | elapsed time per iteration (ms): 36924.8 | learning rate 4.414E-06 | lm loss 6.572889E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5622.70 | backward: 20068.54 | optimizer: 10755.37 | batch generator: 2.63 | data loader: 1.25
10.0.2.15:  iteration      328/    1000 | elapsed time per iteration (ms): 36352.9 | learning rate 4.429E-06 | lm loss 6.691238E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5397.81 | backward: 19851.82 | optimizer: 10954.73 | batch generator: 2.68 | data loader: 1.27
10.0.2.15:  iteration      329/    1000 | elapsed time per iteration (ms): 36614.0 | learning rate 4.443E-06 | lm loss 7.049326E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4833.79 | backward: 19745.99 | optimizer: 11783.14 | batch generator: 2.41 | data loader: 1.02
10.0.2.15:  iteration      330/    1000 | elapsed time per iteration (ms): 34426.9 | learning rate 4.457E-06 | lm loss 6.557168E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4153.49 | backward: 19705.34 | optimizer: 10207.56 | batch generator: 1.91 | data loader: 0.59
10.0.2.15:  iteration      331/    1000 | elapsed time per iteration (ms): 36484.9 | learning rate 4.471E-06 | lm loss 6.437448E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5688.08 | backward: 20249.11 | optimizer: 10165.52 | batch generator: 2.43 | data loader: 1.09
10.0.2.15:  iteration      332/    1000 | elapsed time per iteration (ms): 36815.8 | learning rate 4.486E-06 | lm loss 6.741192E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5530.92 | backward: 19394.48 | optimizer: 11612.31 | batch generator: 1.96 | data loader: 0.76
10.0.2.15:  iteration      333/    1000 | elapsed time per iteration (ms): 34865.8 | learning rate 4.500E-06 | lm loss 6.016222E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4329.70 | backward: 19826.60 | optimizer: 10380.76 | batch generator: 2.52 | data loader: 1.17
10.0.2.15:  iteration      334/    1000 | elapsed time per iteration (ms): 37354.0 | learning rate 4.514E-06 | lm loss 6.036536E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6011.04 | backward: 20249.76 | optimizer: 11091.65 | batch generator: 2.10 | data loader: 0.86
10.0.2.15:  iteration      335/    1000 | elapsed time per iteration (ms): 36317.9 | learning rate 4.529E-06 | lm loss 6.393348E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4609.65 | backward: 19591.11 | optimizer: 11524.78 | batch generator: 2.00 | data loader: 0.91
10.0.2.15:  iteration      336/    1000 | elapsed time per iteration (ms): 34460.7 | learning rate 4.543E-06 | lm loss 6.355996E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4588.10 | backward: 19686.15 | optimizer: 10039.06 | batch generator: 2.24 | data loader: 0.96
10.0.2.15:  iteration      337/    1000 | elapsed time per iteration (ms): 36784.6 | learning rate 4.557E-06 | lm loss 6.262379E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6358.00 | backward: 20619.80 | optimizer: 9805.60 | batch generator: 2.00 | data loader: 0.82
10.0.2.15:  iteration      338/    1000 | elapsed time per iteration (ms): 37822.3 | learning rate 4.571E-06 | lm loss 6.438565E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6147.78 | backward: 19980.23 | optimizer: 11616.06 | batch generator: 1.35 | data loader: 0.26
10.0.2.15:  iteration      339/    1000 | elapsed time per iteration (ms): 36517.0 | learning rate 4.586E-06 | lm loss 7.322870E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4164.77 | backward: 20168.21 | optimizer: 11807.27 | batch generator: 1.42 | data loader: 0.31
10.0.2.15:  iteration      340/    1000 | elapsed time per iteration (ms): 35593.6 | learning rate 4.600E-06 | lm loss 6.980761E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3913.66 | backward: 19569.61 | optimizer: 11847.24 | batch generator: 1.88 | data loader: 0.57
10.0.2.15:  iteration      341/    1000 | elapsed time per iteration (ms): 34878.1 | learning rate 4.614E-06 | lm loss 6.700128E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4126.34 | backward: 19999.46 | optimizer: 10606.46 | batch generator: 1.77 | data loader: 0.38
10.0.2.15:  iteration      342/    1000 | elapsed time per iteration (ms): 36868.4 | learning rate 4.629E-06 | lm loss 6.568768E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5130.04 | backward: 20149.49 | optimizer: 10878.28 | batch generator: 1.69 | data loader: 0.36
10.0.2.15:  iteration      343/    1000 | elapsed time per iteration (ms): 35833.4 | learning rate 4.643E-06 | lm loss 6.746774E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4839.12 | backward: 20051.53 | optimizer: 10646.42 | batch generator: 1.75 | data loader: 0.37
10.0.2.15:  iteration      344/    1000 | elapsed time per iteration (ms): 36131.9 | learning rate 4.657E-06 | lm loss 6.540720E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5188.20 | backward: 19315.80 | optimizer: 11205.46 | batch generator: 1.90 | data loader: 0.56
10.0.2.15:  iteration      345/    1000 | elapsed time per iteration (ms): 36418.2 | learning rate 4.671E-06 | lm loss 6.357414E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4815.34 | backward: 19732.81 | optimizer: 11601.34 | batch generator: 2.50 | data loader: 1.08
10.0.2.15:  iteration      346/    1000 | elapsed time per iteration (ms): 35218.5 | learning rate 4.686E-06 | lm loss 6.330113E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4381.14 | backward: 19647.00 | optimizer: 11019.26 | batch generator: 2.40 | data loader: 1.03
10.0.2.15:  iteration      347/    1000 | elapsed time per iteration (ms): 36792.3 | learning rate 4.700E-06 | lm loss 6.841041E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4957.73 | backward: 19997.17 | optimizer: 11835.06 | batch generator: 2.69 | data loader: 0.79
10.0.2.15:  iteration      348/    1000 | elapsed time per iteration (ms): 36762.3 | learning rate 4.714E-06 | lm loss 6.749943E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4104.73 | backward: 20111.69 | optimizer: 12225.41 | batch generator: 2.31 | data loader: 0.44
10.0.2.15:  iteration      349/    1000 | elapsed time per iteration (ms): 36063.9 | learning rate 4.729E-06 | lm loss 6.008168E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4192.33 | backward: 20213.45 | optimizer: 11656.27 | batch generator: 1.66 | data loader: 0.38
10.0.2.15: [2021-12-31 22:17:09,901] [INFO] [logging.py:60:log_dist] [Rank 0] step=350, skipped=18, lr=[4.728571428571429e-06, 4.728571428571429e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 22:17:20,207] [INFO] [timer.py:154:stop] 0/350, SamplesPerSec=2.233305524636185
10.0.2.15:  iteration      350/    1000 | elapsed time per iteration (ms): 34649.2 | learning rate 4.743E-06 | lm loss 6.861779E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4355.56 | backward: 19565.98 | optimizer: 10405.24 | batch generator: 1.86 | data loader: 0.56
10.0.2.15:  iteration      351/    1000 | elapsed time per iteration (ms): 36671.6 | learning rate 4.757E-06 | lm loss 6.561181E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5338.39 | backward: 20038.70 | optimizer: 10760.57 | batch generator: 1.40 | data loader: 0.29
10.0.2.15:  iteration      352/    1000 | elapsed time per iteration (ms): 35782.1 | learning rate 4.771E-06 | lm loss 6.139709E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5202.64 | backward: 19838.55 | optimizer: 10521.08 | batch generator: 1.37 | data loader: 0.27
10.0.2.15:  iteration      353/    1000 | elapsed time per iteration (ms): 36434.7 | learning rate 4.786E-06 | lm loss 6.706210E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5432.60 | backward: 19888.72 | optimizer: 10882.59 | batch generator: 1.50 | data loader: 0.40
10.0.2.15:  iteration      354/    1000 | elapsed time per iteration (ms): 35690.1 | learning rate 4.800E-06 | lm loss 6.428447E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5005.45 | backward: 19224.29 | optimizer: 11054.01 | batch generator: 2.14 | data loader: 0.67
10.0.2.15:  iteration      355/    1000 | elapsed time per iteration (ms): 36104.4 | learning rate 4.814E-06 | lm loss 6.519116E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5207.12 | backward: 19775.89 | optimizer: 11119.24 | batch generator: 1.90 | data loader: 0.63
10.0.2.15:  iteration      356/    1000 | elapsed time per iteration (ms): 36642.2 | learning rate 4.829E-06 | lm loss 6.844265E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4679.23 | backward: 19672.04 | optimizer: 11786.02 | batch generator: 2.53 | data loader: 0.91
10.0.2.15:  iteration      357/    1000 | elapsed time per iteration (ms): 33826.5 | learning rate 4.843E-06 | lm loss 6.994272E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3958.63 | backward: 19777.80 | optimizer: 9866.59 | batch generator: 2.38 | data loader: 0.95
10.0.2.15:  iteration      358/    1000 | elapsed time per iteration (ms): 36625.9 | learning rate 4.857E-06 | lm loss 6.715898E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6116.20 | backward: 19804.55 | optimizer: 10429.32 | batch generator: 1.73 | data loader: 0.36
10.0.2.15:  iteration      359/    1000 | elapsed time per iteration (ms): 37269.1 | learning rate 4.871E-06 | lm loss 5.981836E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5978.62 | backward: 20068.39 | optimizer: 11219.98 | batch generator: 1.63 | data loader: 0.36
10.0.2.15:  iteration      360/    1000 | elapsed time per iteration (ms): 36094.6 | learning rate 4.886E-06 | lm loss 6.883148E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4803.46 | backward: 19580.80 | optimizer: 11708.38 | batch generator: 1.96 | data loader: 0.74
10.0.2.15:  iteration      361/    1000 | elapsed time per iteration (ms): 35036.1 | learning rate 4.900E-06 | lm loss 7.092558E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4039.73 | backward: 19220.87 | optimizer: 11565.57 | batch generator: 1.68 | data loader: 0.36
10.0.2.15:  iteration      362/    1000 | elapsed time per iteration (ms): 35477.3 | learning rate 4.914E-06 | lm loss 6.276234E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4633.45 | backward: 19700.99 | optimizer: 11140.70 | batch generator: 1.66 | data loader: 0.33
10.0.2.15:  iteration      363/    1000 | elapsed time per iteration (ms): 36405.1 | learning rate 4.929E-06 | lm loss 6.717596E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4787.49 | backward: 19737.86 | optimizer: 11496.36 | batch generator: 1.69 | data loader: 0.36
10.0.2.15:  iteration      364/    1000 | elapsed time per iteration (ms): 35835.6 | learning rate 4.943E-06 | lm loss 6.628922E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4469.91 | backward: 19498.57 | optimizer: 11842.20 | batch generator: 1.84 | data loader: 0.39
10.0.2.15:  iteration      365/    1000 | elapsed time per iteration (ms): 35990.9 | learning rate 4.957E-06 | lm loss 6.569912E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4409.32 | backward: 19965.66 | optimizer: 11613.97 | batch generator: 1.82 | data loader: 0.38
10.0.2.15:  iteration      366/    1000 | elapsed time per iteration (ms): 35653.7 | learning rate 4.971E-06 | lm loss 6.646744E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4344.13 | backward: 19642.24 | optimizer: 11665.58 | batch generator: 1.65 | data loader: 0.36
10.0.2.15:  iteration      367/    1000 | elapsed time per iteration (ms): 36180.3 | learning rate 4.986E-06 | lm loss 6.194166E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4359.11 | backward: 20025.03 | optimizer: 11471.67 | batch generator: 1.61 | data loader: 0.38
10.0.2.15:  iteration      368/    1000 | elapsed time per iteration (ms): 36672.0 | learning rate 5.000E-06 | lm loss 6.158502E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4813.07 | backward: 19926.92 | optimizer: 11928.69 | batch generator: 1.71 | data loader: 0.35
10.0.2.15:  iteration      369/    1000 | elapsed time per iteration (ms): 35002.6 | learning rate 5.014E-06 | lm loss 5.821394E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3994.12 | backward: 19669.77 | optimizer: 11062.35 | batch generator: 1.74 | data loader: 0.39
10.0.2.15:  iteration      370/    1000 | elapsed time per iteration (ms): 35909.5 | learning rate 5.029E-06 | lm loss 6.180338E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4928.32 | backward: 19164.99 | optimizer: 11432.37 | batch generator: 1.72 | data loader: 0.49
10.0.2.15:  iteration      371/    1000 | elapsed time per iteration (ms): 36370.9 | learning rate 5.043E-06 | lm loss 7.039589E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4684.33 | backward: 19913.07 | optimizer: 11593.56 | batch generator: 1.41 | data loader: 0.27
10.0.2.15:  iteration      372/    1000 | elapsed time per iteration (ms): 34785.3 | learning rate 5.057E-06 | lm loss 6.062949E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4383.19 | backward: 19451.80 | optimizer: 10947.98 | batch generator: 1.65 | data loader: 0.36
10.0.2.15:  iteration      373/    1000 | elapsed time per iteration (ms): 36231.7 | learning rate 5.071E-06 | lm loss 6.421690E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5451.86 | backward: 19651.84 | optimizer: 11126.22 | batch generator: 1.91 | data loader: 0.46
10.0.2.15:  iteration      374/    1000 | elapsed time per iteration (ms): 36504.6 | learning rate 5.086E-06 | lm loss 7.006752E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4595.32 | backward: 19668.08 | optimizer: 11947.57 | batch generator: 1.45 | data loader: 0.30
10.0.2.15:  iteration      375/    1000 | elapsed time per iteration (ms): 36718.6 | learning rate 5.100E-06 | lm loss 6.338938E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4481.12 | backward: 20057.59 | optimizer: 12178.04 | batch generator: 1.70 | data loader: 0.45
10.0.2.15:  iteration      376/    1000 | elapsed time per iteration (ms): 36100.5 | learning rate 5.114E-06 | lm loss 6.154885E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3917.81 | backward: 19727.37 | optimizer: 12383.05 | batch generator: 1.36 | data loader: 0.28
10.0.2.15:  iteration      377/    1000 | elapsed time per iteration (ms): 32999.5 | learning rate 5.129E-06 | lm loss 7.038638E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3363.83 | backward: 19394.86 | optimizer: 9769.81 | batch generator: 2.28 | data loader: 1.04
10.0.2.15:  iteration      378/    1000 | elapsed time per iteration (ms): 37020.4 | learning rate 5.143E-06 | lm loss 6.665005E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6265.62 | backward: 19637.21 | optimizer: 11115.74 | batch generator: 1.41 | data loader: 0.41
10.0.2.15:  iteration      379/    1000 | elapsed time per iteration (ms): 35530.3 | learning rate 5.157E-06 | lm loss 6.388731E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4652.54 | backward: 19193.83 | optimizer: 11180.73 | batch generator: 1.65 | data loader: 0.41
10.0.2.15:  iteration      380/    1000 | elapsed time per iteration (ms): 35420.0 | learning rate 5.171E-06 | lm loss 6.288217E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4892.75 | backward: 19662.18 | optimizer: 10810.15 | batch generator: 1.76 | data loader: 0.69
10.0.2.15:  iteration      381/    1000 | elapsed time per iteration (ms): 37337.3 | learning rate 5.186E-06 | lm loss 6.115917E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5186.97 | backward: 19990.49 | optimizer: 12092.65 | batch generator: 1.58 | data loader: 0.44
10.0.2.15:  iteration      382/    1000 | elapsed time per iteration (ms): 36168.3 | learning rate 5.200E-06 | lm loss 6.504730E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3878.72 | backward: 19886.96 | optimizer: 12094.05 | batch generator: 2.22 | data loader: 0.96
10.0.2.15:  iteration      383/    1000 | elapsed time per iteration (ms): 35028.2 | learning rate 5.214E-06 | lm loss 6.976223E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3652.94 | backward: 19444.42 | optimizer: 11421.70 | batch generator: 1.60 | data loader: 0.36
10.0.2.15:  iteration      384/    1000 | elapsed time per iteration (ms): 35931.2 | learning rate 5.229E-06 | lm loss 6.081258E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4839.63 | backward: 20135.77 | optimizer: 10953.57 | batch generator: 2.42 | data loader: 1.20
10.0.2.15:  iteration      385/    1000 | elapsed time per iteration (ms): 36953.8 | learning rate 5.243E-06 | lm loss 6.250542E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5024.10 | backward: 19571.06 | optimizer: 12072.80 | batch generator: 2.01 | data loader: 0.48
10.0.2.15:  iteration      386/    1000 | elapsed time per iteration (ms): 36309.7 | learning rate 5.257E-06 | lm loss 6.288070E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4322.34 | backward: 19471.39 | optimizer: 12513.81 | batch generator: 2.44 | data loader: 1.09
10.0.2.15:  iteration      387/    1000 | elapsed time per iteration (ms): 35084.2 | learning rate 5.271E-06 | lm loss 6.786596E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3510.31 | backward: 19620.06 | optimizer: 11929.40 | batch generator: 2.43 | data loader: 1.23
10.0.2.15:  iteration      388/    1000 | elapsed time per iteration (ms): 36354.7 | learning rate 5.286E-06 | lm loss 5.749380E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4028.45 | backward: 19831.31 | optimizer: 12165.90 | batch generator: 2.05 | data loader: 0.92
10.0.2.15:  iteration      389/    1000 | elapsed time per iteration (ms): 36392.7 | learning rate 5.300E-06 | lm loss 6.353556E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4051.04 | backward: 19958.47 | optimizer: 12215.68 | batch generator: 1.68 | data loader: 0.55
10.0.2.15:  iteration      390/    1000 | elapsed time per iteration (ms): 35516.7 | learning rate 5.314E-06 | lm loss 6.573334E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4035.99 | backward: 20345.23 | optimizer: 11134.25 | batch generator: 2.03 | data loader: 0.87
10.0.2.15:  iteration      391/    1000 | elapsed time per iteration (ms): 35224.6 | learning rate 5.329E-06 | lm loss 6.701977E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5082.46 | backward: 19634.81 | optimizer: 10505.91 | batch generator: 2.10 | data loader: 0.94
10.0.2.15:  iteration      392/    1000 | elapsed time per iteration (ms): 36845.3 | learning rate 5.343E-06 | lm loss 6.936293E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5456.33 | backward: 19713.68 | optimizer: 11369.64 | batch generator: 1.91 | data loader: 0.83
10.0.2.15:  iteration      393/    1000 | elapsed time per iteration (ms): 34729.8 | learning rate 5.357E-06 | lm loss 6.645602E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4527.81 | backward: 19720.13 | optimizer: 10217.70 | batch generator: 2.71 | data loader: 1.44
10.0.2.15:  iteration      394/    1000 | elapsed time per iteration (ms): 37162.4 | learning rate 5.371E-06 | lm loss 6.783661E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6180.84 | backward: 20362.14 | optimizer: 10617.93 | batch generator: 2.59 | data loader: 1.25
10.0.2.15:  iteration      395/    1000 | elapsed time per iteration (ms): 35141.8 | learning rate 5.386E-06 | lm loss 6.299035E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5306.08 | backward: 19413.24 | optimizer: 10045.27 | batch generator: 1.82 | data loader: 0.55
10.0.2.15:  iteration      396/    1000 | elapsed time per iteration (ms): 36144.1 | learning rate 5.400E-06 | lm loss 6.374129E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6016.03 | backward: 19671.36 | optimizer: 10147.57 | batch generator: 2.10 | data loader: 1.02
10.0.2.15:  iteration      397/    1000 | elapsed time per iteration (ms): 36950.0 | learning rate 5.414E-06 | lm loss 6.654506E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5631.97 | backward: 19765.04 | optimizer: 11269.83 | batch generator: 1.46 | data loader: 0.36
10.0.2.15:  iteration      398/    1000 | elapsed time per iteration (ms): 34867.3 | learning rate 5.429E-06 | lm loss 6.023105E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4740.78 | backward: 19825.59 | optimizer: 10299.57 | batch generator: 2.06 | data loader: 0.96
10.0.2.15:  iteration      399/    1000 | elapsed time per iteration (ms): 35645.6 | learning rate 5.443E-06 | lm loss 6.293253E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5676.18 | backward: 19426.18 | optimizer: 10518.25 | batch generator: 2.46 | data loader: 1.23
10.0.2.15: [2021-12-31 22:47:05,731] [INFO] [logging.py:60:log_dist] [Rank 0] step=400, skipped=18, lr=[5.442857142857143e-06, 5.442857142857143e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 22:47:16,229] [INFO] [timer.py:154:stop] 0/400, SamplesPerSec=2.2460267046633184
10.0.2.15:  iteration      400/    1000 | elapsed time per iteration (ms): 35247.7 | learning rate 5.457E-06 | lm loss 6.441321E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5462.48 | backward: 19229.84 | optimizer: 10554.15 | batch generator: 1.97 | data loader: 0.29
10.0.2.15:  iteration      401/    1000 | elapsed time per iteration (ms): 34899.7 | learning rate 5.471E-06 | lm loss 6.875510E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4496.78 | backward: 18963.84 | optimizer: 10526.35 | batch generator: 1.66 | data loader: 0.66
10.0.2.15:  iteration      402/    1000 | elapsed time per iteration (ms): 37229.2 | learning rate 5.486E-06 | lm loss 6.340280E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5433.40 | backward: 19749.29 | optimizer: 11625.00 | batch generator: 1.85 | data loader: 0.55
10.0.2.15:  iteration      403/    1000 | elapsed time per iteration (ms): 34596.3 | learning rate 5.500E-06 | lm loss 6.188630E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4176.64 | backward: 19664.62 | optimizer: 10254.45 | batch generator: 1.56 | data loader: 0.36
10.0.2.15:  iteration      404/    1000 | elapsed time per iteration (ms): 37841.5 | learning rate 5.514E-06 | lm loss 6.226010E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5829.92 | backward: 19869.47 | optimizer: 12046.77 | batch generator: 1.52 | data loader: 0.34
10.0.2.15:  iteration      405/    1000 | elapsed time per iteration (ms): 36550.0 | learning rate 5.529E-06 | lm loss 6.577025E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4228.42 | backward: 19659.58 | optimizer: 12523.41 | batch generator: 1.53 | data loader: 0.28
10.0.2.15:  iteration      406/    1000 | elapsed time per iteration (ms): 35645.9 | learning rate 5.543E-06 | lm loss 6.541671E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3399.12 | backward: 19477.23 | optimizer: 12501.44 | batch generator: 1.81 | data loader: 0.55
10.0.2.15:  iteration      407/    1000 | elapsed time per iteration (ms): 34849.4 | learning rate 5.557E-06 | lm loss 6.276798E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3487.37 | backward: 19396.41 | optimizer: 11815.07 | batch generator: 1.82 | data loader: 0.67
10.0.2.15:  iteration      408/    1000 | elapsed time per iteration (ms): 35741.1 | learning rate 5.571E-06 | lm loss 6.570132E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4222.55 | backward: 19304.23 | optimizer: 12193.09 | batch generator: 2.16 | data loader: 0.92
10.0.2.15:  iteration      409/    1000 | elapsed time per iteration (ms): 35825.5 | learning rate 5.586E-06 | lm loss 6.802712E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3550.43 | backward: 19172.11 | optimizer: 12507.84 | batch generator: 1.92 | data loader: 0.79
10.0.2.15:  iteration      410/    1000 | elapsed time per iteration (ms): 35527.6 | learning rate 5.600E-06 | lm loss 6.672522E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3555.84 | backward: 19792.25 | optimizer: 11853.57 | batch generator: 1.92 | data loader: 0.84
10.0.2.15:  iteration      411/    1000 | elapsed time per iteration (ms): 36026.0 | learning rate 5.614E-06 | lm loss 6.514276E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3943.62 | backward: 19821.45 | optimizer: 11859.85 | batch generator: 2.25 | data loader: 0.99
10.0.2.15:  iteration      412/    1000 | elapsed time per iteration (ms): 35647.5 | learning rate 5.629E-06 | lm loss 6.344348E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3831.81 | backward: 19271.45 | optimizer: 12316.57 | batch generator: 1.81 | data loader: 0.72
10.0.2.15:  iteration      413/    1000 | elapsed time per iteration (ms): 35780.4 | learning rate 5.643E-06 | lm loss 6.424585E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 2532.75 | backward: 19634.24 | optimizer: 12170.97 | batch generator: 1.75 | data loader: 0.74
10.0.2.15:  iteration      414/    1000 | elapsed time per iteration (ms): 34528.6 | learning rate 5.657E-06 | lm loss 6.576152E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3803.03 | backward: 19314.60 | optimizer: 11307.13 | batch generator: 2.24 | data loader: 1.01
10.0.2.15:  iteration      415/    1000 | elapsed time per iteration (ms): 36712.3 | learning rate 5.671E-06 | lm loss 6.269944E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4837.92 | backward: 19743.58 | optimizer: 11925.77 | batch generator: 1.80 | data loader: 0.70
10.0.2.15:  iteration      416/    1000 | elapsed time per iteration (ms): 35700.1 | learning rate 5.686E-06 | lm loss 6.870623E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3888.97 | backward: 19190.09 | optimizer: 12460.82 | batch generator: 1.90 | data loader: 0.83
10.0.2.15:  iteration      417/    1000 | elapsed time per iteration (ms): 35738.7 | learning rate 5.700E-06 | lm loss 6.584401E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3587.23 | backward: 19712.37 | optimizer: 12392.74 | batch generator: 1.71 | data loader: 0.39
10.0.2.15:  iteration      418/    1000 | elapsed time per iteration (ms): 35503.0 | learning rate 5.714E-06 | lm loss 6.501629E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3754.73 | backward: 19525.34 | optimizer: 12011.51 | batch generator: 1.40 | data loader: 0.29
10.0.2.15:  iteration      419/    1000 | elapsed time per iteration (ms): 35657.0 | learning rate 5.729E-06 | lm loss 6.331742E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3964.51 | backward: 19636.50 | optimizer: 11636.50 | batch generator: 1.85 | data loader: 0.85
10.0.2.15:  iteration      420/    1000 | elapsed time per iteration (ms): 36618.2 | learning rate 5.743E-06 | lm loss 6.389059E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4761.93 | backward: 20260.36 | optimizer: 11593.65 | batch generator: 1.93 | data loader: 0.66
10.0.2.15:  iteration      421/    1000 | elapsed time per iteration (ms): 34076.4 | learning rate 5.757E-06 | lm loss 6.352262E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4528.69 | backward: 19777.91 | optimizer: 9759.53 | batch generator: 1.72 | data loader: 0.58
10.0.2.15:  iteration      422/    1000 | elapsed time per iteration (ms): 36934.9 | learning rate 5.771E-06 | lm loss 6.345569E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6621.43 | backward: 20048.07 | optimizer: 10263.61 | batch generator: 1.39 | data loader: 0.26
10.0.2.15:  iteration      423/    1000 | elapsed time per iteration (ms): 36567.4 | learning rate 5.786E-06 | lm loss 6.689649E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5803.51 | backward: 19731.72 | optimizer: 10726.15 | batch generator: 1.58 | data loader: 0.37
10.0.2.15:  iteration      424/    1000 | elapsed time per iteration (ms): 35611.5 | learning rate 5.800E-06 | lm loss 5.504701E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5254.07 | backward: 19676.52 | optimizer: 10494.67 | batch generator: 1.40 | data loader: 0.30
10.0.2.15:  iteration      425/    1000 | elapsed time per iteration (ms): 36610.8 | learning rate 5.814E-06 | lm loss 6.266674E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5729.42 | backward: 19802.50 | optimizer: 11068.63 | batch generator: 1.37 | data loader: 0.24
10.0.2.15:  iteration      426/    1000 | elapsed time per iteration (ms): 36197.2 | learning rate 5.829E-06 | lm loss 6.780623E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4998.49 | backward: 19641.61 | optimizer: 11484.47 | batch generator: 1.61 | data loader: 0.37
10.0.2.15:  iteration      427/    1000 | elapsed time per iteration (ms): 36781.9 | learning rate 5.843E-06 | lm loss 6.337067E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4501.84 | backward: 19834.04 | optimizer: 12096.40 | batch generator: 1.30 | data loader: 0.23
10.0.2.15:  iteration      428/    1000 | elapsed time per iteration (ms): 35877.5 | learning rate 5.857E-06 | lm loss 6.559421E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3887.25 | backward: 19768.04 | optimizer: 12169.32 | batch generator: 1.61 | data loader: 0.37
10.0.2.15:  iteration      429/    1000 | elapsed time per iteration (ms): 34272.7 | learning rate 5.871E-06 | lm loss 6.188365E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3835.37 | backward: 19518.22 | optimizer: 10582.75 | batch generator: 1.61 | data loader: 0.53
10.0.2.15:  iteration      430/    1000 | elapsed time per iteration (ms): 36372.7 | learning rate 5.886E-06 | lm loss 7.077433E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5057.14 | backward: 19374.33 | optimizer: 11651.04 | batch generator: 1.70 | data loader: 0.55
10.0.2.15:  iteration      431/    1000 | elapsed time per iteration (ms): 35830.3 | learning rate 5.900E-06 | lm loss 6.433086E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4563.09 | backward: 19648.65 | optimizer: 11584.29 | batch generator: 1.39 | data loader: 0.27
10.0.2.15:  iteration      432/    1000 | elapsed time per iteration (ms): 36641.6 | learning rate 5.914E-06 | lm loss 6.211318E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4640.77 | backward: 19792.81 | optimizer: 12206.59 | batch generator: 2.27 | data loader: 1.08
10.0.2.15:  iteration      433/    1000 | elapsed time per iteration (ms): 35469.3 | learning rate 5.929E-06 | lm loss 6.202803E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3789.88 | backward: 19622.20 | optimizer: 12050.96 | batch generator: 1.83 | data loader: 0.29
10.0.2.15:  iteration      434/    1000 | elapsed time per iteration (ms): 35869.5 | learning rate 5.943E-06 | lm loss 6.059967E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4024.59 | backward: 19822.35 | optimizer: 11667.62 | batch generator: 2.31 | data loader: 1.10
10.0.2.15:  iteration      435/    1000 | elapsed time per iteration (ms): 34595.4 | learning rate 5.957E-06 | lm loss 6.184827E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4623.65 | backward: 19816.50 | optimizer: 10154.02 | batch generator: 2.02 | data loader: 0.92
10.0.2.15:  iteration      436/    1000 | elapsed time per iteration (ms): 36336.8 | learning rate 5.971E-06 | lm loss 6.816440E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5734.41 | backward: 19736.43 | optimizer: 10355.67 | batch generator: 1.91 | data loader: 0.87
10.0.2.15:  iteration      437/    1000 | elapsed time per iteration (ms): 36457.9 | learning rate 5.986E-06 | lm loss 6.264156E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5816.75 | backward: 19949.55 | optimizer: 10589.17 | batch generator: 1.87 | data loader: 0.76
10.0.2.15:  iteration      438/    1000 | elapsed time per iteration (ms): 35753.7 | learning rate 6.000E-06 | lm loss 6.486648E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5644.02 | backward: 20068.95 | optimizer: 10038.64 | batch generator: 1.71 | data loader: 0.54
10.0.2.15:  iteration      439/    1000 | elapsed time per iteration (ms): 36827.5 | learning rate 6.014E-06 | lm loss 6.111041E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6183.23 | backward: 19896.98 | optimizer: 10743.00 | batch generator: 1.69 | data loader: 0.63
10.0.2.15:  iteration      440/    1000 | elapsed time per iteration (ms): 36235.5 | learning rate 6.029E-06 | lm loss 6.500100E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5287.80 | backward: 19544.41 | optimizer: 11012.81 | batch generator: 2.06 | data loader: 0.93
10.0.2.15:  iteration      441/    1000 | elapsed time per iteration (ms): 37191.2 | learning rate 6.043E-06 | lm loss 6.296256E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5009.95 | backward: 20024.97 | optimizer: 11811.21 | batch generator: 2.09 | data loader: 0.93
10.0.2.15:  iteration      442/    1000 | elapsed time per iteration (ms): 36872.4 | learning rate 6.057E-06 | lm loss 6.152062E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4579.46 | backward: 20078.98 | optimizer: 12212.66 | batch generator: 2.23 | data loader: 1.11
10.0.2.15:  iteration      443/    1000 | elapsed time per iteration (ms): 35620.3 | learning rate 6.071E-06 | lm loss 6.682765E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4113.75 | backward: 19847.05 | optimizer: 11658.09 | batch generator: 2.23 | data loader: 0.96
10.0.2.15:  iteration      444/    1000 | elapsed time per iteration (ms): 36056.3 | learning rate 6.086E-06 | lm loss 6.188443E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4332.34 | backward: 19619.93 | optimizer: 11777.08 | batch generator: 1.93 | data loader: 0.86
10.0.2.15:  iteration      445/    1000 | elapsed time per iteration (ms): 35774.6 | learning rate 6.100E-06 | lm loss 6.546636E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3991.77 | backward: 19648.49 | optimizer: 11783.18 | batch generator: 1.95 | data loader: 0.86
10.0.2.15:  iteration      446/    1000 | elapsed time per iteration (ms): 36037.3 | learning rate 6.114E-06 | lm loss 6.285171E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4168.49 | backward: 19611.85 | optimizer: 11791.31 | batch generator: 2.16 | data loader: 0.90
10.0.2.15:  iteration      447/    1000 | elapsed time per iteration (ms): 35198.2 | learning rate 6.129E-06 | lm loss 5.997548E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4408.09 | backward: 19792.27 | optimizer: 10996.40 | batch generator: 1.67 | data loader: 0.62
10.0.2.15:  iteration      448/    1000 | elapsed time per iteration (ms): 36403.0 | learning rate 6.143E-06 | lm loss 6.349748E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5260.56 | backward: 19758.73 | optimizer: 11356.17 | batch generator: 1.39 | data loader: 0.24
10.0.2.15:  iteration      449/    1000 | elapsed time per iteration (ms): 36696.3 | learning rate 6.157E-06 | lm loss 7.042182E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4400.47 | backward: 19598.58 | optimizer: 12160.16 | batch generator: 1.47 | data loader: 0.35
10.0.2.15: [2021-12-31 23:17:02,269] [INFO] [logging.py:60:log_dist] [Rank 0] step=450, skipped=18, lr=[6.157142857142857e-06, 6.157142857142857e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 23:17:14,239] [INFO] [timer.py:154:stop] 0/450, SamplesPerSec=2.2557376299473404
10.0.2.15:  iteration      450/    1000 | elapsed time per iteration (ms): 36220.6 | learning rate 6.171E-06 | lm loss 6.460935E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4386.15 | backward: 19756.08 | optimizer: 12077.05 | batch generator: 2.11 | data loader: 1.05
10.0.2.15:  iteration      451/    1000 | elapsed time per iteration (ms): 35248.3 | learning rate 6.186E-06 | lm loss 5.892678E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4191.26 | backward: 19711.49 | optimizer: 11344.00 | batch generator: 2.32 | data loader: 1.03
10.0.2.15:  iteration      452/    1000 | elapsed time per iteration (ms): 36042.5 | learning rate 6.200E-06 | lm loss 6.320687E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4644.34 | backward: 19703.60 | optimizer: 11505.91 | batch generator: 2.01 | data loader: 0.89
10.0.2.15:  iteration      453/    1000 | elapsed time per iteration (ms): 35450.2 | learning rate 6.214E-06 | lm loss 6.801810E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3802.94 | backward: 18949.52 | optimizer: 12071.83 | batch generator: 1.77 | data loader: 0.74
10.0.2.15:  iteration      454/    1000 | elapsed time per iteration (ms): 37140.0 | learning rate 6.229E-06 | lm loss 6.254972E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4040.96 | backward: 20385.30 | optimizer: 12380.80 | batch generator: 2.00 | data loader: 0.89
10.0.2.15:  iteration      455/    1000 | elapsed time per iteration (ms): 34908.1 | learning rate 6.243E-06 | lm loss 6.330034E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3327.70 | backward: 19248.41 | optimizer: 11777.45 | batch generator: 2.10 | data loader: 0.98
10.0.2.15:  iteration      456/    1000 | elapsed time per iteration (ms): 34159.9 | learning rate 6.257E-06 | lm loss 6.332178E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4286.00 | backward: 19510.39 | optimizer: 10362.28 | batch generator: 2.32 | data loader: 1.21
10.0.2.15:  iteration      457/    1000 | elapsed time per iteration (ms): 37046.4 | learning rate 6.271E-06 | lm loss 6.268973E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5596.10 | backward: 20122.18 | optimizer: 10969.54 | batch generator: 2.08 | data loader: 1.07
10.0.2.15:  iteration      458/    1000 | elapsed time per iteration (ms): 36427.5 | learning rate 6.286E-06 | lm loss 6.566276E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5129.68 | backward: 19976.07 | optimizer: 11029.84 | batch generator: 2.12 | data loader: 1.04
10.0.2.15:  iteration      459/    1000 | elapsed time per iteration (ms): 37516.2 | learning rate 6.300E-06 | lm loss 6.262754E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4808.40 | backward: 20514.59 | optimizer: 11803.71 | batch generator: 1.88 | data loader: 0.81
10.0.2.15:  iteration      460/    1000 | elapsed time per iteration (ms): 35798.4 | learning rate 6.314E-06 | lm loss 6.550500E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4350.96 | backward: 19627.80 | optimizer: 11818.26 | batch generator: 2.22 | data loader: 1.02
10.0.2.15:  iteration      461/    1000 | elapsed time per iteration (ms): 35800.5 | learning rate 6.329E-06 | lm loss 7.003669E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4152.78 | backward: 19165.58 | optimizer: 12480.80 | batch generator: 2.05 | data loader: 0.98
10.0.2.15:  iteration      462/    1000 | elapsed time per iteration (ms): 35973.8 | learning rate 6.343E-06 | lm loss 6.247708E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3553.36 | backward: 19669.54 | optimizer: 12408.53 | batch generator: 1.95 | data loader: 0.81
10.0.2.15:  iteration      463/    1000 | elapsed time per iteration (ms): 34976.9 | learning rate 6.357E-06 | lm loss 5.836327E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3868.31 | backward: 19873.55 | optimizer: 11233.69 | batch generator: 2.61 | data loader: 1.34
10.0.2.15:  iteration      464/    1000 | elapsed time per iteration (ms): 36067.5 | learning rate 6.371E-06 | lm loss 6.852161E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4989.20 | backward: 19681.48 | optimizer: 11278.71 | batch generator: 2.31 | data loader: 1.27
10.0.2.15:  iteration      465/    1000 | elapsed time per iteration (ms): 36055.3 | learning rate 6.386E-06 | lm loss 6.449047E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4670.22 | backward: 19432.63 | optimizer: 11463.87 | batch generator: 1.84 | data loader: 0.70
10.0.2.15:  iteration      466/    1000 | elapsed time per iteration (ms): 36016.2 | learning rate 6.400E-06 | lm loss 6.460268E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4559.98 | backward: 19439.10 | optimizer: 12015.70 | batch generator: 2.24 | data loader: 0.97
10.0.2.15:  iteration      467/    1000 | elapsed time per iteration (ms): 35452.7 | learning rate 6.414E-06 | lm loss 6.515339E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3924.07 | backward: 19913.34 | optimizer: 11327.59 | batch generator: 1.91 | data loader: 0.74
10.0.2.15:  iteration      468/    1000 | elapsed time per iteration (ms): 35869.8 | learning rate 6.429E-06 | lm loss 6.515475E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4762.38 | backward: 19554.18 | optimizer: 11500.79 | batch generator: 1.92 | data loader: 0.74
10.0.2.15:  iteration      469/    1000 | elapsed time per iteration (ms): 36024.6 | learning rate 6.443E-06 | lm loss 6.796899E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4428.65 | backward: 19984.63 | optimizer: 11312.24 | batch generator: 1.89 | data loader: 0.56
10.0.2.15:  iteration      470/    1000 | elapsed time per iteration (ms): 35001.0 | learning rate 6.457E-06 | lm loss 6.122995E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4668.41 | backward: 19686.96 | optimizer: 10346.71 | batch generator: 2.02 | data loader: 0.88
10.0.2.15:  iteration      471/    1000 | elapsed time per iteration (ms): 36207.9 | learning rate 6.471E-06 | lm loss 6.611712E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5950.08 | backward: 19821.94 | optimizer: 10433.79 | batch generator: 1.67 | data loader: 0.30
10.0.2.15:  iteration      472/    1000 | elapsed time per iteration (ms): 36099.9 | learning rate 6.486E-06 | lm loss 6.743921E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5328.70 | backward: 19808.44 | optimizer: 10672.44 | batch generator: 1.69 | data loader: 0.37
10.0.2.15:  iteration      473/    1000 | elapsed time per iteration (ms): 35842.8 | learning rate 6.500E-06 | lm loss 5.948783E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5280.23 | backward: 20205.45 | optimizer: 10137.62 | batch generator: 1.76 | data loader: 0.37
10.0.2.15:  iteration      474/    1000 | elapsed time per iteration (ms): 36782.9 | learning rate 6.514E-06 | lm loss 6.723947E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6000.30 | backward: 19967.21 | optimizer: 10656.56 | batch generator: 2.28 | data loader: 0.93
10.0.2.15:  iteration      475/    1000 | elapsed time per iteration (ms): 37206.7 | learning rate 6.529E-06 | lm loss 6.153659E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5670.57 | backward: 20278.56 | optimizer: 11255.97 | batch generator: 2.51 | data loader: 1.13
10.0.2.15:  iteration      476/    1000 | elapsed time per iteration (ms): 36295.5 | learning rate 6.543E-06 | lm loss 6.164811E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5178.19 | backward: 20009.59 | optimizer: 11106.27 | batch generator: 2.34 | data loader: 0.96
10.0.2.15:  iteration      477/    1000 | elapsed time per iteration (ms): 35861.0 | learning rate 6.557E-06 | lm loss 6.351273E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4696.95 | backward: 19375.12 | optimizer: 11542.97 | batch generator: 1.93 | data loader: 0.76
10.0.2.15:  iteration      478/    1000 | elapsed time per iteration (ms): 36122.5 | learning rate 6.571E-06 | lm loss 6.230942E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4356.43 | backward: 19831.74 | optimizer: 11388.03 | batch generator: 1.60 | data loader: 0.33
10.0.2.15:  iteration      479/    1000 | elapsed time per iteration (ms): 35918.4 | learning rate 6.586E-06 | lm loss 6.484999E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4433.96 | backward: 19177.11 | optimizer: 12000.56 | batch generator: 2.02 | data loader: 0.90
10.0.2.15:  iteration      480/    1000 | elapsed time per iteration (ms): 36056.1 | learning rate 6.600E-06 | lm loss 6.215146E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3702.10 | backward: 19645.04 | optimizer: 12199.01 | batch generator: 1.77 | data loader: 0.72
10.0.2.15:  iteration      481/    1000 | elapsed time per iteration (ms): 35895.3 | learning rate 6.614E-06 | lm loss 5.941539E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4227.38 | backward: 19965.78 | optimizer: 11700.81 | batch generator: 2.18 | data loader: 0.92
10.0.2.15:  iteration      482/    1000 | elapsed time per iteration (ms): 33857.5 | learning rate 6.629E-06 | lm loss 6.683876E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4348.13 | backward: 19325.54 | optimizer: 10128.74 | batch generator: 2.07 | data loader: 1.00
10.0.2.15:  iteration      483/    1000 | elapsed time per iteration (ms): 37234.2 | learning rate 6.643E-06 | lm loss 6.270069E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5766.53 | backward: 19331.20 | optimizer: 11582.81 | batch generator: 1.96 | data loader: 0.96
10.0.2.15:  iteration      484/    1000 | elapsed time per iteration (ms): 34097.2 | learning rate 6.657E-06 | lm loss 6.565121E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4114.40 | backward: 19415.75 | optimizer: 10082.64 | batch generator: 1.85 | data loader: 0.78
10.0.2.15:  iteration      485/    1000 | elapsed time per iteration (ms): 36521.7 | learning rate 6.671E-06 | lm loss 6.134612E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5884.89 | backward: 19528.06 | optimizer: 10763.16 | batch generator: 2.36 | data loader: 1.01
10.0.2.15:  iteration      486/    1000 | elapsed time per iteration (ms): 36368.1 | learning rate 6.686E-06 | lm loss 6.042887E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5170.50 | backward: 19624.17 | optimizer: 11226.16 | batch generator: 1.53 | data loader: 0.48
10.0.2.15:  iteration      487/    1000 | elapsed time per iteration (ms): 35942.5 | learning rate 6.700E-06 | lm loss 6.675307E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4635.78 | backward: 20290.28 | optimizer: 10579.87 | batch generator: 1.81 | data loader: 0.82
10.0.2.15:  iteration      488/    1000 | elapsed time per iteration (ms): 36178.3 | learning rate 6.714E-06 | lm loss 6.538153E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5387.68 | backward: 19766.63 | optimizer: 10889.93 | batch generator: 2.21 | data loader: 1.05
10.0.2.15:  iteration      489/    1000 | elapsed time per iteration (ms): 36153.3 | learning rate 6.729E-06 | lm loss 7.072051E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5171.85 | backward: 19408.82 | optimizer: 11571.06 | batch generator: 1.46 | data loader: 0.31
10.0.2.15:  iteration      490/    1000 | elapsed time per iteration (ms): 35633.7 | learning rate 6.743E-06 | lm loss 6.406539E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4584.39 | backward: 19523.73 | optimizer: 11199.43 | batch generator: 1.71 | data loader: 0.74
10.0.2.15:  iteration      491/    1000 | elapsed time per iteration (ms): 35305.5 | learning rate 6.757E-06 | lm loss 6.292024E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4855.86 | backward: 19939.42 | optimizer: 10276.52 | batch generator: 1.67 | data loader: 0.47
10.0.2.15:  iteration      492/    1000 | elapsed time per iteration (ms): 36111.9 | learning rate 6.771E-06 | lm loss 6.099833E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5946.86 | backward: 19944.79 | optimizer: 10156.63 | batch generator: 1.58 | data loader: 0.36
10.0.2.15:  iteration      493/    1000 | elapsed time per iteration (ms): 37665.8 | learning rate 6.786E-06 | lm loss 5.960779E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6007.70 | backward: 19778.10 | optimizer: 11704.73 | batch generator: 1.70 | data loader: 0.37
10.0.2.15:  iteration      494/    1000 | elapsed time per iteration (ms): 36723.8 | learning rate 6.800E-06 | lm loss 6.471747E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4733.03 | backward: 19659.01 | optimizer: 12329.91 | batch generator: 1.67 | data loader: 0.37
10.0.2.15:  iteration      495/    1000 | elapsed time per iteration (ms): 36106.6 | learning rate 6.814E-06 | lm loss 6.573386E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3712.19 | backward: 19635.48 | optimizer: 12460.07 | batch generator: 1.73 | data loader: 0.39
10.0.2.15:  iteration      496/    1000 | elapsed time per iteration (ms): 35263.9 | learning rate 6.829E-06 | lm loss 6.236793E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3266.29 | backward: 19359.62 | optimizer: 12295.11 | batch generator: 1.71 | data loader: 0.53
10.0.2.15:  iteration      497/    1000 | elapsed time per iteration (ms): 35668.2 | learning rate 6.843E-06 | lm loss 6.376553E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4041.18 | backward: 19987.93 | optimizer: 11637.11 | batch generator: 1.96 | data loader: 0.57
10.0.2.15:  iteration      498/    1000 | elapsed time per iteration (ms): 33813.2 | learning rate 6.857E-06 | lm loss 6.931535E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4155.00 | backward: 19066.19 | optimizer: 10382.30 | batch generator: 1.33 | data loader: 0.26
10.0.2.15:  iteration      499/    1000 | elapsed time per iteration (ms): 37209.4 | learning rate 6.871E-06 | lm loss 6.162817E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5973.27 | backward: 19864.81 | optimizer: 11169.69 | batch generator: 2.20 | data loader: 0.89
10.0.2.15: [2021-12-31 23:46:59,891] [INFO] [logging.py:60:log_dist] [Rank 0] step=500, skipped=18, lr=[6.871428571428572e-06, 6.871428571428572e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2021-12-31 23:47:12,242] [INFO] [timer.py:154:stop] 0/500, SamplesPerSec=2.263560190523548
10.0.2.15:  iteration      500/    1000 | elapsed time per iteration (ms): 36882.7 | learning rate 6.886E-06 | lm loss 6.224488E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4604.69 | backward: 19497.74 | optimizer: 12400.52 | batch generator: 1.39 | data loader: 0.35
10.0.2.15:  iteration      501/    1000 | elapsed time per iteration (ms): 36118.3 | learning rate 6.900E-06 | lm loss 6.216648E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3995.37 | backward: 19786.52 | optimizer: 12334.65 | batch generator: 1.67 | data loader: 0.36
10.0.2.15:  iteration      502/    1000 | elapsed time per iteration (ms): 34180.8 | learning rate 6.914E-06 | lm loss 6.621153E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3619.24 | backward: 19576.77 | optimizer: 10971.84 | batch generator: 1.58 | data loader: 0.48
10.0.2.15:  iteration      503/    1000 | elapsed time per iteration (ms): 36147.5 | learning rate 6.929E-06 | lm loss 5.772295E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5051.65 | backward: 19665.66 | optimizer: 11428.70 | batch generator: 1.47 | data loader: 0.29
10.0.2.15:  iteration      504/    1000 | elapsed time per iteration (ms): 36992.0 | learning rate 6.943E-06 | lm loss 6.389521E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4710.04 | backward: 19773.55 | optimizer: 12488.00 | batch generator: 1.51 | data loader: 0.35
10.0.2.15:  iteration      505/    1000 | elapsed time per iteration (ms): 35836.4 | learning rate 6.957E-06 | lm loss 6.621923E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3291.73 | backward: 19453.91 | optimizer: 12370.88 | batch generator: 2.00 | data loader: 0.93
10.0.2.15:  iteration      506/    1000 | elapsed time per iteration (ms): 35130.2 | learning rate 6.971E-06 | lm loss 6.443696E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3611.14 | backward: 19241.95 | optimizer: 12027.36 | batch generator: 2.30 | data loader: 1.05
10.0.2.15:  iteration      507/    1000 | elapsed time per iteration (ms): 35981.3 | learning rate 6.986E-06 | lm loss 6.162970E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4357.88 | backward: 19724.79 | optimizer: 11876.00 | batch generator: 1.85 | data loader: 0.70
10.0.2.15:  iteration      508/    1000 | elapsed time per iteration (ms): 34182.4 | learning rate 7.000E-06 | lm loss 6.566675E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4471.31 | backward: 20077.02 | optimizer: 9632.87 | batch generator: 1.97 | data loader: 0.96
10.0.2.15:  iteration      509/    1000 | elapsed time per iteration (ms): 37250.4 | learning rate 7.014E-06 | lm loss 6.877100E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6261.79 | backward: 19433.02 | optimizer: 11215.30 | batch generator: 1.99 | data loader: 0.86
10.0.2.15:  iteration      510/    1000 | elapsed time per iteration (ms): 36782.0 | learning rate 7.029E-06 | lm loss 6.711572E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4819.50 | backward: 19421.56 | optimizer: 12539.08 | batch generator: 1.65 | data loader: 0.54
10.0.2.15:  iteration      511/    1000 | elapsed time per iteration (ms): 35784.6 | learning rate 7.043E-06 | lm loss 6.231338E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3265.68 | backward: 19713.53 | optimizer: 12371.75 | batch generator: 1.82 | data loader: 0.71
10.0.2.15:  iteration      512/    1000 | elapsed time per iteration (ms): 34596.4 | learning rate 7.057E-06 | lm loss 6.230471E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3725.31 | backward: 19869.12 | optimizer: 10724.97 | batch generator: 1.88 | data loader: 0.73
10.0.2.15:  iteration      513/    1000 | elapsed time per iteration (ms): 36683.2 | learning rate 7.071E-06 | lm loss 6.415257E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5172.97 | backward: 19902.98 | optimizer: 11280.07 | batch generator: 2.01 | data loader: 0.80
10.0.2.15:  iteration      514/    1000 | elapsed time per iteration (ms): 35873.6 | learning rate 7.086E-06 | lm loss 6.145284E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4778.79 | backward: 19527.01 | optimizer: 11553.09 | batch generator: 2.02 | data loader: 1.02
10.0.2.15:  iteration      515/    1000 | elapsed time per iteration (ms): 36243.4 | learning rate 7.100E-06 | lm loss 6.628908E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4173.98 | backward: 19399.25 | optimizer: 12177.52 | batch generator: 1.95 | data loader: 0.99
10.0.2.15:  iteration      516/    1000 | elapsed time per iteration (ms): 36341.7 | learning rate 7.114E-06 | lm loss 6.150969E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4280.61 | backward: 19688.29 | optimizer: 12371.59 | batch generator: 1.83 | data loader: 0.71
10.0.2.15:  iteration      517/    1000 | elapsed time per iteration (ms): 35465.7 | learning rate 7.129E-06 | lm loss 6.000209E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3708.60 | backward: 19837.94 | optimizer: 11671.77 | batch generator: 1.91 | data loader: 0.54
10.0.2.15:  iteration      518/    1000 | elapsed time per iteration (ms): 34269.9 | learning rate 7.143E-06 | lm loss 6.542154E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4320.71 | backward: 19697.77 | optimizer: 10250.11 | batch generator: 1.31 | data loader: 0.28
10.0.2.15:  iteration      519/    1000 | elapsed time per iteration (ms): 37347.6 | learning rate 7.157E-06 | lm loss 6.259568E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5723.38 | backward: 19555.08 | optimizer: 11713.09 | batch generator: 1.33 | data loader: 0.28
10.0.2.15:  iteration      520/    1000 | elapsed time per iteration (ms): 33611.1 | learning rate 7.171E-06 | lm loss 6.417747E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4117.30 | backward: 19400.93 | optimizer: 9573.60 | batch generator: 1.52 | data loader: 0.45
10.0.2.15:  iteration      521/    1000 | elapsed time per iteration (ms): 37310.2 | learning rate 7.186E-06 | lm loss 6.355445E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6213.46 | backward: 19516.77 | optimizer: 11236.79 | batch generator: 1.46 | data loader: 0.35
10.0.2.15:  iteration      522/    1000 | elapsed time per iteration (ms): 36996.3 | learning rate 7.200E-06 | lm loss 6.073518E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5002.52 | backward: 19733.00 | optimizer: 12259.04 | batch generator: 1.37 | data loader: 0.28
10.0.2.15:  iteration      523/    1000 | elapsed time per iteration (ms): 35953.2 | learning rate 7.214E-06 | lm loss 6.286810E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3476.00 | backward: 19396.69 | optimizer: 12530.22 | batch generator: 1.94 | data loader: 0.87
10.0.2.15:  iteration      524/    1000 | elapsed time per iteration (ms): 33830.0 | learning rate 7.229E-06 | lm loss 6.591522E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3379.63 | backward: 19281.66 | optimizer: 10678.59 | batch generator: 2.03 | data loader: 0.88
10.0.2.15:  iteration      525/    1000 | elapsed time per iteration (ms): 37055.8 | learning rate 7.243E-06 | lm loss 5.735662E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5462.87 | backward: 19716.44 | optimizer: 11540.54 | batch generator: 1.63 | data loader: 0.61
10.0.2.15:  iteration      526/    1000 | elapsed time per iteration (ms): 36622.8 | learning rate 7.257E-06 | lm loss 5.899205E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4693.59 | backward: 19877.22 | optimizer: 11993.40 | batch generator: 1.72 | data loader: 0.73
10.0.2.15:  iteration      527/    1000 | elapsed time per iteration (ms): 35682.7 | learning rate 7.271E-06 | lm loss 6.859075E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4302.28 | backward: 19402.68 | optimizer: 11976.51 | batch generator: 1.86 | data loader: 0.72
10.0.2.15:  iteration      528/    1000 | elapsed time per iteration (ms): 35929.1 | learning rate 7.286E-06 | lm loss 6.688816E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3975.31 | backward: 19796.40 | optimizer: 12116.22 | batch generator: 1.53 | data loader: 0.35
10.0.2.15:  iteration      529/    1000 | elapsed time per iteration (ms): 35596.7 | learning rate 7.300E-06 | lm loss 6.111145E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3832.79 | backward: 19240.80 | optimizer: 12439.83 | batch generator: 1.34 | data loader: 0.37
10.0.2.15:  iteration      530/    1000 | elapsed time per iteration (ms): 34155.0 | learning rate 7.314E-06 | lm loss 6.398811E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3656.77 | backward: 19090.35 | optimizer: 11351.42 | batch generator: 1.74 | data loader: 0.61
10.0.2.15:  iteration      531/    1000 | elapsed time per iteration (ms): 34693.3 | learning rate 7.329E-06 | lm loss 6.431309E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4746.94 | backward: 19495.75 | optimizer: 10347.37 | batch generator: 1.47 | data loader: 0.44
10.0.2.15:  iteration      532/    1000 | elapsed time per iteration (ms): 36389.2 | learning rate 7.343E-06 | lm loss 6.624930E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5481.33 | backward: 19259.63 | optimizer: 11332.33 | batch generator: 1.37 | data loader: 0.31
10.0.2.15:  iteration      533/    1000 | elapsed time per iteration (ms): 37235.9 | learning rate 7.357E-06 | lm loss 5.977526E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5088.84 | backward: 19859.19 | optimizer: 12286.20 | batch generator: 1.51 | data loader: 0.47
10.0.2.15:  iteration      534/    1000 | elapsed time per iteration (ms): 36012.0 | learning rate 7.371E-06 | lm loss 6.045801E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3792.65 | backward: 19599.77 | optimizer: 12457.46 | batch generator: 1.58 | data loader: 0.46
10.0.2.15:  iteration      535/    1000 | elapsed time per iteration (ms): 34824.6 | learning rate 7.386E-06 | lm loss 6.205916E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3518.84 | backward: 19524.82 | optimizer: 11731.02 | batch generator: 1.70 | data loader: 0.37
10.0.2.15:  iteration      536/    1000 | elapsed time per iteration (ms): 35859.6 | learning rate 7.400E-06 | lm loss 6.665483E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4211.47 | backward: 19399.63 | optimizer: 12210.15 | batch generator: 1.75 | data loader: 0.68
10.0.2.15:  iteration      537/    1000 | elapsed time per iteration (ms): 34951.3 | learning rate 7.414E-06 | lm loss 5.963137E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3825.97 | backward: 19340.43 | optimizer: 11508.01 | batch generator: 1.38 | data loader: 0.37
10.0.2.15:  iteration      538/    1000 | elapsed time per iteration (ms): 35668.3 | learning rate 7.429E-06 | lm loss 5.990288E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4380.04 | backward: 19323.22 | optimizer: 11470.39 | batch generator: 1.40 | data loader: 0.27
10.0.2.15:  iteration      539/    1000 | elapsed time per iteration (ms): 35612.7 | learning rate 7.443E-06 | lm loss 5.857893E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4488.35 | backward: 18837.23 | optimizer: 11950.66 | batch generator: 1.62 | data loader: 0.51
10.0.2.15:  iteration      540/    1000 | elapsed time per iteration (ms): 35694.3 | learning rate 7.457E-06 | lm loss 6.098078E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4119.45 | backward: 19293.08 | optimizer: 12280.16 | batch generator: 1.37 | data loader: 0.38
10.0.2.15:  iteration      541/    1000 | elapsed time per iteration (ms): 35069.2 | learning rate 7.471E-06 | lm loss 5.986491E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3911.97 | backward: 19443.89 | optimizer: 11452.79 | batch generator: 1.81 | data loader: 0.80
10.0.2.15:  iteration      542/    1000 | elapsed time per iteration (ms): 35424.0 | learning rate 7.486E-06 | lm loss 6.416125E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4492.23 | backward: 19397.06 | optimizer: 11338.15 | batch generator: 1.55 | data loader: 0.44
10.0.2.15:  iteration      543/    1000 | elapsed time per iteration (ms): 35839.3 | learning rate 7.500E-06 | lm loss 5.995939E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4668.14 | backward: 19595.41 | optimizer: 11472.48 | batch generator: 1.57 | data loader: 0.39
10.0.2.15:  iteration      544/    1000 | elapsed time per iteration (ms): 35386.2 | learning rate 7.514E-06 | lm loss 5.929933E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4511.82 | backward: 19344.19 | optimizer: 11273.92 | batch generator: 1.89 | data loader: 0.86
10.0.2.15:  iteration      545/    1000 | elapsed time per iteration (ms): 37170.2 | learning rate 7.529E-06 | lm loss 5.901798E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5152.34 | backward: 20300.02 | optimizer: 11716.56 | batch generator: 2.01 | data loader: 0.83
10.0.2.15:  iteration      546/    1000 | elapsed time per iteration (ms): 35909.4 | learning rate 7.543E-06 | lm loss 5.735389E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4687.54 | backward: 19745.04 | optimizer: 11475.39 | batch generator: 2.24 | data loader: 1.03
10.0.2.15:  iteration      547/    1000 | elapsed time per iteration (ms): 34436.6 | learning rate 7.557E-06 | lm loss 5.734865E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4975.06 | backward: 19188.14 | optimizer: 10271.92 | batch generator: 2.14 | data loader: 0.98
10.0.2.15:  iteration      548/    1000 | elapsed time per iteration (ms): 36674.8 | learning rate 7.571E-06 | lm loss 6.218449E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5829.69 | backward: 19243.84 | optimizer: 11214.37 | batch generator: 2.31 | data loader: 1.24
10.0.2.15:  iteration      549/    1000 | elapsed time per iteration (ms): 37120.7 | learning rate 7.586E-06 | lm loss 6.056967E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4898.94 | backward: 19925.28 | optimizer: 12114.22 | batch generator: 2.30 | data loader: 1.05
10.0.2.15: [2022-01-01 00:16:50,346] [INFO] [logging.py:60:log_dist] [Rank 0] step=550, skipped=18, lr=[7.585714285714286e-06, 7.585714285714286e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2022-01-01 00:17:02,192] [INFO] [timer.py:154:stop] 0/550, SamplesPerSec=2.2708963003241234
10.0.2.15:  iteration      550/    1000 | elapsed time per iteration (ms): 36027.2 | learning rate 7.600E-06 | lm loss 6.074389E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3759.95 | backward: 20033.63 | optimizer: 11900.12 | batch generator: 2.07 | data loader: 0.97
10.0.2.15:  iteration      551/    1000 | elapsed time per iteration (ms): 34212.7 | learning rate 7.614E-06 | lm loss 6.180850E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4063.91 | backward: 19106.03 | optimizer: 11038.27 | batch generator: 1.93 | data loader: 0.75
10.0.2.15:  iteration      552/    1000 | elapsed time per iteration (ms): 36230.6 | learning rate 7.629E-06 | lm loss 6.700239E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4743.57 | backward: 19304.34 | optimizer: 11992.00 | batch generator: 2.41 | data loader: 1.20
10.0.2.15:  iteration      553/    1000 | elapsed time per iteration (ms): 34647.2 | learning rate 7.643E-06 | lm loss 6.202297E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3971.13 | backward: 19093.70 | optimizer: 11414.80 | batch generator: 2.35 | data loader: 1.14
10.0.2.15:  iteration      554/    1000 | elapsed time per iteration (ms): 36496.3 | learning rate 7.657E-06 | lm loss 6.280543E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4652.01 | backward: 19470.24 | optimizer: 11938.32 | batch generator: 2.24 | data loader: 1.00
10.0.2.15:  iteration      555/    1000 | elapsed time per iteration (ms): 35050.4 | learning rate 7.671E-06 | lm loss 6.185290E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4455.07 | backward: 19787.27 | optimizer: 10806.81 | batch generator: 2.35 | data loader: 1.15
10.0.2.15:  iteration      556/    1000 | elapsed time per iteration (ms): 37063.2 | learning rate 7.686E-06 | lm loss 6.295148E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5277.11 | backward: 19645.45 | optimizer: 11944.44 | batch generator: 2.43 | data loader: 1.22
10.0.2.15:  iteration      557/    1000 | elapsed time per iteration (ms): 33404.5 | learning rate 7.700E-06 | lm loss 6.353943E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4058.46 | backward: 19155.10 | optimizer: 9867.49 | batch generator: 1.97 | data loader: 0.83
10.0.2.15:  iteration      558/    1000 | elapsed time per iteration (ms): 37586.7 | learning rate 7.714E-06 | lm loss 6.057908E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6293.09 | backward: 19428.02 | optimizer: 11483.05 | batch generator: 2.43 | data loader: 1.18
10.0.2.15:  iteration      559/    1000 | elapsed time per iteration (ms): 37209.3 | learning rate 7.729E-06 | lm loss 6.244276E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4965.14 | backward: 19986.99 | optimizer: 12255.65 | batch generator: 2.60 | data loader: 1.24
10.0.2.15:  iteration      560/    1000 | elapsed time per iteration (ms): 33543.9 | learning rate 7.743E-06 | lm loss 5.855261E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3688.43 | backward: 19357.06 | optimizer: 10178.87 | batch generator: 2.13 | data loader: 1.02
10.0.2.15:  iteration      561/    1000 | elapsed time per iteration (ms): 36943.7 | learning rate 7.757E-06 | lm loss 5.994125E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5753.21 | backward: 19649.36 | optimizer: 11251.70 | batch generator: 2.17 | data loader: 0.87
10.0.2.15:  iteration      562/    1000 | elapsed time per iteration (ms): 36663.3 | learning rate 7.771E-06 | lm loss 6.583388E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4511.80 | backward: 19492.32 | optimizer: 12233.82 | batch generator: 1.95 | data loader: 0.85
10.0.2.15:  iteration      563/    1000 | elapsed time per iteration (ms): 36385.1 | learning rate 7.786E-06 | lm loss 6.333504E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3630.43 | backward: 19977.54 | optimizer: 12468.48 | batch generator: 2.26 | data loader: 0.97
10.0.2.15:  iteration      564/    1000 | elapsed time per iteration (ms): 35176.2 | learning rate 7.800E-06 | lm loss 6.049121E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3321.63 | backward: 19764.32 | optimizer: 11860.25 | batch generator: 1.92 | data loader: 0.79
10.0.2.15:  iteration      565/    1000 | elapsed time per iteration (ms): 33882.8 | learning rate 7.814E-06 | lm loss 6.553138E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4199.19 | backward: 19787.73 | optimizer: 9849.54 | batch generator: 1.92 | data loader: 0.75
10.0.2.15:  iteration      566/    1000 | elapsed time per iteration (ms): 37365.7 | learning rate 7.829E-06 | lm loss 6.257072E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6145.00 | backward: 19966.86 | optimizer: 11053.93 | batch generator: 2.17 | data loader: 1.06
10.0.2.15:  iteration      567/    1000 | elapsed time per iteration (ms): 35472.2 | learning rate 7.843E-06 | lm loss 6.288997E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5052.04 | backward: 19651.77 | optimizer: 10521.45 | batch generator: 2.13 | data loader: 1.13
10.0.2.15:  iteration      568/    1000 | elapsed time per iteration (ms): 37479.4 | learning rate 7.857E-06 | lm loss 6.380889E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5491.38 | backward: 19683.61 | optimizer: 11952.59 | batch generator: 1.97 | data loader: 0.79
10.0.2.15:  iteration      569/    1000 | elapsed time per iteration (ms): 36002.9 | learning rate 7.871E-06 | lm loss 6.316432E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3824.50 | backward: 19460.18 | optimizer: 12150.65 | batch generator: 2.48 | data loader: 1.17
10.0.2.15:  iteration      570/    1000 | elapsed time per iteration (ms): 34586.9 | learning rate 7.886E-06 | lm loss 6.065314E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3790.09 | backward: 19300.64 | optimizer: 11159.78 | batch generator: 2.05 | data loader: 0.96
10.0.2.15:  iteration      571/    1000 | elapsed time per iteration (ms): 35545.6 | learning rate 7.900E-06 | lm loss 6.333862E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4891.68 | backward: 19841.14 | optimizer: 10452.99 | batch generator: 1.98 | data loader: 0.80
10.0.2.15:  iteration      572/    1000 | elapsed time per iteration (ms): 37442.9 | learning rate 7.914E-06 | lm loss 6.299673E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5961.03 | backward: 19751.95 | optimizer: 11728.68 | batch generator: 1.85 | data loader: 0.70
10.0.2.15:  iteration      573/    1000 | elapsed time per iteration (ms): 35091.7 | learning rate 7.929E-06 | lm loss 6.423213E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4259.50 | backward: 19643.69 | optimizer: 10879.44 | batch generator: 2.41 | data loader: 1.12
10.0.2.15:  iteration      574/    1000 | elapsed time per iteration (ms): 36799.1 | learning rate 7.943E-06 | lm loss 6.343680E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5276.68 | backward: 19705.58 | optimizer: 11815.64 | batch generator: 1.97 | data loader: 0.86
10.0.2.15:  iteration      575/    1000 | elapsed time per iteration (ms): 36424.9 | learning rate 7.957E-06 | lm loss 5.918818E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4323.25 | backward: 19473.26 | optimizer: 12354.16 | batch generator: 2.01 | data loader: 0.87
10.0.2.15:  iteration      576/    1000 | elapsed time per iteration (ms): 35504.0 | learning rate 7.971E-06 | lm loss 5.775305E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4106.20 | backward: 19911.08 | optimizer: 11423.01 | batch generator: 2.20 | data loader: 0.91
10.0.2.15:  iteration      577/    1000 | elapsed time per iteration (ms): 36415.1 | learning rate 7.986E-06 | lm loss 6.795759E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4915.50 | backward: 19515.46 | optimizer: 11982.49 | batch generator: 1.64 | data loader: 0.46
10.0.2.15:  iteration      578/    1000 | elapsed time per iteration (ms): 35474.5 | learning rate 8.000E-06 | lm loss 6.431995E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4019.53 | backward: 19543.70 | optimizer: 11910.10 | batch generator: 1.77 | data loader: 0.64
10.0.2.15:  iteration      579/    1000 | elapsed time per iteration (ms): 36030.5 | learning rate 8.014E-06 | lm loss 6.288741E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3846.05 | backward: 19597.72 | optimizer: 12050.78 | batch generator: 1.99 | data loader: 1.01
10.0.2.15:  iteration      580/    1000 | elapsed time per iteration (ms): 35666.8 | learning rate 8.029E-06 | lm loss 6.008470E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3759.79 | backward: 19476.07 | optimizer: 11925.30 | batch generator: 1.88 | data loader: 0.69
10.0.2.15:  iteration      581/    1000 | elapsed time per iteration (ms): 36187.8 | learning rate 8.043E-06 | lm loss 6.520610E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4201.62 | backward: 20091.45 | optimizer: 11820.10 | batch generator: 2.01 | data loader: 0.89
10.0.2.15:  iteration      582/    1000 | elapsed time per iteration (ms): 35240.7 | learning rate 8.057E-06 | lm loss 6.490536E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4128.05 | backward: 19062.61 | optimizer: 12048.91 | batch generator: 1.82 | data loader: 0.68
10.0.2.15:  iteration      583/    1000 | elapsed time per iteration (ms): 33913.1 | learning rate 8.071E-06 | lm loss 6.259084E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4049.35 | backward: 19284.27 | optimizer: 10578.12 | batch generator: 1.73 | data loader: 0.68
10.0.2.15:  iteration      584/    1000 | elapsed time per iteration (ms): 35998.3 | learning rate 8.086E-06 | lm loss 6.904199E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5668.71 | backward: 19805.20 | optimizer: 10523.15 | batch generator: 1.90 | data loader: 0.74
10.0.2.15:  iteration      585/    1000 | elapsed time per iteration (ms): 35486.3 | learning rate 8.100E-06 | lm loss 6.404468E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5510.61 | backward: 19702.97 | optimizer: 9995.38 | batch generator: 1.31 | data loader: 0.28
10.0.2.15:  iteration      586/    1000 | elapsed time per iteration (ms): 37117.9 | learning rate 8.114E-06 | lm loss 6.324199E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5894.46 | backward: 19704.87 | optimizer: 11125.73 | batch generator: 1.53 | data loader: 0.34
10.0.2.15:  iteration      587/    1000 | elapsed time per iteration (ms): 36891.7 | learning rate 8.129E-06 | lm loss 6.409142E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4802.08 | backward: 20317.11 | optimizer: 11451.91 | batch generator: 1.96 | data loader: 0.88
10.0.2.15:  iteration      588/    1000 | elapsed time per iteration (ms): 36767.2 | learning rate 8.143E-06 | lm loss 6.110406E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4610.91 | backward: 19839.67 | optimizer: 12135.16 | batch generator: 2.03 | data loader: 0.92
10.0.2.15:  iteration      589/    1000 | elapsed time per iteration (ms): 34262.0 | learning rate 8.157E-06 | lm loss 6.863946E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3636.07 | backward: 19406.31 | optimizer: 10920.35 | batch generator: 1.40 | data loader: 0.28
10.0.2.15:  iteration      590/    1000 | elapsed time per iteration (ms): 35630.7 | learning rate 8.171E-06 | lm loss 5.859954E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5023.05 | backward: 19160.58 | optimizer: 11155.73 | batch generator: 2.26 | data loader: 0.94
10.0.2.15:  iteration      591/    1000 | elapsed time per iteration (ms): 34264.3 | learning rate 8.186E-06 | lm loss 5.740166E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4894.63 | backward: 19258.83 | optimizer: 10019.57 | batch generator: 2.49 | data loader: 1.05
10.0.2.15:  iteration      592/    1000 | elapsed time per iteration (ms): 38235.2 | learning rate 8.200E-06 | lm loss 5.588913E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6037.53 | backward: 19647.14 | optimizer: 12270.30 | batch generator: 1.55 | data loader: 0.31
10.0.2.15:  iteration      593/    1000 | elapsed time per iteration (ms): 36166.1 | learning rate 8.214E-06 | lm loss 6.642139E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3426.00 | backward: 19685.38 | optimizer: 12552.71 | batch generator: 2.17 | data loader: 0.90
10.0.2.15:  iteration      594/    1000 | elapsed time per iteration (ms): 35078.1 | learning rate 8.229E-06 | lm loss 6.930106E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3313.29 | backward: 19366.68 | optimizer: 12307.73 | batch generator: 1.42 | data loader: 0.29
10.0.2.15:  iteration      595/    1000 | elapsed time per iteration (ms): 36370.4 | learning rate 8.243E-06 | lm loss 5.992958E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4003.61 | backward: 19834.54 | optimizer: 12531.02 | batch generator: 1.93 | data loader: 0.75
10.0.2.15:  iteration      596/    1000 | elapsed time per iteration (ms): 35250.5 | learning rate 8.257E-06 | lm loss 6.239620E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3848.67 | backward: 20019.07 | optimizer: 11381.38 | batch generator: 2.40 | data loader: 1.11
10.0.2.15:  iteration      597/    1000 | elapsed time per iteration (ms): 35343.1 | learning rate 8.271E-06 | lm loss 6.301466E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4628.60 | backward: 19115.04 | optimizer: 11390.42 | batch generator: 1.85 | data loader: 0.81
10.0.2.15:  iteration      598/    1000 | elapsed time per iteration (ms): 36025.2 | learning rate 8.286E-06 | lm loss 6.177260E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4866.15 | backward: 19706.19 | optimizer: 11427.93 | batch generator: 1.87 | data loader: 0.72
10.0.2.15:  iteration      599/    1000 | elapsed time per iteration (ms): 35668.4 | learning rate 8.300E-06 | lm loss 6.275673E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4622.08 | backward: 19583.98 | optimizer: 11460.49 | batch generator: 1.61 | data loader: 0.36
10.0.2.15: [2022-01-01 00:46:41,719] [INFO] [logging.py:60:log_dist] [Rank 0] step=600, skipped=18, lr=[8.3e-06, 8.3e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2022-01-01 00:46:53,111] [INFO] [timer.py:154:stop] 0/600, SamplesPerSec=2.2769430170365776
10.0.2.15:  iteration      600/    1000 | elapsed time per iteration (ms): 35223.2 | learning rate 8.314E-06 | lm loss 5.944051E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4395.82 | backward: 19054.03 | optimizer: 11582.77 | batch generator: 1.48 | data loader: 0.30
10.0.2.15:  iteration      601/    1000 | elapsed time per iteration (ms): 36292.3 | learning rate 8.329E-06 | lm loss 6.277211E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4938.75 | backward: 19768.06 | optimizer: 11584.32 | batch generator: 1.78 | data loader: 0.66
10.0.2.15:  iteration      602/    1000 | elapsed time per iteration (ms): 36718.0 | learning rate 8.343E-06 | lm loss 6.044260E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4711.79 | backward: 19960.24 | optimizer: 11936.23 | batch generator: 1.70 | data loader: 0.64
10.0.2.15:  iteration      603/    1000 | elapsed time per iteration (ms): 34436.4 | learning rate 8.357E-06 | lm loss 6.274610E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4047.64 | backward: 19771.06 | optimizer: 10616.13 | batch generator: 1.47 | data loader: 0.27
10.0.2.15:  iteration      604/    1000 | elapsed time per iteration (ms): 36623.7 | learning rate 8.371E-06 | lm loss 6.031286E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5613.55 | backward: 19339.00 | optimizer: 11634.28 | batch generator: 1.61 | data loader: 0.34
10.0.2.15:  iteration      605/    1000 | elapsed time per iteration (ms): 34804.6 | learning rate 8.386E-06 | lm loss 6.362073E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4448.88 | backward: 19556.49 | optimizer: 10774.86 | batch generator: 1.85 | data loader: 0.69
10.0.2.15:  iteration      606/    1000 | elapsed time per iteration (ms): 37264.7 | learning rate 8.400E-06 | lm loss 6.273984E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5465.30 | backward: 19898.82 | optimizer: 11812.98 | batch generator: 1.35 | data loader: 0.27
10.0.2.15:  iteration      607/    1000 | elapsed time per iteration (ms): 36136.1 | learning rate 8.414E-06 | lm loss 6.101997E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4181.34 | backward: 19814.22 | optimizer: 12138.80 | batch generator: 1.56 | data loader: 0.46
10.0.2.15:  iteration      608/    1000 | elapsed time per iteration (ms): 34181.2 | learning rate 8.429E-06 | lm loss 6.324297E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3694.42 | backward: 19195.71 | optimizer: 10850.48 | batch generator: 2.13 | data loader: 0.88
10.0.2.15:  iteration      609/    1000 | elapsed time per iteration (ms): 35440.9 | learning rate 8.443E-06 | lm loss 6.628362E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5367.03 | backward: 19513.43 | optimizer: 10376.80 | batch generator: 2.12 | data loader: 0.92
10.0.2.15:  iteration      610/    1000 | elapsed time per iteration (ms): 38119.2 | learning rate 8.457E-06 | lm loss 6.282230E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5899.06 | backward: 20071.10 | optimizer: 12012.21 | batch generator: 1.80 | data loader: 0.70
10.0.2.15:  iteration      611/    1000 | elapsed time per iteration (ms): 36320.7 | learning rate 8.471E-06 | lm loss 5.988014E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3733.65 | backward: 19790.73 | optimizer: 12280.68 | batch generator: 2.02 | data loader: 1.02
10.0.2.15:  iteration      612/    1000 | elapsed time per iteration (ms): 35701.3 | learning rate 8.486E-06 | lm loss 6.087369E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3489.23 | backward: 19272.02 | optimizer: 12569.18 | batch generator: 2.04 | data loader: 0.89
10.0.2.15:  iteration      613/    1000 | elapsed time per iteration (ms): 35792.0 | learning rate 8.500E-06 | lm loss 6.640359E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3521.54 | backward: 19835.32 | optimizer: 12260.72 | batch generator: 2.10 | data loader: 0.90
10.0.2.15:  iteration      614/    1000 | elapsed time per iteration (ms): 35247.1 | learning rate 8.514E-06 | lm loss 6.020357E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3689.19 | backward: 19423.59 | optimizer: 12068.22 | batch generator: 1.87 | data loader: 0.71
10.0.2.15:  iteration      615/    1000 | elapsed time per iteration (ms): 35329.5 | learning rate 8.529E-06 | lm loss 6.508211E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3931.26 | backward: 19319.45 | optimizer: 12077.57 | batch generator: 1.73 | data loader: 0.66
10.0.2.15:  iteration      616/    1000 | elapsed time per iteration (ms): 35389.2 | learning rate 8.543E-06 | lm loss 6.022280E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3900.38 | backward: 19535.92 | optimizer: 11696.00 | batch generator: 1.73 | data loader: 0.65
10.0.2.15:  iteration      617/    1000 | elapsed time per iteration (ms): 35958.1 | learning rate 8.557E-06 | lm loss 6.721121E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4073.99 | backward: 19255.34 | optimizer: 12352.31 | batch generator: 1.85 | data loader: 0.72
10.0.2.15:  iteration      618/    1000 | elapsed time per iteration (ms): 35741.3 | learning rate 8.571E-06 | lm loss 5.833860E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3740.58 | backward: 19450.55 | optimizer: 12511.07 | batch generator: 1.83 | data loader: 0.69
10.0.2.15:  iteration      619/    1000 | elapsed time per iteration (ms): 35697.9 | learning rate 8.586E-06 | lm loss 6.450564E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3856.00 | backward: 19568.26 | optimizer: 12255.81 | batch generator: 2.14 | data loader: 0.89
10.0.2.15:  iteration      620/    1000 | elapsed time per iteration (ms): 35444.7 | learning rate 8.600E-06 | lm loss 6.606782E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3865.16 | backward: 19596.37 | optimizer: 11880.91 | batch generator: 1.81 | data loader: 0.67
10.0.2.15:  iteration      621/    1000 | elapsed time per iteration (ms): 36247.9 | learning rate 8.614E-06 | lm loss 6.807973E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3863.46 | backward: 19401.80 | optimizer: 12439.29 | batch generator: 1.94 | data loader: 0.96
10.0.2.15:  iteration      622/    1000 | elapsed time per iteration (ms): 34546.3 | learning rate 8.629E-06 | lm loss 5.850680E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3544.24 | backward: 19450.31 | optimizer: 11205.40 | batch generator: 1.95 | data loader: 0.88
10.0.2.15:  iteration      623/    1000 | elapsed time per iteration (ms): 34773.5 | learning rate 8.643E-06 | lm loss 6.489612E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4740.77 | backward: 19638.97 | optimizer: 9980.63 | batch generator: 1.99 | data loader: 0.84
10.0.2.15:  iteration      624/    1000 | elapsed time per iteration (ms): 36472.9 | learning rate 8.657E-06 | lm loss 6.493945E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5780.73 | backward: 19185.23 | optimizer: 10995.29 | batch generator: 1.93 | data loader: 0.88
10.0.2.15:  iteration      625/    1000 | elapsed time per iteration (ms): 35538.4 | learning rate 8.671E-06 | lm loss 6.708447E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4698.46 | backward: 19062.27 | optimizer: 11499.36 | batch generator: 1.59 | data loader: 0.36
10.0.2.15:  iteration      626/    1000 | elapsed time per iteration (ms): 35723.8 | learning rate 8.686E-06 | lm loss 6.210478E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4516.80 | backward: 19555.74 | optimizer: 11369.86 | batch generator: 1.44 | data loader: 0.41
10.0.2.15:  iteration      627/    1000 | elapsed time per iteration (ms): 35881.2 | learning rate 8.700E-06 | lm loss 6.497238E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4580.95 | backward: 19745.07 | optimizer: 11308.03 | batch generator: 1.80 | data loader: 0.82
10.0.2.15:  iteration      628/    1000 | elapsed time per iteration (ms): 36269.9 | learning rate 8.714E-06 | lm loss 5.976259E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4933.22 | backward: 19292.95 | optimizer: 12042.45 | batch generator: 1.84 | data loader: 0.68
10.0.2.15:  iteration      629/    1000 | elapsed time per iteration (ms): 35971.4 | learning rate 8.729E-06 | lm loss 6.319072E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4245.51 | backward: 19591.71 | optimizer: 12132.74 | batch generator: 2.43 | data loader: 1.15
10.0.2.15:  iteration      630/    1000 | elapsed time per iteration (ms): 32948.2 | learning rate 8.743E-06 | lm loss 6.392566E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3943.30 | backward: 19002.52 | optimizer: 10000.73 | batch generator: 1.61 | data loader: 0.47
10.0.2.15:  iteration      631/    1000 | elapsed time per iteration (ms): 37443.1 | learning rate 8.757E-06 | lm loss 6.314846E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5997.26 | backward: 19543.75 | optimizer: 11787.12 | batch generator: 2.02 | data loader: 1.01
10.0.2.15:  iteration      632/    1000 | elapsed time per iteration (ms): 36035.9 | learning rate 8.771E-06 | lm loss 6.562719E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4008.79 | backward: 19438.57 | optimizer: 12103.98 | batch generator: 1.88 | data loader: 0.70
10.0.2.15:  iteration      633/    1000 | elapsed time per iteration (ms): 35812.4 | learning rate 8.786E-06 | lm loss 6.052666E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3858.08 | backward: 19320.09 | optimizer: 12468.32 | batch generator: 2.27 | data loader: 0.94
10.0.2.15:  iteration      634/    1000 | elapsed time per iteration (ms): 36420.7 | learning rate 8.800E-06 | lm loss 6.503021E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4073.36 | backward: 19840.61 | optimizer: 12504.87 | batch generator: 1.63 | data loader: 0.59
10.0.2.15:  iteration      635/    1000 | elapsed time per iteration (ms): 33012.1 | learning rate 8.814E-06 | lm loss 6.331983E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3474.62 | backward: 19582.90 | optimizer: 9691.06 | batch generator: 2.06 | data loader: 0.90
10.0.2.15:  iteration      636/    1000 | elapsed time per iteration (ms): 37209.2 | learning rate 8.829E-06 | lm loss 6.076679E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6685.37 | backward: 19806.30 | optimizer: 10716.18 | batch generator: 2.08 | data loader: 0.87
10.0.2.15:  iteration      637/    1000 | elapsed time per iteration (ms): 35966.2 | learning rate 8.843E-06 | lm loss 6.262889E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5287.36 | backward: 19690.03 | optimizer: 10877.42 | batch generator: 1.58 | data loader: 0.48
10.0.2.15:  iteration      638/    1000 | elapsed time per iteration (ms): 35131.9 | learning rate 8.857E-06 | lm loss 6.110976E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5092.58 | backward: 19832.50 | optimizer: 9975.54 | batch generator: 1.83 | data loader: 0.71
10.0.2.15:  iteration      639/    1000 | elapsed time per iteration (ms): 37159.1 | learning rate 8.871E-06 | lm loss 6.118255E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6243.21 | backward: 19864.83 | optimizer: 11049.94 | batch generator: 1.89 | data loader: 0.72
10.0.2.15:  iteration      640/    1000 | elapsed time per iteration (ms): 35730.0 | learning rate 8.886E-06 | lm loss 6.605332E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4838.75 | backward: 19522.34 | optimizer: 11367.55 | batch generator: 2.26 | data loader: 1.07
10.0.2.15:  iteration      641/    1000 | elapsed time per iteration (ms): 34529.8 | learning rate 8.900E-06 | lm loss 6.816956E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4370.86 | backward: 19255.07 | optimizer: 10352.37 | batch generator: 1.90 | data loader: 0.92
10.0.2.15:  iteration      642/    1000 | elapsed time per iteration (ms): 35022.2 | learning rate 8.914E-06 | lm loss 6.449888E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5567.67 | backward: 19281.95 | optimizer: 9979.58 | batch generator: 1.72 | data loader: 0.74
10.0.2.15:  iteration      643/    1000 | elapsed time per iteration (ms): 36377.0 | learning rate 8.929E-06 | lm loss 6.257082E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6151.74 | backward: 19522.48 | optimizer: 10610.42 | batch generator: 1.43 | data loader: 0.38
10.0.2.15:  iteration      644/    1000 | elapsed time per iteration (ms): 34972.2 | learning rate 8.943E-06 | lm loss 6.688011E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5156.74 | backward: 19370.45 | optimizer: 10178.03 | batch generator: 2.30 | data loader: 1.04
10.0.2.15:  iteration      645/    1000 | elapsed time per iteration (ms): 36901.2 | learning rate 8.957E-06 | lm loss 6.462401E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5820.87 | backward: 19370.42 | optimizer: 11485.56 | batch generator: 2.05 | data loader: 0.89
10.0.2.15:  iteration      646/    1000 | elapsed time per iteration (ms): 36881.7 | learning rate 8.971E-06 | lm loss 5.760674E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4566.65 | backward: 19592.72 | optimizer: 12455.54 | batch generator: 1.98 | data loader: 0.82
10.0.2.15:  iteration      647/    1000 | elapsed time per iteration (ms): 35494.1 | learning rate 8.986E-06 | lm loss 6.335772E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3883.23 | backward: 19221.41 | optimizer: 12388.14 | batch generator: 1.96 | data loader: 0.86
10.0.2.15:  iteration      648/    1000 | elapsed time per iteration (ms): 35699.1 | learning rate 9.000E-06 | lm loss 6.023690E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4019.37 | backward: 19436.80 | optimizer: 12241.60 | batch generator: 2.07 | data loader: 0.89
10.0.2.15:  iteration      649/    1000 | elapsed time per iteration (ms): 34702.1 | learning rate 9.014E-06 | lm loss 6.362473E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4090.02 | backward: 19959.23 | optimizer: 10650.85 | batch generator: 1.91 | data loader: 0.56
10.0.2.15: [2022-01-01 01:16:30,309] [INFO] [logging.py:60:log_dist] [Rank 0] step=650, skipped=18, lr=[9.014285714285715e-06, 9.014285714285715e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2022-01-01 01:16:41,097] [INFO] [timer.py:154:stop] 0/650, SamplesPerSec=2.2823613794513
10.0.2.15:  iteration      650/    1000 | elapsed time per iteration (ms): 36473.8 | learning rate 9.029E-06 | lm loss 6.141745E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5391.13 | backward: 19759.73 | optimizer: 10856.51 | batch generator: 1.80 | data loader: 0.67
10.0.2.15:  iteration      651/    1000 | elapsed time per iteration (ms): 35473.3 | learning rate 9.043E-06 | lm loss 6.475598E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4852.49 | backward: 19352.70 | optimizer: 11000.98 | batch generator: 1.76 | data loader: 0.79
10.0.2.15:  iteration      652/    1000 | elapsed time per iteration (ms): 35922.4 | learning rate 9.057E-06 | lm loss 6.279845E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4887.96 | backward: 19269.91 | optimizer: 11246.76 | batch generator: 1.93 | data loader: 0.87
10.0.2.15:  iteration      653/    1000 | elapsed time per iteration (ms): 36299.1 | learning rate 9.071E-06 | lm loss 6.511943E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4526.51 | backward: 19134.96 | optimizer: 12143.26 | batch generator: 1.78 | data loader: 0.71
10.0.2.15:  iteration      654/    1000 | elapsed time per iteration (ms): 34928.1 | learning rate 9.086E-06 | lm loss 6.357173E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3845.16 | backward: 19851.00 | optimizer: 11144.96 | batch generator: 1.91 | data loader: 0.56
10.0.2.15:  iteration      655/    1000 | elapsed time per iteration (ms): 36505.9 | learning rate 9.100E-06 | lm loss 6.284749E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4657.82 | backward: 19519.36 | optimizer: 11993.57 | batch generator: 1.81 | data loader: 0.84
10.0.2.15:  iteration      656/    1000 | elapsed time per iteration (ms): 35609.5 | learning rate 9.114E-06 | lm loss 6.529607E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3780.19 | backward: 19317.88 | optimizer: 12311.47 | batch generator: 1.96 | data loader: 0.86
10.0.2.15:  iteration      657/    1000 | elapsed time per iteration (ms): 35692.1 | learning rate 9.129E-06 | lm loss 6.182184E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3987.53 | backward: 20062.36 | optimizer: 11641.01 | batch generator: 1.94 | data loader: 0.86
10.0.2.15:  iteration      658/    1000 | elapsed time per iteration (ms): 34369.5 | learning rate 9.143E-06 | lm loss 5.902159E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4318.94 | backward: 19606.49 | optimizer: 10255.91 | batch generator: 1.78 | data loader: 0.70
10.0.2.15:  iteration      659/    1000 | elapsed time per iteration (ms): 36218.4 | learning rate 9.157E-06 | lm loss 6.204688E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5507.62 | backward: 19474.60 | optimizer: 10751.99 | batch generator: 2.31 | data loader: 0.95
10.0.2.15:  iteration      660/    1000 | elapsed time per iteration (ms): 36915.2 | learning rate 9.171E-06 | lm loss 6.032301E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5310.38 | backward: 19509.78 | optimizer: 12035.87 | batch generator: 1.81 | data loader: 0.80
10.0.2.15:  iteration      661/    1000 | elapsed time per iteration (ms): 35642.6 | learning rate 9.186E-06 | lm loss 6.225226E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3664.16 | backward: 19280.97 | optimizer: 12208.02 | batch generator: 1.75 | data loader: 0.66
10.0.2.15:  iteration      662/    1000 | elapsed time per iteration (ms): 35993.7 | learning rate 9.200E-06 | lm loss 6.389894E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3826.05 | backward: 19584.63 | optimizer: 12386.24 | batch generator: 1.87 | data loader: 0.44
10.0.2.15:  iteration      663/    1000 | elapsed time per iteration (ms): 35590.1 | learning rate 9.214E-06 | lm loss 6.415805E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3542.14 | backward: 19282.16 | optimizer: 12429.57 | batch generator: 1.99 | data loader: 0.74
10.0.2.15:  iteration      664/    1000 | elapsed time per iteration (ms): 34538.8 | learning rate 9.229E-06 | lm loss 5.649092E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4009.28 | backward: 19800.13 | optimizer: 10727.16 | batch generator: 1.92 | data loader: 0.47
10.0.2.15:  iteration      665/    1000 | elapsed time per iteration (ms): 35562.1 | learning rate 9.243E-06 | lm loss 6.722349E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4985.41 | backward: 19231.15 | optimizer: 11062.20 | batch generator: 1.59 | data loader: 0.35
10.0.2.15:  iteration      666/    1000 | elapsed time per iteration (ms): 35236.7 | learning rate 9.257E-06 | lm loss 6.003183E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5152.46 | backward: 19820.10 | optimizer: 10205.83 | batch generator: 1.72 | data loader: 0.36
10.0.2.15:  iteration      667/    1000 | elapsed time per iteration (ms): 36997.5 | learning rate 9.271E-06 | lm loss 6.002792E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5858.52 | backward: 19458.98 | optimizer: 11410.93 | batch generator: 2.27 | data loader: 0.47
10.0.2.15:  iteration      668/    1000 | elapsed time per iteration (ms): 35210.3 | learning rate 9.286E-06 | lm loss 5.898618E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4678.24 | backward: 19527.98 | optimizer: 10801.54 | batch generator: 2.45 | data loader: 0.97
10.0.2.15:  iteration      669/    1000 | elapsed time per iteration (ms): 36028.0 | learning rate 9.300E-06 | lm loss 5.943630E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5145.11 | backward: 19477.69 | optimizer: 10981.58 | batch generator: 1.81 | data loader: 0.68
10.0.2.15:  iteration      670/    1000 | elapsed time per iteration (ms): 36263.6 | learning rate 9.314E-06 | lm loss 6.352887E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5067.54 | backward: 19791.68 | optimizer: 11256.11 | batch generator: 1.94 | data loader: 0.59
10.0.2.15:  iteration      671/    1000 | elapsed time per iteration (ms): 35968.3 | learning rate 9.329E-06 | lm loss 6.487123E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4747.67 | backward: 19643.19 | optimizer: 11406.19 | batch generator: 1.48 | data loader: 0.28
10.0.2.15:  iteration      672/    1000 | elapsed time per iteration (ms): 35609.8 | learning rate 9.343E-06 | lm loss 6.262208E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4746.21 | backward: 19359.88 | optimizer: 11501.99 | batch generator: 1.84 | data loader: 0.66
10.0.2.15:  iteration      673/    1000 | elapsed time per iteration (ms): 36386.6 | learning rate 9.357E-06 | lm loss 6.089176E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4608.29 | backward: 19467.19 | optimizer: 11953.71 | batch generator: 2.17 | data loader: 0.89
10.0.2.15:  iteration      674/    1000 | elapsed time per iteration (ms): 35209.3 | learning rate 9.371E-06 | lm loss 6.038806E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3861.95 | backward: 19405.53 | optimizer: 11459.98 | batch generator: 1.98 | data loader: 1.00
10.0.2.15:  iteration      675/    1000 | elapsed time per iteration (ms): 36263.6 | learning rate 9.386E-06 | lm loss 6.632470E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4297.97 | backward: 19475.43 | optimizer: 12205.96 | batch generator: 1.65 | data loader: 0.47
10.0.2.15:  iteration      676/    1000 | elapsed time per iteration (ms): 36100.4 | learning rate 9.400E-06 | lm loss 5.753434E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4144.03 | backward: 19526.90 | optimizer: 12418.70 | batch generator: 1.84 | data loader: 0.55
10.0.2.15:  iteration      677/    1000 | elapsed time per iteration (ms): 36437.8 | learning rate 9.414E-06 | lm loss 6.174066E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4084.02 | backward: 19995.77 | optimizer: 12356.80 | batch generator: 1.87 | data loader: 0.73
10.0.2.15:  iteration      678/    1000 | elapsed time per iteration (ms): 36075.2 | learning rate 9.429E-06 | lm loss 5.668333E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3893.83 | backward: 20032.04 | optimizer: 12139.26 | batch generator: 1.82 | data loader: 0.66
10.0.2.15:  iteration      679/    1000 | elapsed time per iteration (ms): 35531.6 | learning rate 9.443E-06 | lm loss 6.346811E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3970.00 | backward: 19567.23 | optimizer: 11702.79 | batch generator: 1.68 | data loader: 0.36
10.0.2.15:  iteration      680/    1000 | elapsed time per iteration (ms): 35895.0 | learning rate 9.457E-06 | lm loss 6.167695E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4369.34 | backward: 19799.77 | optimizer: 11646.19 | batch generator: 2.51 | data loader: 1.14
10.0.2.15:  iteration      681/    1000 | elapsed time per iteration (ms): 35641.0 | learning rate 9.471E-06 | lm loss 6.145923E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4308.78 | backward: 19613.32 | optimizer: 11636.63 | batch generator: 2.48 | data loader: 1.15
10.0.2.15:  iteration      682/    1000 | elapsed time per iteration (ms): 33692.2 | learning rate 9.486E-06 | lm loss 6.058401E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4384.83 | backward: 19541.22 | optimizer: 9624.87 | batch generator: 1.95 | data loader: 0.75
10.0.2.15:  iteration      683/    1000 | elapsed time per iteration (ms): 37476.4 | learning rate 9.500E-06 | lm loss 6.095752E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6085.32 | backward: 19122.02 | optimizer: 11698.52 | batch generator: 2.20 | data loader: 0.93
10.0.2.15:  iteration      684/    1000 | elapsed time per iteration (ms): 36578.5 | learning rate 9.514E-06 | lm loss 6.441447E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4692.19 | backward: 19468.18 | optimizer: 12416.53 | batch generator: 1.42 | data loader: 0.28
10.0.2.15:  iteration      685/    1000 | elapsed time per iteration (ms): 34224.9 | learning rate 9.529E-06 | lm loss 5.800528E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3539.72 | backward: 19944.19 | optimizer: 10248.60 | batch generator: 2.08 | data loader: 1.03
10.0.2.15:  iteration      686/    1000 | elapsed time per iteration (ms): 36866.6 | learning rate 9.543E-06 | lm loss 6.364452E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5485.53 | backward: 19787.50 | optimizer: 11070.31 | batch generator: 2.38 | data loader: 1.04
10.0.2.15:  iteration      687/    1000 | elapsed time per iteration (ms): 35870.4 | learning rate 9.557E-06 | lm loss 5.819796E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5094.69 | backward: 19415.75 | optimizer: 11358.52 | batch generator: 2.04 | data loader: 0.47
10.0.2.15:  iteration      688/    1000 | elapsed time per iteration (ms): 34745.6 | learning rate 9.571E-06 | lm loss 6.579016E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4656.78 | backward: 19316.90 | optimizer: 10442.31 | batch generator: 1.44 | data loader: 0.29
10.0.2.15:  iteration      689/    1000 | elapsed time per iteration (ms): 36607.2 | learning rate 9.586E-06 | lm loss 6.422896E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5566.08 | backward: 19445.60 | optimizer: 11593.47 | batch generator: 1.93 | data loader: 0.58
10.0.2.15:  iteration      690/    1000 | elapsed time per iteration (ms): 36530.5 | learning rate 9.600E-06 | lm loss 5.867145E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4456.47 | backward: 19664.31 | optimizer: 12256.33 | batch generator: 1.43 | data loader: 0.28
10.0.2.15:  iteration      691/    1000 | elapsed time per iteration (ms): 35209.2 | learning rate 9.614E-06 | lm loss 6.342027E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3661.28 | backward: 19515.08 | optimizer: 11978.87 | batch generator: 1.82 | data loader: 0.75
10.0.2.15:  iteration      692/    1000 | elapsed time per iteration (ms): 35551.5 | learning rate 9.629E-06 | lm loss 5.862161E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3983.86 | backward: 18770.26 | optimizer: 12490.50 | batch generator: 1.67 | data loader: 0.39
10.0.2.15:  iteration      693/    1000 | elapsed time per iteration (ms): 34398.1 | learning rate 9.643E-06 | lm loss 6.578995E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3604.43 | backward: 19143.03 | optimizer: 11474.43 | batch generator: 1.47 | data loader: 0.30
10.0.2.15:  iteration      694/    1000 | elapsed time per iteration (ms): 35720.3 | learning rate 9.657E-06 | lm loss 5.876789E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4603.99 | backward: 19663.67 | optimizer: 11165.75 | batch generator: 1.95 | data loader: 0.80
10.0.2.15:  iteration      695/    1000 | elapsed time per iteration (ms): 36255.4 | learning rate 9.671E-06 | lm loss 6.223126E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5213.17 | backward: 19769.43 | optimizer: 11271.30 | batch generator: 2.32 | data loader: 0.90
10.0.2.15:  iteration      696/    1000 | elapsed time per iteration (ms): 36063.9 | learning rate 9.686E-06 | lm loss 6.101815E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4755.32 | backward: 19740.16 | optimizer: 11565.99 | batch generator: 2.13 | data loader: 0.46
10.0.2.15:  iteration      697/    1000 | elapsed time per iteration (ms): 36092.2 | learning rate 9.700E-06 | lm loss 6.128313E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4589.82 | backward: 19630.44 | optimizer: 11510.82 | batch generator: 1.97 | data loader: 0.41
10.0.2.15:  iteration      698/    1000 | elapsed time per iteration (ms): 36204.5 | learning rate 9.714E-06 | lm loss 6.311320E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4651.78 | backward: 19412.31 | optimizer: 12025.54 | batch generator: 1.89 | data loader: 0.40
10.0.2.15:  iteration      699/    1000 | elapsed time per iteration (ms): 33071.0 | learning rate 9.729E-06 | lm loss 6.672303E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3765.88 | backward: 19383.41 | optimizer: 9676.92 | batch generator: 1.90 | data loader: 0.56
10.0.2.15: [2022-01-01 01:46:18,171] [INFO] [logging.py:60:log_dist] [Rank 0] step=700, skipped=18, lr=[9.72857142857143e-06, 9.72857142857143e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2022-01-01 01:46:28,227] [INFO] [timer.py:154:stop] 0/700, SamplesPerSec=2.2871021840580754
10.0.2.15:  iteration      700/    1000 | elapsed time per iteration (ms): 35855.5 | learning rate 9.743E-06 | lm loss 6.565524E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6031.97 | backward: 19124.41 | optimizer: 10177.41 | batch generator: 1.60 | data loader: 0.47
10.0.2.15:  iteration      701/    1000 | elapsed time per iteration (ms): 37249.8 | learning rate 9.757E-06 | lm loss 5.952817E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5820.16 | backward: 19648.64 | optimizer: 11456.52 | batch generator: 1.61 | data loader: 0.33
10.0.2.15:  iteration      702/    1000 | elapsed time per iteration (ms): 36555.8 | learning rate 9.771E-06 | lm loss 6.073491E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4485.34 | backward: 19484.84 | optimizer: 12513.47 | batch generator: 1.67 | data loader: 0.46
10.0.2.15:  iteration      703/    1000 | elapsed time per iteration (ms): 35266.2 | learning rate 9.786E-06 | lm loss 6.548604E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3490.99 | backward: 19329.56 | optimizer: 12443.52 | batch generator: 1.91 | data loader: 0.45
10.0.2.15:  iteration      704/    1000 | elapsed time per iteration (ms): 35785.6 | learning rate 9.800E-06 | lm loss 6.046618E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3507.67 | backward: 19670.38 | optimizer: 12279.01 | batch generator: 1.73 | data loader: 0.35
10.0.2.15:  iteration      705/    1000 | elapsed time per iteration (ms): 35074.0 | learning rate 9.814E-06 | lm loss 6.303056E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3668.27 | backward: 19334.42 | optimizer: 12061.15 | batch generator: 1.59 | data loader: 0.32
10.0.2.15:  iteration      706/    1000 | elapsed time per iteration (ms): 36189.6 | learning rate 9.829E-06 | lm loss 6.069100E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4342.35 | backward: 19930.61 | optimizer: 11914.83 | batch generator: 1.71 | data loader: 0.33
10.0.2.15:  iteration      707/    1000 | elapsed time per iteration (ms): 35313.6 | learning rate 9.843E-06 | lm loss 5.912611E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4028.75 | backward: 19274.72 | optimizer: 11696.08 | batch generator: 2.12 | data loader: 0.66
10.0.2.15:  iteration      708/    1000 | elapsed time per iteration (ms): 36514.5 | learning rate 9.857E-06 | lm loss 5.761771E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4494.26 | backward: 19616.86 | optimizer: 12401.58 | batch generator: 1.49 | data loader: 0.32
10.0.2.15:  iteration      709/    1000 | elapsed time per iteration (ms): 35356.4 | learning rate 9.871E-06 | lm loss 6.086047E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3636.63 | backward: 19277.19 | optimizer: 12395.08 | batch generator: 2.00 | data loader: 0.46
10.0.2.15:  iteration      710/    1000 | elapsed time per iteration (ms): 34985.6 | learning rate 9.886E-06 | lm loss 6.643865E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3615.68 | backward: 19094.09 | optimizer: 12273.78 | batch generator: 1.72 | data loader: 0.46
10.0.2.15:  iteration      711/    1000 | elapsed time per iteration (ms): 35754.7 | learning rate 9.900E-06 | lm loss 6.271707E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3785.05 | backward: 19277.07 | optimizer: 12454.62 | batch generator: 1.69 | data loader: 0.37
10.0.2.15:  iteration      712/    1000 | elapsed time per iteration (ms): 35593.7 | learning rate 9.914E-06 | lm loss 5.928710E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3977.74 | backward: 19805.27 | optimizer: 11808.56 | batch generator: 1.85 | data loader: 0.41
10.0.2.15:  iteration      713/    1000 | elapsed time per iteration (ms): 35386.0 | learning rate 9.929E-06 | lm loss 6.654956E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4166.23 | backward: 19046.47 | optimizer: 12171.89 | batch generator: 2.11 | data loader: 0.75
10.0.2.15:  iteration      714/    1000 | elapsed time per iteration (ms): 35418.2 | learning rate 9.943E-06 | lm loss 6.727169E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3879.59 | backward: 19137.66 | optimizer: 12399.38 | batch generator: 2.31 | data loader: 1.13
10.0.2.15:  iteration      715/    1000 | elapsed time per iteration (ms): 35851.6 | learning rate 9.957E-06 | lm loss 6.621006E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3725.14 | backward: 19581.70 | optimizer: 12542.27 | batch generator: 2.34 | data loader: 0.67
10.0.2.15:  iteration      716/    1000 | elapsed time per iteration (ms): 35405.6 | learning rate 9.971E-06 | lm loss 6.632099E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3690.24 | backward: 20180.70 | optimizer: 11532.48 | batch generator: 1.98 | data loader: 0.39
10.0.2.15:  iteration      717/    1000 | elapsed time per iteration (ms): 36085.7 | learning rate 9.986E-06 | lm loss 6.543197E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4280.66 | backward: 19722.12 | optimizer: 11902.29 | batch generator: 1.68 | data loader: 0.35
10.0.2.15:  iteration      718/    1000 | elapsed time per iteration (ms): 36310.5 | learning rate 1.000E-05 | lm loss 5.975577E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4053.44 | backward: 19570.28 | optimizer: 12438.52 | batch generator: 1.88 | data loader: 0.40
10.0.2.15:  iteration      719/    1000 | elapsed time per iteration (ms): 34811.6 | learning rate 1.001E-05 | lm loss 6.484491E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3571.67 | backward: 19664.88 | optimizer: 11264.32 | batch generator: 1.71 | data loader: 0.36
10.0.2.15:  iteration      720/    1000 | elapsed time per iteration (ms): 36592.6 | learning rate 1.003E-05 | lm loss 6.302488E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4701.32 | backward: 19492.12 | optimizer: 12394.16 | batch generator: 2.23 | data loader: 0.63
10.0.2.15:  iteration      721/    1000 | elapsed time per iteration (ms): 36031.6 | learning rate 1.004E-05 | lm loss 6.118836E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3561.62 | backward: 20002.22 | optimizer: 12181.69 | batch generator: 1.63 | data loader: 0.30
10.0.2.15:  iteration      722/    1000 | elapsed time per iteration (ms): 35815.2 | learning rate 1.006E-05 | lm loss 6.282296E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3914.35 | backward: 19986.55 | optimizer: 11848.97 | batch generator: 1.69 | data loader: 0.38
10.0.2.15:  iteration      723/    1000 | elapsed time per iteration (ms): 36758.6 | learning rate 1.007E-05 | lm loss 6.049520E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4560.91 | backward: 19689.44 | optimizer: 12506.04 | batch generator: 1.96 | data loader: 0.48
10.0.2.15:  iteration      724/    1000 | elapsed time per iteration (ms): 33529.2 | learning rate 1.009E-05 | lm loss 6.577815E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3547.17 | backward: 19623.40 | optimizer: 10254.94 | batch generator: 2.13 | data loader: 0.75
10.0.2.15:  iteration      725/    1000 | elapsed time per iteration (ms): 35925.4 | learning rate 1.010E-05 | lm loss 5.955218E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5783.34 | backward: 19490.85 | optimizer: 10649.68 | batch generator: 2.10 | data loader: 0.82
10.0.2.15:  iteration      726/    1000 | elapsed time per iteration (ms): 36907.9 | learning rate 1.011E-05 | lm loss 5.613593E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5365.95 | backward: 19515.71 | optimizer: 11773.71 | batch generator: 1.89 | data loader: 0.42
10.0.2.15:  iteration      727/    1000 | elapsed time per iteration (ms): 36470.5 | learning rate 1.013E-05 | lm loss 6.076473E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4235.69 | backward: 19655.61 | optimizer: 12320.70 | batch generator: 1.75 | data loader: 0.36
10.0.2.15:  iteration      728/    1000 | elapsed time per iteration (ms): 35433.6 | learning rate 1.014E-05 | lm loss 5.695579E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3958.31 | backward: 19373.23 | optimizer: 12100.42 | batch generator: 2.09 | data loader: 0.75
10.0.2.15:  iteration      729/    1000 | elapsed time per iteration (ms): 35893.5 | learning rate 1.016E-05 | lm loss 5.930183E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4000.61 | backward: 19269.31 | optimizer: 12304.09 | batch generator: 2.15 | data loader: 0.86
10.0.2.15:  iteration      730/    1000 | elapsed time per iteration (ms): 35778.9 | learning rate 1.017E-05 | lm loss 6.056568E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3934.83 | backward: 19581.84 | optimizer: 12214.29 | batch generator: 1.96 | data loader: 0.44
10.0.2.15:  iteration      731/    1000 | elapsed time per iteration (ms): 34881.2 | learning rate 1.019E-05 | lm loss 6.472373E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3752.50 | backward: 19185.34 | optimizer: 11941.49 | batch generator: 1.74 | data loader: 0.34
10.0.2.15:  iteration      732/    1000 | elapsed time per iteration (ms): 34969.7 | learning rate 1.020E-05 | lm loss 6.649468E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4015.77 | backward: 19437.72 | optimizer: 11507.88 | batch generator: 1.82 | data loader: 0.54
10.0.2.15:  iteration      733/    1000 | elapsed time per iteration (ms): 36650.6 | learning rate 1.021E-05 | lm loss 6.329534E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4448.05 | backward: 19670.62 | optimizer: 12444.09 | batch generator: 2.08 | data loader: 0.76
10.0.2.15:  iteration      734/    1000 | elapsed time per iteration (ms): 34950.3 | learning rate 1.023E-05 | lm loss 5.798368E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3407.52 | backward: 18828.77 | optimizer: 12216.84 | batch generator: 2.08 | data loader: 0.65
10.0.2.15:  iteration      735/    1000 | elapsed time per iteration (ms): 35180.9 | learning rate 1.024E-05 | lm loss 6.328496E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3545.43 | backward: 19460.71 | optimizer: 11903.92 | batch generator: 1.98 | data loader: 0.44
10.0.2.15:  iteration      736/    1000 | elapsed time per iteration (ms): 34869.5 | learning rate 1.026E-05 | lm loss 6.107546E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3832.71 | backward: 19243.95 | optimizer: 11100.84 | batch generator: 1.98 | data loader: 0.40
10.0.2.15:  iteration      737/    1000 | elapsed time per iteration (ms): 36494.5 | learning rate 1.027E-05 | lm loss 6.346128E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4852.70 | backward: 19617.55 | optimizer: 11914.71 | batch generator: 2.31 | data loader: 0.97
10.0.2.15:  iteration      738/    1000 | elapsed time per iteration (ms): 35942.0 | learning rate 1.029E-05 | lm loss 6.259373E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4028.78 | backward: 19559.48 | optimizer: 12349.76 | batch generator: 2.35 | data loader: 0.98
10.0.2.15:  iteration      739/    1000 | elapsed time per iteration (ms): 35440.2 | learning rate 1.030E-05 | lm loss 6.109618E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3593.34 | backward: 19446.44 | optimizer: 12357.39 | batch generator: 2.50 | data loader: 0.98
10.0.2.15:  iteration      740/    1000 | elapsed time per iteration (ms): 35083.0 | learning rate 1.031E-05 | lm loss 6.078378E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3728.99 | backward: 19286.09 | optimizer: 11957.14 | batch generator: 2.60 | data loader: 0.99
10.0.2.15:  iteration      741/    1000 | elapsed time per iteration (ms): 36603.1 | learning rate 1.033E-05 | lm loss 6.149016E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4546.14 | backward: 20094.48 | optimizer: 11960.42 | batch generator: 1.91 | data loader: 0.37
10.0.2.15:  iteration      742/    1000 | elapsed time per iteration (ms): 33789.7 | learning rate 1.034E-05 | lm loss 6.021662E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3984.39 | backward: 19510.68 | optimizer: 10137.87 | batch generator: 1.96 | data loader: 0.41
10.0.2.15:  iteration      743/    1000 | elapsed time per iteration (ms): 37455.5 | learning rate 1.036E-05 | lm loss 6.139403E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6035.86 | backward: 19560.18 | optimizer: 11634.69 | batch generator: 1.42 | data loader: 0.37
10.0.2.15:  iteration      744/    1000 | elapsed time per iteration (ms): 36182.7 | learning rate 1.037E-05 | lm loss 6.141844E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4557.15 | backward: 19212.83 | optimizer: 12410.75 | batch generator: 1.96 | data loader: 0.63
10.0.2.15:  iteration      745/    1000 | elapsed time per iteration (ms): 35961.5 | learning rate 1.039E-05 | lm loss 5.974012E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4026.23 | backward: 19650.61 | optimizer: 12282.82 | batch generator: 1.61 | data loader: 0.33
10.0.2.15:  iteration      746/    1000 | elapsed time per iteration (ms): 33013.3 | learning rate 1.040E-05 | lm loss 6.272758E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3876.26 | backward: 19423.90 | optimizer: 9700.74 | batch generator: 2.25 | data loader: 0.82
10.0.2.15:  iteration      747/    1000 | elapsed time per iteration (ms): 36805.1 | learning rate 1.041E-05 | lm loss 6.028123E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 6062.37 | backward: 19078.11 | optimizer: 11317.73 | batch generator: 1.45 | data loader: 0.34
10.0.2.15:  iteration      748/    1000 | elapsed time per iteration (ms): 36505.9 | learning rate 1.043E-05 | lm loss 6.254685E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4464.87 | backward: 19464.53 | optimizer: 12442.56 | batch generator: 1.91 | data loader: 0.42
10.0.2.15:  iteration      749/    1000 | elapsed time per iteration (ms): 34796.4 | learning rate 1.044E-05 | lm loss 6.338651E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3536.02 | backward: 19081.02 | optimizer: 12176.93 | batch generator: 1.93 | data loader: 0.59
10.0.2.15: [2022-01-01 02:16:01,612] [INFO] [logging.py:60:log_dist] [Rank 0] step=750, skipped=18, lr=[1.0442857142857143e-05, 1.0442857142857143e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2022-01-01 02:16:12,177] [INFO] [timer.py:154:stop] 0/750, SamplesPerSec=2.2914923534649048
10.0.2.15:  iteration      750/    1000 | elapsed time per iteration (ms): 34328.0 | learning rate 1.046E-05 | lm loss 5.946176E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3882.16 | backward: 19479.98 | optimizer: 10730.75 | batch generator: 1.92 | data loader: 0.45
10.0.2.15:  iteration      751/    1000 | elapsed time per iteration (ms): 37582.8 | learning rate 1.047E-05 | lm loss 5.969627E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5782.72 | backward: 19894.94 | optimizer: 11903.20 | batch generator: 1.64 | data loader: 0.45
10.0.2.15:  iteration      752/    1000 | elapsed time per iteration (ms): 36303.4 | learning rate 1.049E-05 | lm loss 6.226856E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4059.24 | backward: 19535.03 | optimizer: 12418.97 | batch generator: 1.82 | data loader: 0.40
10.0.2.15:  iteration      753/    1000 | elapsed time per iteration (ms): 35842.7 | learning rate 1.050E-05 | lm loss 6.150282E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4011.02 | backward: 19970.84 | optimizer: 11858.70 | batch generator: 1.73 | data loader: 0.36
10.0.2.15:  iteration      754/    1000 | elapsed time per iteration (ms): 35947.9 | learning rate 1.051E-05 | lm loss 6.126776E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4114.00 | backward: 19554.28 | optimizer: 12080.52 | batch generator: 2.20 | data loader: 0.88
10.0.2.15:  iteration      755/    1000 | elapsed time per iteration (ms): 35793.7 | learning rate 1.053E-05 | lm loss 6.397801E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3877.65 | backward: 19501.73 | optimizer: 12412.25 | batch generator: 1.74 | data loader: 0.53
10.0.2.15:  iteration      756/    1000 | elapsed time per iteration (ms): 35613.0 | learning rate 1.054E-05 | lm loss 5.990693E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3651.72 | backward: 19775.79 | optimizer: 11937.24 | batch generator: 1.54 | data loader: 0.30
10.0.2.15:  iteration      757/    1000 | elapsed time per iteration (ms): 36141.1 | learning rate 1.056E-05 | lm loss 6.383086E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4239.29 | backward: 19626.85 | optimizer: 12273.51 | batch generator: 1.76 | data loader: 0.73
10.0.2.15:  iteration      758/    1000 | elapsed time per iteration (ms): 35406.6 | learning rate 1.057E-05 | lm loss 5.765625E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3705.98 | backward: 19350.82 | optimizer: 12345.31 | batch generator: 1.63 | data loader: 0.32
10.0.2.15:  iteration      759/    1000 | elapsed time per iteration (ms): 36120.5 | learning rate 1.059E-05 | lm loss 5.830842E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4123.86 | backward: 20114.72 | optimizer: 11879.88 | batch generator: 1.98 | data loader: 0.75
10.0.2.15:  iteration      760/    1000 | elapsed time per iteration (ms): 36484.0 | learning rate 1.060E-05 | lm loss 6.083717E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4170.62 | backward: 19952.89 | optimizer: 12156.03 | batch generator: 2.20 | data loader: 0.89
10.0.2.15:  iteration      761/    1000 | elapsed time per iteration (ms): 36180.4 | learning rate 1.061E-05 | lm loss 6.077909E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3821.34 | backward: 19730.93 | optimizer: 12428.33 | batch generator: 1.75 | data loader: 0.54
10.0.2.15:  iteration      762/    1000 | elapsed time per iteration (ms): 36622.9 | learning rate 1.063E-05 | lm loss 6.120567E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3867.70 | backward: 20423.21 | optimizer: 12330.14 | batch generator: 1.64 | data loader: 0.39
10.0.2.15:  iteration      763/    1000 | elapsed time per iteration (ms): 36039.6 | learning rate 1.064E-05 | lm loss 5.976746E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3593.44 | backward: 19845.25 | optimizer: 12330.28 | batch generator: 2.40 | data loader: 1.05
10.0.2.15:  iteration      764/    1000 | elapsed time per iteration (ms): 34026.4 | learning rate 1.066E-05 | lm loss 6.010322E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3675.96 | backward: 19643.18 | optimizer: 10616.38 | batch generator: 1.65 | data loader: 0.29
10.0.2.15:  iteration      765/    1000 | elapsed time per iteration (ms): 35968.3 | learning rate 1.067E-05 | lm loss 6.046353E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5508.27 | backward: 19354.07 | optimizer: 11104.21 | batch generator: 1.75 | data loader: 0.53
10.0.2.15:  iteration      766/    1000 | elapsed time per iteration (ms): 36715.6 | learning rate 1.069E-05 | lm loss 6.464796E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4908.66 | backward: 19747.87 | optimizer: 11792.04 | batch generator: 1.62 | data loader: 0.31
10.0.2.15:  iteration      767/    1000 | elapsed time per iteration (ms): 36182.5 | learning rate 1.070E-05 | lm loss 5.989335E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4253.64 | backward: 19395.96 | optimizer: 12340.93 | batch generator: 1.83 | data loader: 0.55
10.0.2.15:  iteration      768/    1000 | elapsed time per iteration (ms): 35882.4 | learning rate 1.071E-05 | lm loss 6.098359E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3625.58 | backward: 19539.93 | optimizer: 12376.54 | batch generator: 2.09 | data loader: 0.77
10.0.2.15:  iteration      769/    1000 | elapsed time per iteration (ms): 34915.0 | learning rate 1.073E-05 | lm loss 5.803140E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3588.38 | backward: 19789.16 | optimizer: 11372.53 | batch generator: 1.92 | data loader: 0.67
10.0.2.15:  iteration      770/    1000 | elapsed time per iteration (ms): 36564.1 | learning rate 1.074E-05 | lm loss 6.191016E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4831.89 | backward: 20000.42 | optimizer: 11634.06 | batch generator: 1.66 | data loader: 0.33
10.0.2.15:  iteration      771/    1000 | elapsed time per iteration (ms): 35677.9 | learning rate 1.076E-05 | lm loss 6.021568E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4238.95 | backward: 19263.81 | optimizer: 11871.56 | batch generator: 2.28 | data loader: 0.97
10.0.2.15:  iteration      772/    1000 | elapsed time per iteration (ms): 35415.3 | learning rate 1.077E-05 | lm loss 6.384037E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3967.16 | backward: 19143.91 | optimizer: 12234.07 | batch generator: 1.84 | data loader: 0.55
10.0.2.15:  iteration      773/    1000 | elapsed time per iteration (ms): 35700.7 | learning rate 1.079E-05 | lm loss 5.863245E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3909.04 | backward: 19364.14 | optimizer: 12221.00 | batch generator: 1.61 | data loader: 0.35
10.0.2.15:  iteration      774/    1000 | elapsed time per iteration (ms): 36103.5 | learning rate 1.080E-05 | lm loss 6.669586E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3551.19 | backward: 19761.41 | optimizer: 12435.00 | batch generator: 2.35 | data loader: 1.00
10.0.2.15:  iteration      775/    1000 | elapsed time per iteration (ms): 36216.6 | learning rate 1.081E-05 | lm loss 6.118042E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3873.05 | backward: 19962.25 | optimizer: 12379.55 | batch generator: 2.64 | data loader: 1.22
10.0.2.15:  iteration      776/    1000 | elapsed time per iteration (ms): 33659.0 | learning rate 1.083E-05 | lm loss 6.082379E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3618.23 | backward: 19501.40 | optimizer: 10481.06 | batch generator: 1.85 | data loader: 0.55
10.0.2.15:  iteration      777/    1000 | elapsed time per iteration (ms): 36775.9 | learning rate 1.084E-05 | lm loss 6.133004E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5594.95 | backward: 19534.33 | optimizer: 11644.93 | batch generator: 1.62 | data loader: 0.30
10.0.2.15:  iteration      778/    1000 | elapsed time per iteration (ms): 36929.1 | learning rate 1.086E-05 | lm loss 5.767790E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4447.56 | backward: 19753.77 | optimizer: 12462.09 | batch generator: 2.22 | data loader: 1.08
10.0.2.15:  iteration      779/    1000 | elapsed time per iteration (ms): 34963.0 | learning rate 1.087E-05 | lm loss 5.920975E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3838.64 | backward: 19726.48 | optimizer: 11386.65 | batch generator: 2.03 | data loader: 0.78
10.0.2.15:  iteration      780/    1000 | elapsed time per iteration (ms): 36583.8 | learning rate 1.089E-05 | lm loss 6.278363E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4323.16 | backward: 19520.60 | optimizer: 12070.51 | batch generator: 2.27 | data loader: 1.07
10.0.2.15:  iteration      781/    1000 | elapsed time per iteration (ms): 35175.2 | learning rate 1.090E-05 | lm loss 6.185760E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3981.12 | backward: 19669.43 | optimizer: 11522.62 | batch generator: 1.73 | data loader: 0.57
10.0.2.15:  iteration      782/    1000 | elapsed time per iteration (ms): 36145.9 | learning rate 1.091E-05 | lm loss 6.111382E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4860.24 | backward: 20002.98 | optimizer: 11280.69 | batch generator: 1.50 | data loader: 0.31
10.0.2.15:  iteration      783/    1000 | elapsed time per iteration (ms): 36028.8 | learning rate 1.093E-05 | lm loss 5.971546E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4661.38 | backward: 19370.54 | optimizer: 11968.41 | batch generator: 1.93 | data loader: 0.69
10.0.2.15:  iteration      784/    1000 | elapsed time per iteration (ms): 36192.8 | learning rate 1.094E-05 | lm loss 6.315233E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3961.71 | backward: 19655.34 | optimizer: 12522.39 | batch generator: 2.03 | data loader: 0.76
10.0.2.15:  iteration      785/    1000 | elapsed time per iteration (ms): 35880.9 | learning rate 1.096E-05 | lm loss 6.500934E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3566.84 | backward: 19648.50 | optimizer: 12381.47 | batch generator: 1.73 | data loader: 0.39
10.0.2.15:  iteration      786/    1000 | elapsed time per iteration (ms): 36034.6 | learning rate 1.097E-05 | lm loss 6.460210E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3328.59 | backward: 19751.58 | optimizer: 12230.46 | batch generator: 2.35 | data loader: 0.98
10.0.2.15:  iteration      787/    1000 | elapsed time per iteration (ms): 35455.8 | learning rate 1.099E-05 | lm loss 5.674677E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3997.61 | backward: 19640.97 | optimizer: 11815.39 | batch generator: 1.52 | data loader: 0.31
10.0.2.15:  iteration      788/    1000 | elapsed time per iteration (ms): 35288.0 | learning rate 1.100E-05 | lm loss 6.099798E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4386.96 | backward: 19808.73 | optimizer: 10982.82 | batch generator: 1.72 | data loader: 0.53
10.0.2.15:  iteration      789/    1000 | elapsed time per iteration (ms): 36846.6 | learning rate 1.101E-05 | lm loss 6.288286E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5001.40 | backward: 19982.22 | optimizer: 11526.43 | batch generator: 1.90 | data loader: 0.57
10.0.2.15:  iteration      790/    1000 | elapsed time per iteration (ms): 35919.5 | learning rate 1.103E-05 | lm loss 5.806589E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4500.96 | backward: 19466.75 | optimizer: 11652.30 | batch generator: 1.77 | data loader: 0.40
10.0.2.15:  iteration      791/    1000 | elapsed time per iteration (ms): 36667.2 | learning rate 1.104E-05 | lm loss 5.760043E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4335.64 | backward: 20215.82 | optimizer: 11912.02 | batch generator: 1.77 | data loader: 0.49
10.0.2.15:  iteration      792/    1000 | elapsed time per iteration (ms): 35452.8 | learning rate 1.106E-05 | lm loss 6.108292E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4321.19 | backward: 19972.86 | optimizer: 11157.06 | batch generator: 2.23 | data loader: 0.96
10.0.2.15:  iteration      793/    1000 | elapsed time per iteration (ms): 37124.0 | learning rate 1.107E-05 | lm loss 6.164912E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 5222.57 | backward: 19884.73 | optimizer: 12014.92 | batch generator: 1.69 | data loader: 0.49
10.0.2.15:  iteration      794/    1000 | elapsed time per iteration (ms): 36358.4 | learning rate 1.109E-05 | lm loss 5.949894E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4373.96 | backward: 19604.05 | optimizer: 12378.35 | batch generator: 1.77 | data loader: 0.57
10.0.2.15:  iteration      795/    1000 | elapsed time per iteration (ms): 36157.6 | learning rate 1.110E-05 | lm loss 5.861786E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3576.46 | backward: 19828.85 | optimizer: 12427.56 | batch generator: 1.72 | data loader: 0.49
10.0.2.15:  iteration      796/    1000 | elapsed time per iteration (ms): 35823.2 | learning rate 1.111E-05 | lm loss 6.154680E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3541.04 | backward: 19456.76 | optimizer: 12412.77 | batch generator: 1.93 | data loader: 0.69
10.0.2.15:  iteration      797/    1000 | elapsed time per iteration (ms): 35486.5 | learning rate 1.113E-05 | lm loss 6.240275E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3539.18 | backward: 19330.59 | optimizer: 12251.33 | batch generator: 2.25 | data loader: 0.90
10.0.2.15:  iteration      798/    1000 | elapsed time per iteration (ms): 35888.2 | learning rate 1.114E-05 | lm loss 6.300758E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3658.82 | backward: 19639.59 | optimizer: 12312.55 | batch generator: 1.66 | data loader: 0.50
10.0.2.15:  iteration      799/    1000 | elapsed time per iteration (ms): 35538.0 | learning rate 1.116E-05 | lm loss 6.220313E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3575.67 | backward: 19516.58 | optimizer: 12086.50 | batch generator: 1.76 | data loader: 0.51
10.0.2.15: [2022-01-01 02:45:57,641] [INFO] [logging.py:60:log_dist] [Rank 0] step=800, skipped=18, lr=[1.1157142857142858e-05, 1.1157142857142858e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.15: [2022-01-01 02:46:09,600] [INFO] [timer.py:154:stop] 0/800, SamplesPerSec=2.294286740236054
10.0.2.15:  iteration      800/    1000 | elapsed time per iteration (ms): 35600.5 | learning rate 1.117E-05 | lm loss 5.971745E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 3855.65 | backward: 19604.18 | optimizer: 12027.90 | batch generator: 1.59 | data loader: 0.37
10.0.2.15:  iteration      801/    1000 | elapsed time per iteration (ms): 35583.8 | learning rate 1.119E-05 | lm loss 5.984305E+00 | loss scale 32768.0 |
10.0.2.15: time (ms) | forward: 4018.28 | backward: 19996.31 | optimizer: 11335.97 | batch generator: 1.97 | data loader: 0.92
10.0.2.15:  iteration      802/    1000 | elapsed time per iteration (ms): 37144.0 | learning rate 1.120E-05 | lm loss 6.009385E+00 | loss scale 32768.0 |
10.