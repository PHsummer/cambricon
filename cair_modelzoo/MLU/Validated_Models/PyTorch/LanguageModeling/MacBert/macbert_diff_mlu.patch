Binary files transformers_mlu/examples/legacy/question-answering/.nfs80d02fac010f891500000913 and question_answering/.nfs80d02fac010f891500000913 differ
diff -Nur transformers_mlu/examples/legacy/question-answering/run_squad.py question_answering/run_squad.py
--- transformers_mlu/examples/legacy/question-answering/run_squad.py	2022-06-30 15:11:49.931336000 +0800
+++ question_answering/run_squad.py	2022-06-30 15:09:36.539356000 +0800
@@ -25,7 +25,6 @@
 
 import numpy as np
 import torch
-import torch_mlu
 from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
 from torch.utils.data.distributed import DistributedSampler
 from tqdm import tqdm, trange
@@ -55,6 +54,7 @@
 except ImportError:
     from tensorboardX import SummaryWriter
 
+from torch_mlu.core.dumptool import dump_cnnl_gencase
 
 logger = logging.getLogger(__name__)
 
@@ -67,7 +67,7 @@
     np.random.seed(args.seed)
     torch.manual_seed(args.seed)
     if args.n_gpu > 0:
-        torch.mlu.manual_seed_all(args.seed)
+        torch.cuda.manual_seed_all(args.seed)
 
 
 def to_list(tensor):
@@ -173,14 +173,18 @@
     for _ in train_iterator:
         epoch_iterator = tqdm(train_dataloader, desc="Iteration", disable=args.local_rank not in [-1, 0])
         for step, batch in enumerate(epoch_iterator):
-
             # Skip past any already trained steps if resuming training
             if steps_trained_in_current_epoch > 0:
                 steps_trained_in_current_epoch -= 1
                 continue
 
+            if args.num_train_epochs == 1.0 and step == args.iters:
+                break
+
             model.train()
             batch = tuple(t.to(args.device) for t in batch)
+            logger.info("*************************devices************************")
+            logger.info(t.device for t in batch)
 
             inputs = {
                 "input_ids": batch[0],
@@ -552,10 +556,8 @@
         "--max_seq_length",
         default=384,
         type=int,
-        help=(
-            "The maximum total input sequence length after WordPiece tokenization. Sequences "
-            "longer than this will be truncated, and sequences shorter than this will be padded."
-        ),
+        help="The maximum total input sequence length after WordPiece tokenization. Sequences "
+        "longer than this will be truncated, and sequences shorter than this will be padded.",
     )
     parser.add_argument(
         "--doc_stride",
@@ -567,10 +569,8 @@
         "--max_query_length",
         default=64,
         type=int,
-        help=(
-            "The maximum number of tokens for the question. Questions longer than this will "
-            "be truncated to this length."
-        ),
+        help="The maximum number of tokens for the question. Questions longer than this will "
+        "be truncated to this length.",
     )
     parser.add_argument("--do_train", action="store_true", help="Whether to run training.")
     parser.add_argument("--do_eval", action="store_true", help="Whether to run eval on the dev set.")
@@ -615,27 +615,20 @@
         "--max_answer_length",
         default=30,
         type=int,
-        help=(
-            "The maximum length of an answer that can be generated. This is needed because the start "
-            "and end predictions are not conditioned on one another."
-        ),
+        help="The maximum length of an answer that can be generated. This is needed because the start "
+        "and end predictions are not conditioned on one another.",
     )
     parser.add_argument(
         "--verbose_logging",
         action="store_true",
-        help=(
-            "If true, all of the warnings related to data processing will be printed. "
-            "A number of warnings are expected for a normal SQuAD evaluation."
-        ),
+        help="If true, all of the warnings related to data processing will be printed. "
+        "A number of warnings are expected for a normal SQuAD evaluation.",
     )
     parser.add_argument(
         "--lang_id",
         default=0,
         type=int,
-        help=(
-            "language id of input for language-specific xlm models (see"
-            " tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)"
-        ),
+        help="language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)",
     )
 
     parser.add_argument("--logging_steps", type=int, default=500, help="Log every X updates steps.")
@@ -645,7 +638,7 @@
         action="store_true",
         help="Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number",
     )
-    parser.add_argument("--no_mlu", action="store_true", help="Whether not to use CUDA when available")
+    parser.add_argument("--no_cuda", action="store_true", help="Whether not to use CUDA when available")
     parser.add_argument(
         "--overwrite_output_dir", action="store_true", help="Overwrite the content of the output directory"
     )
@@ -664,17 +657,26 @@
         "--fp16_opt_level",
         type=str,
         default="O1",
-        help=(
-            "For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
-            "See details at https://nvidia.github.io/apex/amp.html"
-        ),
+        help="For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
+        "See details at https://nvidia.github.io/apex/amp.html",
     )
     parser.add_argument("--server_ip", type=str, default="", help="Can be used for distant debugging.")
     parser.add_argument("--server_port", type=str, default="", help="Can be used for distant debugging.")
 
     parser.add_argument("--threads", type=int, default=1, help="multiple threads for converting example to features")
+    parser.add_argument("--device", type=str, default="", help="Use GPU or MLU to train.")
+    parser.add_argument("--step_per_epoch", type=int, default=-1, help="How many steps in one epoch.")
+    parser.add_argument("--iters", type=int, default=200, help="the iteration where to stop")
     args = parser.parse_args()
 
+    if args.device == "mlu":
+        try:
+            import torch_mlu
+            import torch_mlu.core.mlu_model as ct
+            from torch_mlu.core.dumptool import dump_cnnl_gencase
+        except Exception as e:
+            print(e)
+
     if args.doc_stride >= args.max_seq_length - args.max_query_length:
         logger.warning(
             "WARNING - You've set a doc stride which may be superior to the document length in some "
@@ -704,17 +706,27 @@
         ptvsd.wait_for_attach()
 
     # Setup CUDA, GPU & distributed training
-    if args.local_rank == -1 or args.no_mlu:
-        device = torch.device("mlu" if torch.mlu.is_available() and not args.no_mlu else "cpu")
-        args.n_gpu = 0 if args.no_mlu else torch.mlu.device_count()
-    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
-        torch.mlu.set_device(args.local_rank)
-        device = torch.device("mlu", args.local_rank)
-        torch.distributed.init_process_group(backend="cncl")
-        args.n_gpu = 1
+    if args.device == "gpu":
+        if args.local_rank == -1 or args.no_cuda:
+            device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
+            args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()
+        else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
+            torch.cuda.set_device(args.local_rank)
+            device = torch.device("cuda", args.local_rank)
+            torch.distributed.init_process_group(backend="nccl")
+            args.n_gpu = 1
+    if args.device == "mlu":
+        if args.local_rank == -1:
+            device = torch.device("mlu" if torch.mlu.is_available() else "cpu")
+            args.n_gpu = torch.mlu.device_count()
+        else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
+            torch.mlu.set_device(args.local_rank)
+            device = torch.device("mlu", args.local_rank)
+            torch.distributed.init_process_group(backend="cncl")
+            args.n_gpu = 1
     args.device = device
 
-    # Setup logging
+    # Setup logging`
     logging.basicConfig(
         format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
         datefmt="%m/%d/%Y %H:%M:%S",
diff -Nur transformers_mlu/examples/legacy/question-answering/run_squad_trainer.py question_answering/run_squad_trainer.py
--- transformers_mlu/examples/legacy/question-answering/run_squad_trainer.py	2022-06-30 15:11:49.953348000 +0800
+++ question_answering/run_squad_trainer.py	2022-05-08 20:32:21.000000000 +0800
@@ -84,8 +84,7 @@
         and not training_args.overwrite_output_dir
     ):
         raise ValueError(
-            f"Output directory ({training_args.output_dir}) already exists and is not empty. Use"
-            " --overwrite_output_dir to overcome."
+            f"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome."
         )
 
     # Setup logging
