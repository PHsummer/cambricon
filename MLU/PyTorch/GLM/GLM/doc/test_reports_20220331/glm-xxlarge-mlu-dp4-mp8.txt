[ERROR][/torch/catch/torch_mlu/csrc/aten/util/version.cpp][line: 69][parse][thread:139693212415808][process:45261]: Unknown library version string :1.2.0-torch1.6
[2022-04-01 14:57:38,733] [INFO] [multinode_runner.py:51:get_cmd] Running on the following workers: 10.0.2.2,10.0.2.8
[2022-04-01 14:57:38,733] [INFO] [runner.py:363:main] cmd = pdsh -f 1024 -w 10.0.2.2,10.0.2.8 export PYTHON_VERSION_NO=3.6; export PYTHONPATH=/home/glm_project/glm; export PILE_DATASET_PATH=/data/cpm/glm/pile/train; export LID_176_PATH=/data/cpm/glm/fastText/lid.176.bin; export CNCL_MEM_POOL_ENABLE=0; export CNCL_IB_DISABLE=0; export CNCL_SLICE_SIZE=2097152; export CNCL_MLU_DIRECT_LEVEL=1; export CNCL_P2P_LEVEL=0; export CNCL_HIERARCHY_RING_NUM=0; export PATH=/torch/venv3/pytorch/bin:/usr/local/openmpi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; export LD_LIBRARY_PATH=/torch/neuware_home/lib64:/usr/local/openmpi/lib:; export NEUWARE_HOME=/torch/neuware_home;  cd /home/glm_project/glm; /torch/venv3/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyIxMC4wLjIuMiI6IFswLCAxLCAyLCAzLCA0LCA1LCA2LCA3LCA4LCA5LCAxMCwgMTEsIDEyLCAxMywgMTQsIDE1XSwgIjEwLjAuMi44IjogWzAsIDEsIDIsIDMsIDQsIDUsIDYsIDcsIDgsIDksIDEwLCAxMSwgMTIsIDEzLCAxNCwgMTVdfQ== --node_rank=%n --master_addr=10.0.2.2 --master_port=49204 pretrain_glm.py --block-lm --task-mask --bert-prob '0.5' --gap-sentence-prob '0.3' --avg-block-length '3' --gpt-min-ratio '0.25' --block-mask-prob '0.1' --short-seq-prob '0.02' --experiment-name 'blocklm-10b' --model-parallel-size '8' --num-layers '48' --hidden-size '4096' --num-attention-heads '64' --seq-length '512' --max-position-embeddings '1024' --save './checkpoints' --log-interval '1' --eval-interval '1000' --save-interval '2000' --train-iters '1000' --train-data 'pile' --resume-dataloader --filter-english --loader-scatter '2' --tokenizer-type 'GPT2BPETokenizer' --split '949,50,1' --distributed-backend 'cncl' --lr-decay-style 'cosine' --lr-decay-ratio '0.1' --lr-decay-iters '175000' --warmup '0.04' --checkpoint-activations --deepspeed-activation-checkpointing --fp16 --deepspeed --deepspeed_config '/home/glm_project/glm/config/config_block_10B.json'
10.0.2.2: [ERROR][/torch/catch/torch_mlu/csrc/aten/util/version.cpp][line: 69][parse][thread:140632033871680][process:45416]: Unknown library version string :1.2.0-torch1.6
10.0.2.8: [ERROR][/torch/catch/torch_mlu/csrc/aten/util/version.cpp][line: 69][parse][thread:140196366718784][process:42108]: Unknown library version string :1.2.0-torch1.6
10.0.2.2: [2022-04-01 14:57:39,584] [INFO] [launch.py:86:main] WORLD INFO DICT: {'10.0.2.2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], '10.0.2.8': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}
10.0.2.2: [2022-04-01 14:57:39,584] [INFO] [launch.py:95:main] nnodes=2, num_local_procs=16, node_rank=0
10.0.2.2: [2022-04-01 14:57:39,584] [INFO] [launch.py:107:main] global_rank_mapping=defaultdict(<class 'list'>, {'10.0.2.2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], '10.0.2.8': [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]})
10.0.2.2: [2022-04-01 14:57:39,584] [INFO] [launch.py:108:main] dist_world_size=32
10.0.2.2: [2022-04-01 14:57:39,584] [INFO] [launch.py:112:main] Setting MLU_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
10.0.2.8: [2022-04-01 14:57:39,605] [INFO] [launch.py:86:main] WORLD INFO DICT: {'10.0.2.2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], '10.0.2.8': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}
10.0.2.8: [2022-04-01 14:57:39,605] [INFO] [launch.py:95:main] nnodes=2, num_local_procs=16, node_rank=1
10.0.2.8: [2022-04-01 14:57:39,605] [INFO] [launch.py:107:main] global_rank_mapping=defaultdict(<class 'list'>, {'10.0.2.2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], '10.0.2.8': [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]})
10.0.2.8: [2022-04-01 14:57:39,605] [INFO] [launch.py:108:main] dist_world_size=32
10.0.2.8: [2022-04-01 14:57:39,605] [INFO] [launch.py:112:main] Setting MLU_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: using world size: 32 and model-parallel size: 8
10.0.2.2:  > using dynamic loss scaling
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/mlu_model.py:629: UserWarning: torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. When the parameter setting value is 8 or 16, the calculation accuracymay be affected..
10.0.2.8:   "torch_mlu.core.mlu_model.set_quantized_bitwidth is deprecated. "
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: > initializing model parallel with size 8
10.0.2.2: [2022-04-01 14:57:42,905] [INFO] [checkpointing.py:763:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.2: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.8: WARNING: MLU does not set parallel random seed
10.0.2.2: > padded vocab (size: 50266) with 38 dummy tokens (new size: 50304)
10.0.2.2: > found end-of-document token: 50256
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:43] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:44] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:45] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 4--[MLU_LINK]->0--[MLU_LINK]->1
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 0--[MLU_LINK]->1--[MLU_LINK]->3
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 1--[MLU_LINK]->3--[MLU_LINK]->2
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 3--[MLU_LINK]->2--[MLU_LINK]->6
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 2--[MLU_LINK]->6--[MLU_LINK]->7
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 6--[MLU_LINK]->7--[MLU_LINK]->5
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 7--[MLU_LINK]->5--[MLU_LINK]->4
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 5--[MLU_LINK]->4--[MLU_LINK]->0
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 2--[MLU_LINK]->0--[MLU_LINK]->1
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 0--[MLU_LINK]->1--[MLU_LINK]->5
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 1--[MLU_LINK]->5--[MLU_LINK]->4
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 5--[MLU_LINK]->4--[MLU_LINK]->6
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 4--[MLU_LINK]->6--[MLU_LINK]->7
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 6--[MLU_LINK]->7--[MLU_LINK]->3
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 7--[MLU_LINK]->3--[MLU_LINK]->2
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 3--[MLU_LINK]->2--[MLU_LINK]->0
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 1--[MLU_LINK]->0--[MLU_LINK]->2
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 0--[MLU_LINK]->2--[MLU_LINK]->3
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 2--[MLU_LINK]->3--[MLU_LINK]->7
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 3--[MLU_LINK]->7--[MLU_LINK]->6
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 7--[MLU_LINK]->6--[MLU_LINK]->4
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 6--[MLU_LINK]->4--[MLU_LINK]->5
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 4--[MLU_LINK]->5--[MLU_LINK]->1
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 5--[MLU_LINK]->1--[MLU_LINK]->0
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 1--[MLU_LINK]->0--[MLU_LINK]->4
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 0--[MLU_LINK]->4--[MLU_LINK]->5
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 4--[MLU_LINK]->5--[MLU_LINK]->7
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 5--[MLU_LINK]->7--[MLU_LINK]->6
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 7--[MLU_LINK]->6--[MLU_LINK]->2
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 6--[MLU_LINK]->2--[MLU_LINK]->3
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 2--[MLU_LINK]->3--[MLU_LINK]->1
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 3--[MLU_LINK]->1--[MLU_LINK]->0
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 4--[MLU_LINK]->0--[MLU_LINK]->1
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 0--[MLU_LINK]->1--[MLU_LINK]->3
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 1--[MLU_LINK]->3--[MLU_LINK]->2
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 3--[MLU_LINK]->2--[MLU_LINK]->6
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 2--[MLU_LINK]->6--[MLU_LINK]->7
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 6--[MLU_LINK]->7--[MLU_LINK]->5
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 7--[MLU_LINK]->5--[MLU_LINK]->4
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 5--[MLU_LINK]->4--[MLU_LINK]->0
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 2--[MLU_LINK]->0--[MLU_LINK]->1
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 0--[MLU_LINK]->1--[MLU_LINK]->5
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 1--[MLU_LINK]->5--[MLU_LINK]->4
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 5--[MLU_LINK]->4--[MLU_LINK]->6
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 4--[MLU_LINK]->6--[MLU_LINK]->7
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 6--[MLU_LINK]->7--[MLU_LINK]->3
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 7--[MLU_LINK]->3--[MLU_LINK]->2
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 3--[MLU_LINK]->2--[MLU_LINK]->0
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 1--[MLU_LINK]->0--[MLU_LINK]->2
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 0--[MLU_LINK]->2--[MLU_LINK]->3
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 2--[MLU_LINK]->3--[MLU_LINK]->7
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 3--[MLU_LINK]->7--[MLU_LINK]->6
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 7--[MLU_LINK]->6--[MLU_LINK]->4
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 6--[MLU_LINK]->4--[MLU_LINK]->5
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 4--[MLU_LINK]->5--[MLU_LINK]->1
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 5--[MLU_LINK]->1--[MLU_LINK]->0
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 1--[MLU_LINK]->0--[MLU_LINK]->4
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 0--[MLU_LINK]->4--[MLU_LINK]->5
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 4--[MLU_LINK]->5--[MLU_LINK]->7
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 5--[MLU_LINK]->7--[MLU_LINK]->6
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 7--[MLU_LINK]->6--[MLU_LINK]->2
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 6--[MLU_LINK]->2--[MLU_LINK]->3
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 2--[MLU_LINK]->3--[MLU_LINK]->1
10.0.2.8: [4-1 14:57:46] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 3--[MLU_LINK]->1--[MLU_LINK]->0
10.0.2.8: Rank 24 is using scatter from /data/cpm/glm/pile/train.scatter/1
10.0.2.8: Rank 16 is using scatter from /data/cpm/glm/pile/train.scatter/0
10.0.2.8: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.8: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: MLU Direct RDMA is enabled.
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 4--[MLU_LINK]->0--[MLU_LINK]->1
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 0--[MLU_LINK]->1--[MLU_LINK]->3
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 1--[MLU_LINK]->3--[MLU_LINK]->2
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 3--[MLU_LINK]->2--[MLU_LINK]->6
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 2--[MLU_LINK]->6--[MLU_LINK]->7
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 6--[MLU_LINK]->7--[MLU_LINK]->5
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 7--[MLU_LINK]->5--[MLU_LINK]->4
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 5--[MLU_LINK]->4--[MLU_LINK]->0
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 2--[MLU_LINK]->0--[MLU_LINK]->1
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 0--[MLU_LINK]->1--[MLU_LINK]->5
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 1--[MLU_LINK]->5--[MLU_LINK]->4
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 5--[MLU_LINK]->4--[MLU_LINK]->6
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 4--[MLU_LINK]->6--[MLU_LINK]->7
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 6--[MLU_LINK]->7--[MLU_LINK]->3
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 7--[MLU_LINK]->3--[MLU_LINK]->2
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 3--[MLU_LINK]->2--[MLU_LINK]->0
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 1--[MLU_LINK]->0--[MLU_LINK]->2
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 0--[MLU_LINK]->2--[MLU_LINK]->3
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 2--[MLU_LINK]->3--[MLU_LINK]->7
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 3--[MLU_LINK]->7--[MLU_LINK]->6
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 7--[MLU_LINK]->6--[MLU_LINK]->4
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 6--[MLU_LINK]->4--[MLU_LINK]->5
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 4--[MLU_LINK]->5--[MLU_LINK]->1
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 5--[MLU_LINK]->1--[MLU_LINK]->0
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 1--[MLU_LINK]->0--[MLU_LINK]->4
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 0--[MLU_LINK]->4--[MLU_LINK]->5
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 4--[MLU_LINK]->5--[MLU_LINK]->7
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 5--[MLU_LINK]->7--[MLU_LINK]->6
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 7--[MLU_LINK]->6--[MLU_LINK]->2
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 6--[MLU_LINK]->2--[MLU_LINK]->3
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 2--[MLU_LINK]->3--[MLU_LINK]->1
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 3--[MLU_LINK]->1--[MLU_LINK]->0
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 4--[MLU_LINK]->0--[MLU_LINK]->1
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 0--[MLU_LINK]->1--[MLU_LINK]->3
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 1--[MLU_LINK]->3--[MLU_LINK]->2
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 3--[MLU_LINK]->2--[MLU_LINK]->6
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 2--[MLU_LINK]->6--[MLU_LINK]->7
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 6--[MLU_LINK]->7--[MLU_LINK]->5
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 7--[MLU_LINK]->5--[MLU_LINK]->4
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 0: 5--[MLU_LINK]->4--[MLU_LINK]->0
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 2--[MLU_LINK]->0--[MLU_LINK]->1
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 0--[MLU_LINK]->1--[MLU_LINK]->5
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 1--[MLU_LINK]->5--[MLU_LINK]->4
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 5--[MLU_LINK]->4--[MLU_LINK]->6
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 4--[MLU_LINK]->6--[MLU_LINK]->7
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 6--[MLU_LINK]->7--[MLU_LINK]->3
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 7--[MLU_LINK]->3--[MLU_LINK]->2
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 1: 3--[MLU_LINK]->2--[MLU_LINK]->0
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 1--[MLU_LINK]->0--[MLU_LINK]->2
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 0--[MLU_LINK]->2--[MLU_LINK]->3
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 2--[MLU_LINK]->3--[MLU_LINK]->7
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 3--[MLU_LINK]->7--[MLU_LINK]->6
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 7--[MLU_LINK]->6--[MLU_LINK]->4
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 6--[MLU_LINK]->4--[MLU_LINK]->5
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 4--[MLU_LINK]->5--[MLU_LINK]->1
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 2: 5--[MLU_LINK]->1--[MLU_LINK]->0
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 1--[MLU_LINK]->0--[MLU_LINK]->4
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 0--[MLU_LINK]->4--[MLU_LINK]->5
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 4--[MLU_LINK]->5--[MLU_LINK]->7
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 5--[MLU_LINK]->7--[MLU_LINK]->6
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 7--[MLU_LINK]->6--[MLU_LINK]->2
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 6--[MLU_LINK]->2--[MLU_LINK]->3
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 2--[MLU_LINK]->3--[MLU_LINK]->1
10.0.2.2: [4-1 14:57:47] [LOG_CNCL] [Info]: Build 4 rings. Ring 3: 3--[MLU_LINK]->1--[MLU_LINK]->0
10.0.2.2: Rank 8 is using scatter from /data/cpm/glm/pile/train.scatter/1
10.0.2.2: configuring data
10.0.2.2: Rank 0 is using scatter from /data/cpm/glm/pile/train.scatter/0
10.0.2.2: Create dataset pile at scatter 1 with 2509316 documents
10.0.2.2: [49, 23573, 78, 1709, 363, 363, 24660, 198, 198, 49, 23573, 78, 1709, 363, 363, 24660, 318, 281, 8200, 1966, 7606, 3785, 1341, 729, 13, 554, 262, 8069, 82, 11, 339, 1839, 262, 2260, 3670, 290, 7997, 8031, 379, 1936, 3180, 52, 24782, 13, 679, 5201, 287, 262, 1353, 17280, 379, 262, 16489, 3427, 24782, 287, 1168, 363, 34806, 13, 198, 198, 9414, 363, 363, 24660, 468, 3111, 355, 257, 6276, 15670, 379, 33051, 24174, 287, 8031, 13, 198, 198, 7293, 17295, 11330, 198, 198, 19927, 220, 198, 198, 27313, 25, 45696, 4257, 2060, 1341, 8605, 198, 27313, 25, 36376, 661, 198, 27313, 25, 17688, 286, 4082, 4814, 357, 19950, 661, 8]
10.0.2.2: b'Rolando Bragaglia\n\nRolando Bragaglia is an Italian former competitive figure skater. In the 1970s, he won the national title and represented Italy at five ISU Championships. He finished in the top fifteen at the 1974 European Championships in Zagreb.\n\nBragaglia has worked as a technical specialist at skating competitions in Italy.\n\nCompetitive highlights\n\nReferences \n\nCategory:Italian male single skaters\nCategory:Living people\nCategory:Year of birth missing (living people)'
10.0.2.2: [25492, 746, 2049, 24207, 271, 357, 11024, 8, 198, 198, 25492, 746, 2049, 24207, 271, 11, 1900, 287, 262, 13316, 355, 38468, 1148, 648, 11, 373, 262, 12841, 19690, 11700, 505, 287, 262, 8830, 8211, 1141, 4751, 290, 38733, 11091, 2465, 287, 14119, 290, 2807, 287, 2932, 286, 326, 614, 13, 383, 24395, 3706, 6388, 286, 262, 1622, 11, 24207, 271, 20973, 422, 281, 1989, 286, 24069, 6193, 326, 4166, 656, 257, 19690, 8862, 26015, 286, 39282, 319, 2932, 1849, 1507, 13, 21131, 6605, 1626, 281, 2858, 4047, 3189, 425, 329, 3767, 19690, 2478, 11, 262, 8862, 32413, 656, 257, 19690, 6388, 257, 1110, 706, 9978, 26, 24207, 271, 373, 17955, 284, 37428, 2049, 3722, 319, 2932, 1849, 1238, 355, 340, 9456, 257, 24821, 1781, 13, 9170, 2383, 11062, 1780, 5087, 11, 262, 37428, 2049, 4251, 9103, 12245, 351, 5415, 12605, 13520, 286, 22538, 1849, 13276, 14, 71, 357, 11623, 1849, 23335, 8, 290, 257, 5288, 2318, 16996, 3833, 286, 47679, 1849, 2022, 283, 357, 71, 28875, 26, 2681, 13, 1558, 1849, 259, 39, 70, 737, 1629, 262, 976, 12245, 24207, 271, 925, 43682, 319, 14119, 262, 1708, 1110, 13, 7945, 663, 1790, 22966, 625, 1956, 11, 24207, 271, 373, 9257, 24135, 11, 290, 925, 257, 4506, 2610, 625, 262, 14119, 41407, 878, 663, 2457, 43682, 319, 2807, 338, 32671, 666, 22783, 1903, 262, 1306, 1110, 13, 26768, 37874, 656, 8774, 1044, 2807, 11, 24207, 271, 2952, 24135, 11, 290, 373, 691, 257, 19690, 8862, 1568, 326, 1110, 26, 262, 8862, 814, 1484, 656, 257, 49332, 1877, 319, 2932, 1849, 1495, 13, 2312, 30468, 18283, 24287, 1973, 262, 12550, 6896, 878, 32008, 803, 287, 262, 6983, 24078, 319, 2932, 1849, 1983, 13, 198, 198, 9171, 13492, 2770, 2106, 198, 198, 464, 18476, 284, 38468, 24207, 271, 2540, 355, 257, 5922, 1989, 286, 24069, 6193, 880, 5366, 12, 82, 14474, 286, 39282, 319, 2932, 1849, 1314, 13, 33530, 3781, 4602, 257, 3154, 1877, 12, 36151, 1989, 3917, 351, 262, 5922, 24748, 596, 13, 12511, 281, 1989, 16443, 286, 19690, 11700, 25908, 11, 262, 30497, 11835, 8389, 290, 32413, 13, 1629, 657, 8054, 1849, 17429, 319, 2932, 1849, 1558, 11, 262, 16798, 38468, 15932, 3337, 1849, 7, 41, 51, 27353, 8, 2540, 1398, 4035, 262, 1080, 355, 257, 19690, 8862, 13, 383, 2869, 25582, 2770, 7732, 1849, 7, 41, 5673, 8, 3940, 6050, 262, 1708, 1110, 379, 24938, 1849, 17429, 13, 37169, 24821, 904, 11, 262, 17070, 3403, 3142, 262, 8862, 284, 2952, 12160, 290, 18188, 17707, 12908, 6290, 21397, 11, 21550, 262, 449, 5673, 284, 8515, 262, 1080, 284, 19690, 6388, 3722, 379, 657, 8054, 1849, 17429, 319, 2932, 1849, 1129, 11, 4145, 1486, 803, 262, 6388, 351, 262, 1438, 24207, 271, 13, 220, 198, 198, 1212, 2278, 286, 5801, 24175, 3767, 3690, 262, 1110, 338, 1781, 11, 351, 262, 1080, 8978, 6049, 19690, 6388, 12245, 14104, 2250, 706, 19264, 290, 788, 37428, 2049, 12245, 379, 24938, 1849, 17429, 319, 2932, 1849, 1238, 13, 5638, 20199, 11210, 19506, 379, 262, 640, 8203, 326, 24207, 271, 9456, 257, 880, 12, 30280, 503, 11125, 3912, 13, 1081, 257, 1255, 286, 663, 5801, 17509, 2649, 11, 24207, 271, 6348, 287, 2546, 11, 46017, 257, 4069, 7226, 286, 19690, 11700, 1952, 37962, 422, 262, 24821, 8211, 937, 36194, 45047, 13, 554, 3090, 11, 262, 37428, 2049, 4166, 281, 4151, 1903, 319, 2932, 1849, 1238, 11, 996, 287, 663, 4238, 9539, 262, 4151, 373, 21388, 287, 5485, 13, 1629, 24938, 1849, 17429, 319, 2932, 1849, 2481, 11, 262, 449, 51, 27353, 5295, 24207, 271, 284, 423, 4251, 2208, 37428, 2049, 12245, 492, 9699, 2250, 1568, 11, 262, 449, 5673, 8203, 326, 262, 37428, 2049, 550, 4251, 663, 9103, 12245, 351, 5415, 12605, 13520, 286, 22538, 1849, 13276, 14, 71, 357, 11623, 1849, 23335, 8, 290, 257, 5288, 2318, 16996, 3833, 286, 47679, 1849, 2022, 283, 357, 71, 28875, 26, 2681, 13, 1558, 1849, 259, 39, 70, 737, 2750, 428, 966, 24207, 271, 6, 4151, 550, 1716, 6279, 12, 5787, 290, 880, 5447, 13, 383, 37428, 2049, 9456, 428, 12245, 351, 1310, 19180, 2288, 329, 7323, 262, 29543, 29095, 2250, 355, 340, 18283, 24821, 13, 12556, 319, 2932, 1849, 1828, 11, 27000, 19506, 8203, 5280, 1173, 1926, 413, 5691, 11, 257, 1051, 286, 257, 1744, 1926, 413, 439, 9014, 6772, 13, 198, 198, 9590, 379, 9103, 12245, 11, 24207, 271, 925, 43682, 319, 262, 7051, 286, 8372, 14119, 379, 36641, 1849, 17429, 319, 2932, 1849, 1828, 13, 13223, 6657, 306, 11, 262, 6388, 338, 5387, 4645, 2540, 284, 905, 5895, 286, 19911, 11, 290, 663, 4151, 2540, 284, 22085, 13, 383, 37428, 2049, 23312, 625, 262, 7022, 11, 290, 355, 884, 9349, 656, 262, 14119, 41407, 645, 2392, 621, 1936, 393, 2237, 2250, 706, 43682, 13, 7945, 262, 1790, 22966, 625, 1956, 11, 24207, 271, 550, 9257, 24135, 11, 290, 373, 691, 257, 10926, 37428, 2049, 2402, 663, 302, 13000, 625, 1660, 13, 2102, 11, 612, 373, 257, 1588, 34466, 1022, 449, 5673, 290, 449, 51, 27353, 12245, 7746, 11, 351, 262, 6846, 9524, 326, 24207, 271, 550, 691, 24135, 4622, 13, 27251, 11, 262, 37428, 2049, 925, 663, 2457, 43682, 17297, 39598, 11, 32671, 666, 379, 657, 6200, 1849, 17429, 319, 2932, 1849, 1731, 13, 770, 9257, 24135, 24207, 271, 11, 290, 1115, 2250, 1568, 262, 449, 5673, 866, 21791, 340, 355, 257, 19690, 6388, 13, 27265, 706, 43682, 11, 262, 34992, 19690, 11700, 505, 2627, 32551, 12, 17529, 560, 655, 37874, 286, 8774, 1044, 2807, 13, 2750, 428, 966, 24748, 596, 373, 38361, 290, 1790, 12, 24489, 11, 290, 379, 24938, 1849, 17429, 262, 449, 5673, 10090, 262, 1080, 355, 257, 19690, 8862, 13, 7486, 262, 449, 51, 27353, 31349, 663, 9904, 4568, 11, 262, 8862, 2540, 284, 7048, 257, 17605, 372, 306, 1781, 319, 2932, 1849, 1731, 13, 1629, 24938, 1849, 17429, 262, 1306, 1110, 11, 262, 8862, 373, 5295, 284, 423, 814, 1484, 656, 257, 49332, 1877, 981, 991, 37874, 13, 24207, 271, 6, 30468, 3767, 284, 2610, 24287, 11, 1973, 262, 12550, 6896, 11, 290, 32008, 515, 319, 2932, 1849, 1983, 8972, 706, 8218, 262, 6983, 24078, 13, 198, 198, 6719, 1845, 602, 290, 2928, 198]
10.0.2.2: b"Typhoon Bilis (2000)\n\nTyphoon Bilis, known in the Philippines as Typhoon Isang, was the strongest tropical cyclone in the western Pacific during 2000 and wrought considerable damage in Taiwan and China in August of that year. The tenth named storm of the season, Bilis originated from an area of disturbed weather that developed into a tropical depression southeast of Guam on August\xc2\xa018. Situated within an environment highly conductive for continued tropical development, the depression intensified into a tropical storm a day after formation; Bilis was upgraded to typhoon status on August\xc2\xa020 as it maintained a northwest course. Without significant inhibiting factors, the typhoon reached peak intensity with maximum sustained winds of 205\xc2\xa0km/h (125\xc2\xa0mph) and a minimum barometric pressure of 920\xc2\xa0mbar (hPa; 27.17\xc2\xa0inHg). At the same intensity Bilis made landfall on Taiwan the following day. Despite its short stint over land, Bilis was greatly weakened, and made a brief track over the Taiwan Strait before its final landfall on China's Fujian Province early the next day. Moving inland into Mainland China, Bilis quickly weakened, and was only a tropical depression later that day; the depression diffused into a remnant low on August\xc2\xa025. These remnants tracked northeast across the Yellow Sea before dissipating in the Korean Peninsula on August\xc2\xa027.\n\nMeteorological history\n\nThe predecessor to Typhoon Bilis began as a developing area of disturbed weather well south-southeast of Guam on August\xc2\xa015. Satellite analysis revealed a broad low-pressure area associated with the developing convection. Within an area supportive of tropical cyclogenesis, the disturbance gradually organized and intensified. At 0600\xc2\xa0UTC on August\xc2\xa017, the Joint Typhoon Warning Center\xc2\xa0(JTWC) began classifying the system as a tropical depression. The Japan Meteorological Agency\xc2\xa0(JMA) followed suit the following day at 1200\xc2\xa0UTC. Tracking northwestward, the favorable conditions allowed the depression to quickly strengthen and attain tightly wrapped rainbands, prompting the JMA to upgrade the system to tropical storm status at 0600\xc2\xa0UTC on August\xc2\xa019, thus designating the storm with the name Bilis. \n\nThis period of rapid strengthening continued throughout the day's course, with the system reaching severe tropical storm intensity twelve hours after naming and then typhoon intensity at 1200\xc2\xa0UTC on August\xc2\xa020. Water vapor satellite imagery at the time indicated that Bilis maintained a well-organized outflow pattern. As a result of its rapid intensification, Bilis grew in size, exhibiting a behavior typical of tropical cyclones originating from the northwest Pacific monsoon trough. In addition, the typhoon developed an eye early on August\xc2\xa020, though in its initial stages the eye was irregular in shape. At 1200\xc2\xa0UTC on August\xc2\xa021, the JTWC determined Bilis to have reached super typhoon intensity.. Six hours later, the JMA indicated that the typhoon had reached its peak intensity with maximum sustained winds of 205\xc2\xa0km/h (125\xc2\xa0mph) and a minimum barometric pressure of 920\xc2\xa0mbar (hPa; 27.17\xc2\xa0inHg). By this point Bilis' eye had become cloud-free and well defined. The typhoon maintained this intensity with little fluctuation for roughly the ensuing eighteen hours as it tracked northwest. Early on August\xc2\xa022, microwave imagery indicated concentric eyewalls, a sign of a possible eyewall replacement cycle.\n\nStill at peak intensity, Bilis made landfall on the coast of southern Taiwan at 1400\xc2\xa0UTC on August\xc2\xa022. Concurrently, the storm's internal structure began to show signs of disruption, and its eye began to shrink. The typhoon accelerated over the island, and as such emerged into the Taiwan Strait no longer than five or six hours after landfall. Despite the short stint over land, Bilis had greatly weakened, and was only a minimal typhoon upon its reentry over water. However, there was a large discrepancy between JMA and JTWC intensity estimates, with the latter suggesting that Bilis had only weakened slightly. Nonetheless, the typhoon made its final landfall Jinjiang, Fujian at 0300\xc2\xa0UTC on August\xc2\xa024. This greatly weakened Bilis, and three hours later the JMA downgraded it as a tropical storm. Shortly after landfall, the weakening tropical cyclone became quasi-stationary just inland of Mainland China. By this point convection was intermittent and short-lived, and at 1200\xc2\xa0UTC the JMA classified the system as a tropical depression. Though the JTWC discontinued its monitoring activities, the depression began to assume a northerly course on August\xc2\xa024. At 1200\xc2\xa0UTC the next day, the depression was determined to have diffused into a remnant low while still inland. Bilis' remnants continued to track northeast, across the Yellow Sea, and dissipated on August\xc2\xa027 shortly after entering the Korean Peninsula.\n\nPreparations and impact\n"
10.0.2.2: [50, 292, 86, 324, 198, 198, 50, 292, 86, 324, 220, 318, 257, 1748, 290, 257, 13474, 6745, 287, 262, 350, 1726, 4783, 286, 262, 3942, 1181, 286, 41624, 13, 23771, 86, 324, 318, 22765, 319, 262, 6341, 286, 9375, 3099, 5866, 13, 23771, 86, 324, 318, 655, 546, 1542, 1849, 13276, 422, 350, 1726, 9327, 13, 198, 198, 18122, 198, 50, 292, 86, 324, 318, 257, 1295, 351, 257, 890, 2106, 13, 632, 3830, 319, 262, 6156, 3292, 6339, 14320, 262, 17475, 17431, 27541, 14090, 284, 262, 1024, 535, 272, 987, 12706, 13, 6363, 2524, 319, 257, 3292, 6339, 925, 340, 257, 3641, 329, 20838, 937, 1603, 444, 13, 383, 6072, 324, 5303, 393, 262, 19186, 1295, 286, 262, 1511, 400, 4289, 569, 668, 2743, 10844, 35643, 272, 318, 5140, 287, 262, 3240, 13, 383, 5079, 978, 26800, 284, 16492, 71, 5117, 333, 1810, 72, 220, 286, 360, 3281, 272, 5069, 5767, 350, 971, 5303, 9911, 287, 262, 3240, 13, 11450, 340, 373, 1900, 355, 262, 3240, 379, 262, 2366, 286, 9566, 9330, 392, 283, 6285, 13, 383, 717, 39654, 10247, 422, 262, 347, 5183, 1641, 11, 8528, 26436, 36900, 8149, 776, 550, 465, 2779, 287, 262, 3240, 13, 2399, 3367, 11, 347, 1228, 343, 5488, 314, 3888, 465, 2779, 287, 1596, 1238, 284, 350, 1726, 290, 2900, 326, 1295, 656, 257, 1588, 1748, 13, 5856, 262, 1248, 400, 4289, 11, 23771, 86, 324, 373, 262, 5852, 286, 262, 9330, 392, 533, 1641, 11, 49718, 27536, 286, 262, 39654, 10247, 13, 383, 1641, 338, 783, 37550, 6340, 34961, 24141, 373, 281, 20814, 6504, 1141, 262, 2739, 279, 5069, 10247, 6980, 13, 198, 198, 818, 262, 1160, 400, 4289, 11, 15889, 3554, 1526, 44202, 6260, 290, 26479, 317, 620, 43898, 1629, 260, 373, 4642, 612, 13, 679, 373, 530, 286, 262, 2766, 286, 262, 3409, 88, 2724, 8326, 41624, 15477, 13, 2399, 2168, 286, 3835, 319, 262, 3356, 13, 48569, 13, 9375, 258, 2395, 350, 3216, 28225, 243, 11976, 109, 24231, 235, 11976, 117, 24231, 229, 11976, 248, 24231, 229, 28225, 103, 48077, 11976, 96, 24231, 222, 2727, 24149, 17128, 1295, 287, 262, 2106, 286, 1526, 44202, 3303, 13, 198, 198, 11522, 24188, 198, 198, 1722, 286, 2813, 11, 23771, 86, 324, 550, 257, 3265, 286, 352, 11, 3132, 11, 23, 2481, 13, 43404, 15613, 6740, 4, 286, 262, 3265, 290, 12366, 4764, 7225, 23771, 86, 324, 468, 281, 2811, 31231, 2494, 286, 7600, 7441, 2440, 621, 262, 2260, 2811, 286, 7863, 13, 20, 48529, 4257, 31231, 373, 4101, 7441, 290, 4048, 31231, 373, 4019, 7225, 554, 23771, 86, 324, 11, 1105, 4, 286, 262, 3265, 373, 739, 718, 13, 198, 198, 10082, 4867, 220, 198, 15056, 663, 20387, 284, 350, 1726, 11, 23771, 86, 324, 743, 1716, 636, 286, 350, 1726, 27953, 1989, 13, 383, 3240, 318, 655, 1679, 1849, 13276, 1497, 422, 350, 1726, 1748, 290, 1160, 2431, 422, 11161, 1686, 283, 13, 383, 412, 11404, 357, 41861, 9330, 392, 283, 8, 1748, 326, 318, 739, 5103, 318, 838, 1849, 13276, 422, 23771, 86, 324, 290, 2944, 5117, 25014, 2254, 11, 317, 9038, 5799, 8765, 2254, 389, 1679, 1849, 13276, 422, 23771, 86, 324, 13, 23771, 86, 324, 4394, 867, 19592, 290, 9735, 42665, 13, 23771, 86, 324, 318, 262, 2779, 329, 9692, 284, 6716, 509, 4993, 19981, 12505, 379, 3852, 73, 9900, 11, 262, 304, 74, 12, 76, 2724, 5303, 360, 25014, 12505, 379, 13596, 22931, 14225, 220, 290, 262, 9566, 9330, 392, 283, 6285, 13, 632, 318, 4750, 355, 262, 11566, 286, 39654, 10247, 347, 1228, 343, 5488, 287, 262, 3195, 19533, 39654, 10247, 347, 1228, 343, 5488, 13, 198, 198, 8291, 634, 198, 47, 1726, 1748, 16893, 357, 5868, 47, 5805, 8, 1057, 2594, 284, 23771, 86, 324, 422, 11161, 1686, 283, 11, 2451, 853, 378, 290, 350, 1726, 16835, 9327, 13, 383, 3240, 468, 663, 898, 3563, 43369, 284, 890, 5253, 23982, 13, 198, 198, 50, 292, 86, 324, 318, 1969, 284, 3852, 73, 9900, 20515, 4429, 351, 2594, 284, 350, 1726, 290, 284, 23982, 5366, 884, 355, 7381, 1228, 13, 198, 198, 47, 1726, 9003, 318, 2026, 12, 1899, 949, 7, 8869, 5311, 283, 9189, 17286, 8, 1497, 13, 198, 198, 41183, 198, 50, 292, 86, 324, 468, 9856, 6712, 326, 20825, 329, 262, 2476, 286, 1180, 9337, 290, 9004, 286, 262, 3592, 13, 18087, 4266, 389, 1057, 416, 262, 13474, 6745, 11, 867, 17092, 1057, 4165, 290, 9233, 4266, 326, 6011, 12064, 287, 1526, 44202, 290, 3594, 11, 290, 4302, 4365, 1057, 4266, 6011, 12064, 287, 3594, 13, 383, 3240, 11453, 15273, 6011, 4922, 1241, 10902, 13, 1052, 7283, 40, 4394, 48368, 3047, 13, 198, 198, 26130, 82, 198, 42, 34183, 778, 1077, 6081, 198, 44, 13, 36, 13, 50, 370, 10471, 557, 1029, 1524, 357, 11976, 106, 13, 11976, 237, 13, 11976, 116, 24231, 233, 13, 11976, 113, 48077, 11976, 246, 24231, 222, 11976, 108, 24231, 229, 28225, 113, 11976, 123, 11976, 99, 24231, 235, 11976, 107, 48077, 11976, 110, 11976, 107, 8, 198, 30026, 392, 283, 1029, 1524, 198, 2484, 452, 26436, 3594, 13398, 198, 38, 333, 2724, 377, 4165, 1524, 198, 33, 620, 6839, 4165, 1524, 198, 1273, 13, 7212, 3594, 7090, 198, 198, 5216, 1455, 274, 198, 30026, 392, 283, 8705, 4152, 198, 2484, 452, 26436, 360, 6, 276, 4152, 198, 54, 10471, 557, 5535, 198, 30026, 392, 283, 4152, 198, 3347, 400, 371, 10471, 403, 776, 9910, 1000, 4152, 286, 19234, 1590, 12, 23771, 86, 324, 198, 198, 35848, 563, 198, 198, 22229, 284, 9025, 287, 4839, 11, 867, 41624, 19034, 7712, 10501, 7593, 14860, 389, 1088, 350, 1726, 357, 47, 320, 3448, 12, 1925, 8589, 86, 324, 25269, 34, 11, 46620, 272, 25269, 34, 11, 23075, 73, 16484, 261, 25269, 34, 290, 3852, 73, 9900, 25269, 34, 389, 1969, 1576, 329, 45309, 422, 23771, 86, 324, 13, 198, 198, 19927, 198, 198, 41506, 6117, 198, 198, 50, 292, 86, 324, 13524, 5119, 198, 198, 27313, 25, 34, 871, 290, 11684, 287, 350, 1726, 4783, 198, 27313, 25, 44, 37325, 38535]
10.0.2.2: b"Saswad\n\nSaswad  is a city and a municipal council in the Pune district of the Indian state of Maharashtra. Saswad is situated on the banks of Karha River. Saswad is just about 30\xc2\xa0km from Pune Station.\n\nHistory\nSaswad is a place with a long history. It lay on the ancient trade route connecting the coastal Konkan ports to the Deccan interiors. Its site on a trade route made it a center for Buddhist monasteries. The samadhi or the resting place of the 13th century Varkari Sant Sopan is located in the town. The annual Alandi to Pandharpur Wari  of Dnyaneshwar Palkhi stops in the town. Later it was known as the town at the foot of historic Purandar fort. The first Peshwa from the Bhat family, Balaji Vishwanath had his base in the town. His son, Bajirao I moved his base in 1720 to Pune and turned that place into a large city. During the 18th century, Saswad was the seat of the Purandare family, hereditary knights of the Peshwa. The family's now crumbling palatial mansion was an imposing sight during the late peshwa era.\n\nIn the 20th century, socialist leader Marathi writer and filmmaker Aacharya Atre was born there. He was one of the leaders of the Samyukta Maharashtra Movement. His series of books on the movement. viz. Karheche Pani \xe0\xa4\x95\xe0\xa4\xb1\xe0\xa5\x8d\xe0\xa4\xb9\xe0\xa5\x87\xe0\xa4\x9a\xe0\xa5\x87 \xe0\xa4\xaa\xe0\xa4\xbe\xe0\xa4\xa3\xe0\xa5\x80 created undisputed place in the history of Marathi language.\n\nDemographics\n\nAs of 2011, Saswad had a population of 1,31,821. Males constitute 52% of the population and females 48%. Saswad has an average literacy rate of 85%, higher than the national average of 59.5%: male literacy was 90%, and female literacy was 80%. In Saswad, 12% of the population was under 6.\n\nGeography \nGiven its proximity to Pune, Saswad may become part of Pune metropolitan area. The town is just 25\xc2\xa0km away from Pune city and 20 minutes from Hadapsar. The Eotte (Royal Purandar) city that is under construction is 10\xc2\xa0km from Saswad and Magarpatta City, Amannora Tower City are 25\xc2\xa0km from Saswad. Saswad offers many apartments and shopping complexes. Saswad is the base for visitors to nearby Khandoba temple at Jejuri, the ek-mukhi Datta temple at Narayanpur  and the historic Purandar fort. It is mentioned as the residence of Peshwa Bajirao in the TV soap Peshwa Bajirao.\n\nTransport\nPune city buses (PMPML) run services to Saswad from Hadapsar, Swargate and Pune Train Station. The town has its own ST depot to long distance destinations.\n\nSaswad is close to Jejuri railway station with services to Pune and to destinations south such as Miraj.\n\nPune airport is 50-60 min(via Kharadi bypass) away.\n\nEducation\nSaswad has educational institutions that cater for the needs of different ages and sections of the society. Eight schools are run by the municipal council, many privately run primary and secondary schools that offering instruction in Marathi and English, and Christian mission run schools offering instruction in English. The town hosts colleges offering degree level courses. An ITI offers vocational training.\n\nSchools\nKanya prashala\nM.E.S Waghire high school (\xe0\xa4\xae.\xe0\xa4\x8f.\xe0\xa4\xb8\xe0\xa5\x8b.\xe0\xa4\xb5\xe0\xa4\xbe\xe0\xa4\x98\xe0\xa5\x80\xe0\xa4\xb0\xe0\xa5\x87 \xe0\xa4\xb5\xe0\xa4\xbf\xe0\xa4\xa6\xe0\xa5\x8d\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\xb2\xe0\xa4\xaf)\nPurandar high school\nShivaji English Medium\nGurukul primary school\nBachpan primary school\nSt. Joseph English medium\n\nColleges\nPurandar engineering college\nShivaji D'ed college\nWaghire College\nPurandar college\nSheth Raghunath Sabale college of Pharmacy- Saswad\n\nIndustry\n\nDue to improvement in transport, many Maharashtra Industrial Development Corporation industrial parks are around Pune (Pimpri-Chinchwad MIDC, Chakan MIDC, Ranjangaon MIDC and Jejuri MIDC are close enough for commuting from Saswad.\n\nReferences\n\nExternal links\n\nSaswad Travelogue\n\nCategory:Cities and towns in Pune district\nCategory:Maharashtra"
10.0.2.2: [11712, 510, 284, 651, 510, 284, 1542, 4, 572, 8587, 1222, 4568, 0, 198, 198, 2061, 4380, 4231, 49642, 7994, 9070, 19077, 198, 198, 45675, 835, 286, 4547, 262, 19077, 198, 198, 1135, 550, 281, 6275, 5698, 508, 4193, 514, 1833, 262, 24862, 286, 6832, 326, 787, 510, 262, 19077, 13, 4900, 407, 636, 286, 674, 4205, 673, 4193, 514, 5713, 23752, 88, 543, 356, 750, 262, 976, 6672, 351, 257, 845, 922, 6597, 5698, 13, 198, 198, 45675, 4205, 780, 286, 11660, 5698, 360, 14715, 13, 198, 198, 464, 19077, 318, 13899, 11, 290, 262, 4205, 5698, 11, 360, 14715, 11, 373, 3737, 262, 1266, 5698, 314, 423, 1683, 12956, 25, 4457, 29549, 11, 673, 19018, 750, 407, 2128, 588, 257, 3375, 28979, 11, 475, 550, 607, 898, 15347, 9317, 13, 9576, 32154, 26, 6275, 3594, 13, 37339, 3131, 661, 262, 5698, 550, 407, 2938, 2900, 510, 379, 262, 938, 2589, 26, 484, 550, 21765, 2884, 257, 1323, 1664, 290, 340, 1718, 4153, 2431, 329, 262, 5698, 290, 607, 4706, 284, 3297, 262, 2085, 503, 13, 383, 2706, 2950, 761, 284, 651, 262, 1492, 654, 517, 2748, 13, 198, 198, 45675, 1231, 852, 5194, 10676, 351, 13079, 6419, 1222, 5538, 198, 198, 464, 4205, 373, 3446, 262, 826, 5236, 286, 1321, 290, 5086, 514, 284, 1011, 287, 262, 27508, 290, 21343, 6731, 13, 383, 4414, 286, 257, 2614, 5698, 318, 326, 345, 651, 6235, 287, 262, 826, 4571, 286, 262, 1994, 27508, 11, 14590, 852, 1297, 11589, 546, 262, 1854, 287, 1339, 345, 765, 284, 423, 257, 4506, 804, 379, 606, 355, 880, 13, 198, 3152, 523, 881, 284, 766, 340, 318, 1593, 284, 423, 2130, 351, 262, 3725, 284, 5698, 345, 832, 262, 3236, 1271, 286, 27508, 1222, 21343, 13, 1675, 264, 2135, 428, 1321, 329, 3511, 422, 257, 5698, 2070, 561, 1011, 8097, 3228, 198, 198, 16371, 3499, 284, 766, 2641, 262, 19077, 351, 257, 29549, 5698, 198, 198, 16371, 881, 8359, 674, 4205, 286, 262, 19077, 1989, 3805, 262, 23147, 6290, 13, 5686, 1437, 373, 281, 6275, 5698, 13, 632, 373, 523, 3499, 284, 766, 2641, 262, 1115, 25322, 378, 3797, 704, 30691, 351, 7714, 5017, 351, 34093, 1073, 274, 13, 22151, 356, 2497, 262, 995, 338, 4094, 21202, 290, 21202, 11333, 290, 262, 3236, 40379, 283, 8966, 13]
10.0.2.2: b"Sign up to get up to 30% off tickets & activities!\n\nWhat People Are Saying About Moscow Kremlin\n\nExcellent way of understanding the Kremlin\n\nWe had an excellent guide who helped us understand the myriad of buildings that make up the Kremlin. Although not part of our tour she helped us secure Armoury which we did the same afternoon with a very good audio guide.\n\nExcellent tour because of outstanding guide Dasha.\n\nThe Kremlin is fascinating, and the tour guide, Dasha, was perhaps the best guide I have ever encountered: extremely knowledgeable, she nonetheless did not sound like a talking textbook, but had her own passionate opinions. Very articulate; excellent English. Eleven extra people the guide had not expected turned up at the last moment; they had booked via a bus company and it took 45 minutes for the guide and her manager to sort the mess out. The companies involved need to get the bookings more exact.\n\nExcellent without being bombarded with endless facts & figures\n\nThe tour was exactly the right balance of information and allowing us to take in the exhibits and sights ourselves. The benefit of a personal guide is that you get pointed in the right direction of the key exhibits, whilst being told briefly about the others in case you want to have a brief look at them as well.\nWith so much to see it is important to have someone with the knowledge to guide you through the huge number of exhibits & sights. To sift this information for yourself from a guidebook would take forever!!\n\nVery interesting to see inside the Kremlin with a knowledgeable guide\n\nVery much enjoyed our tour of the Kremlin area despite the pouring rain. Irina was an excellent guide. It was so interesting to see inside the three ornate cathedrals with walls covered with frescoes. Outside we saw the world's biggest cannon and cannon balls and the huge tsar bell."
10.0.2.2: [13838, 338, 19502, 198, 198, 2202, 262, 21432, 33068, 3052, 11, 618, 24353, 281, 2708, 393, 1242, 11, 345, 460, 787, 257, 2912, 319, 534, 670, 13, 10127, 340, 338, 546, 644, 7867, 262, 1621, 14, 433, 3704, 393, 703, 262, 1772, 14, 49016, 5300, 546, 340, 11, 777, 3651, 2614, 1096, 262, 14498, 13, 198, 198, 3633, 617, 7035, 6, 3651, 389, 366, 5492, 2494, 290, 2912, 2474, 357, 40, 760, 345, 423, 531, 428, 11, 836, 470, 6486, 3926, 314, 423, 1165, 828, 749, 389, 546, 12141, 393, 4648, 3736, 13, 2312, 389, 3499, 290, 1593, 329, 262, 9173, 11, 1201, 1690, 31888, 423, 2614, 3923, 22020, 42200, 606, 393, 281, 12531, 1242, 3704, 743, 307, 257, 6194, 329, 366, 27404, 3469, 290, 34689, 262, 11397, 526, 2312, 3651, 1037, 7183, 1833, 644, 262, 16294, 547, 3612, 618, 484, 18025, 511, 5207, 13, 198, 198, 13898, 11, 7035, 6, 3651, 389, 691, 1695, 319, 262, 3052, 26, 645, 2272, 318, 2810, 329, 606, 287, 262, 7093, 13, 2893, 617, 743, 761, 284, 307, 356, 15395, 503, 11, 867, 7035, 6, 3651, 389, 20050, 290, 1037, 514, 1833, 262, 3704, 1365, 13, 314, 892, 21432, 33068, 815, 2074, 4375, 428, 284, 262, 7093, 13, 198, 198, 18234, 262, 27766, 198, 198, 40, 1224, 83, 306, 4987, 764, 40, 466, 326, 257, 1256, 475, 1312, 5465, 340, 618, 340, 4325, 319, 616, 5207, 780, 1312, 651, 6568, 326, 1312, 423, 257, 2912, 290, 788, 340, 338, 588, 366, 1219, 71, 16317, 77, 3008, 13, 22324, 508, 655, 16609, 546, 2405, 526, 543, 318, 17855, 8258, 269, 10277, 356, 466, 262, 976, 1517, 16317, 271, 77, 470, 340, 1464, 326, 345, 5465, 262, 1517, 326, 345, 466, 618, 345, 766, 340, 287, 584, 661, 30, 616, 5770, 198, 198, 5779, 11, 1682, 644, 314, 1101, 3375, 546, 318, 11, 345, 760, 703, 572, 284, 286, 617, 6685, 14, 433, 5207, 612, 389, 50056, 290, 340, 1139, 1772, 338, 3651, 30, 1320, 338, 644, 314, 1101, 3375, 546, 11, 407, 262, 3651, 326, 584, 661, 787, 319, 534, 670, 13, 921, 760, 11, 588, 262, 35940, 4710, 286, 257, 2647, 5062, 13]
10.0.2.2: b'Author\'s Comments\n\nOn the Teen Ink website, when submitting an article or art, you can make a comment on your work. Whether it\'s about what inspired the story/art piece or how the author/artist feels about it, these comments personalize the submission.\n\nWhile some authors\' comments are "Please rate and comment!" (I know you have said this, don\'t lie \xe2\x80\xa6 I have too), most are about inspiration or dedications. These are interesting and important for the reader, since often poems have personal stories \xc2\xadbehind them or an abstract art piece may be a symbol for "Going Green and Saving the Planet." These comments help readers understand what the creators were thinking when they crafted their pieces.\n\nUnfortunately, authors\' comments are only available on the website; no space is provided for them in the magazine. While some may need to be weeded out, many authors\' comments are enjoyable and help us understand the piece better. I think Teen Ink should consider adding this to the magazine.\n\nJoin the Discussion\n\nI completly agreed .I do that a lot but i hate it when it happens on my pieces because i get excited that i have a comment and then it\'s like "ohh......nope. idiot who just cares about themselves." which is kinda funny cuz we do the same thing......isn\'t it always that you hate the thing that you do when you see it in other people? my god\n\nWell, actually what I\'m talking about is, you know how off to of some articles/art pieces there are quotations and it says author\'s comments? That\'s what I\'m talking about, not the comments that other people make on your work. You know, like the liner notes of a music album.'
10.0.2.2: [10258, 198, 5446, 3185, 25241, 198, 198, 10699, 198, 55, 50, 198, 198, 50, 24080, 6462, 24530, 25491, 3970, 198, 198, 3, 13348, 198, 198, 10258, 198, 5446, 3185, 25241, 198, 198, 10699, 198, 50, 198, 198, 50, 24080, 6462, 24530, 25491, 3970, 198, 198, 3, 13348, 198, 198, 10258, 198, 5446, 3185, 25241, 198, 198, 10699, 198, 44, 198, 198, 50, 24080, 6462, 24530, 25491, 3970, 198, 198, 3, 13348, 198, 198, 10258, 198, 5446, 3185, 25241, 198, 198, 10699, 198, 43, 198, 198, 33796, 5740, 25, 383, 12578, 1345, 5592, 1550, 3887, 35953, 6063, 925, 351, 262, 7579, 3185, 25241, 12578, 481, 307, 360, 5064, 24302, 3525, 422, 262, 1306, 13, 48221, 870, 326, 790, 3704, 318, 4988, 530, 12, 1659, 12, 64, 12, 11031, 13, 1002, 345, 765, 1223, 645, 530, 468, 11, 428, 6050, 318, 329, 345, 13, 198, 198, 24616, 284, 8284, 34844, 362, 12, 20, 12579, 2293, 27637, 13, 198, 198, 25207, 290, 3562, 416, 1466, 329, 1466, 287, 262, 1294, 198, 198, 464, 15638, 13713, 347, 35542, 318, 674, 256, 5362, 395, 45810, 284, 3128, 13, 632, 338, 477, 546, 17144, 21654, 13, 383, 4220, 318, 257, 294, 506, 13, 6822, 503, 674, 35336, 329, 1180, 2842, 284, 9839, 0, 4222, 3465, 326, 428, 45810, 11414, 4622, 1180, 422, 262, 15638, 13713, 347, 35542, 422, 2180, 812, 13, 770, 45810, 11414, 2081, 284, 2546, 13, 198, 198, 45, 15158, 14, 4561, 392, 1069, 12885, 13502, 4692, 11, 5894, 6228, 198, 198, 17633, 287, 2042, 318, 5762, 257, 2546, 1402, 1353, 1222, 1402, 4220, 13, 23106, 25, 642, 6, 940, 1, 36988, 25, 4974, 34, 15329, 396, 25, 2681, 29437, 25, 4353, 198, 198, 17633, 287, 32630, 318, 5762, 257, 2546, 1402, 1353, 1222, 1402, 4220, 13, 23106, 25, 642, 6, 24, 1, 36988, 25, 4974, 33, 15329, 396, 25, 1679, 29437, 25, 5214, 198, 198, 17633, 287, 1125, 316, 993, 290, 25988, 318, 5762, 257, 2546, 1402, 1353, 1222, 1402, 4220, 13, 23106, 25, 642, 6, 23, 1, 36988, 25, 4747, 34, 15329, 396, 25, 1679, 29437, 25, 5214, 198, 198, 5492, 3465, 25, 1439, 19745, 3709, 11, 1390, 3709, 8155, 351, 257, 30879, 2438, 389, 25261, 311, 21358, 13, 1400, 1475, 11755, 13, 198, 198, 33, 436, 198, 198, 16818, 9154, 3953, 739, 5101, 11, 1973, 8163, 20784, 379, 736, 1088, 40830, 636, 286, 13076, 379, 2166, 13, 46883, 5101, 379, 5389, 981, 5291, 9154, 10730, 351, 4314, 13, 770, 318, 534, 13076, 15558, 13, 1002, 428, 15558, 8953, 319, 262, 25208, 11111, 11, 2835, 510, 13, 198, 198, 33484, 396, 198, 198, 33, 437, 284, 530, 1735, 284, 1064, 3288, 33793, 341, 287, 28668, 13, 770, 318, 534, 3288, 16139, 13, 5660, 9154, 1088, 3288, 16139, 1370, 11, 5291, 9154, 10730, 351, 4314, 290, 530, 7660, 1022, 1767, 290, 9154, 329, 257, 517, 6792, 4197, 13, 198, 198, 39, 541, 198, 198, 15480, 319, 257, 1241, 4417, 351, 3625, 1978, 13, 24291, 1088, 40830, 636, 286, 21497, 290, 4220, 357, 47498, 767, 26793, 24, 1, 2174, 16139, 1370, 828, 5291, 9154, 10730, 351, 4314, 13, 198, 198, 1532, 534, 2378, 318, 5610, 355, 14446, 1675, 8284, 13, 4222, 1249, 362, 12, 22, 43949, 44180, 24644, 50, 329, 7587, 38331, 262, 2378, 318, 14338, 357, 732, 466, 407, 4074, 41703, 11, 32714, 11, 393, 17122, 737, 1114, 1672, 11, 611, 345, 1502, 674, 5416, 5978, 347, 35542, 290, 2922, 352, 3596, 24147, 11, 340, 481, 1011, 6609, 1022, 362, 12, 22, 1528, 284, 1429, 290, 788, 534, 3709, 481, 307, 14338, 1262, 352, 1110, 8440, 13, 4377, 6266, 326, 389, 8155, 625, 262, 5041, 481, 307, 13686, 262, 1708, 3321, 13, 3406, 1502, 481, 307, 14338, 2884, 44640, 34416, 11099, 4556, 345, 2922, 281, 5559, 13, 1002, 345, 423, 597, 2683, 546, 534, 1502, 3387, 869, 357, 23, 2598, 8, 34091, 12, 2425, 2231]
10.0.2.2: b'Color\nTROPICA\n\nSize\nXS\n\nSiren Full Bottom Tropica\n\n$105\n\nColor\nTROPICA\n\nSize\nS\n\nSiren Full Bottom Tropica\n\n$105\n\nColor\nTROPICA\n\nSize\nM\n\nSiren Full Bottom Tropica\n\n$105\n\nColor\nTROPICA\n\nSize\nL\n\nImportant Note: The Print Placement On Every Swimsuit made with the TROPICA Print will be DIFFERENT from the next. Ensuring that every piece is truly one-of-a-kind. If you want something no one has, this suit is for you.\n\nMade to Order Ships 2-5 Days After Purchase.\n\nConstructed and designed by women for women in the US\n\nThe Sienna Bikini is our tiniest bikini to date. It\'s all about sexy simplicity. The bottom is a thong. Check out our IG for different ways to tie! Please note that this bikini fits slightly different from the Sienna Bikini from previous years. This bikini fits true to size.\n\nNylon/SpandexHand wash cold, dry flat\n\nModel in black is wearing a size small top & small bottom.Height: 5\'10" Bust: 34C Waist: 27 Hip: 38\n\nModel in ivory is wearing a size small top & small bottom.Height: 5\'9" Bust: 34B Waist: 25 Hip: 37\n\nModel in cheetah and neon is wearing a size small top & small bottom.Height: 5\'8" Bust: 33C Waist: 25 Hip: 37\n\nPlease note: All clearance items, including items purchased with a promo code are FINAL SALE. No Exceptions.\n\nBust\n\nThread tape measure under arms, across shoulder blades at back around fullest part of bust at front. Relax arms at sides while keeping tape parallel with floor. This is your bust measurement. If this measurement falls on the \xc2\xbd inch, round up.\n\nWaist\n\nBend to one side to find natural indentation in torso. This is your natural waist. Run tape around natural waistline, keeping tape parallel with floor and one finger between body and tape for a more comfortable fit.\n\nHip\n\nStand on a level surface with feet together. Measure around fullest part of hips and bottom (approximately 7"-9" below waistline), keeping tape parallel with floor.\n\nIf your item is listed as Made To Order. Please allow 2-7 BUSINESS DAYS for processing BEFORE the item is shipped (we do not ship Saturdays, Sundays, or holidays). For example, if you order our Revolt Bikini and select 1 Day Shipping, it will take anywhere between 2-7 days to process and then your items will be shipped using 1 day shipping. Any orders that are purchased over the weekend will be processed the following Monday. Your order will be shipped via USPS Priority Mail unless you select an alternative. If you have any questions about your order please call (844) 460-7545'
10.0.2.2: [38, 6996, 18751, 198, 198, 38, 12573, 12155, 366, 38, 6996, 1, 18751, 357, 6286, 1542, 2795, 24648, 8, 318, 281, 6638, 14971, 13, 679, 318, 257, 7882, 2888, 286, 262, 8342, 25219, 30356, 10006, 11, 1719, 2714, 465, 5852, 286, 20511, 306, 1201, 262, 3648, 2276, 3071, 13, 679, 468, 587, 4139, 329, 16797, 290, 8108, 7712, 11, 4139, 329, 36294, 6168, 290, 4139, 329, 5094, 24656, 287, 262, 6748, 1008, 9475, 1201, 2693, 1584, 13, 198, 198, 9742, 9914, 20057, 635, 2714, 1811, 45933, 287, 3362, 22016, 338, 13447, 25, 319, 860, 3945, 3717, 339, 5399, 262, 13447, 355, 4139, 329, 19940, 290, 4139, 329, 48371, 6168, 13, 1550, 718, 2932, 11, 339, 8618, 262, 3224, 47837, 286, 11536, 290, 32887, 5700, 11, 14017, 18968, 45825, 11, 6960, 18968, 45825, 11, 290, 4139, 2195, 9665, 262, 5953, 4139, 319, 7854, 26823, 10665, 290, 7868, 13, 3574, 604, 3426, 3717, 284, 262, 7433, 286, 262, 22016, 1230, 379, 262, 2321, 8342, 25219, 3071, 11, 339, 373, 4139, 329, 24182, 290, 21913, 11, 19940, 11, 20395, 11, 48371, 6168, 11, 290, 11536, 290, 32887, 5700, 13, 198, 198, 19927, 198, 198, 27313, 25, 1129, 3365, 27244, 198, 27313, 25, 36376, 661, 198, 27313, 25, 25341, 286, 262, 8342, 25219, 30356, 10006, 198, 27313, 25, 38036, 7882, 3615, 1866, 286, 262, 8342, 25219, 30356, 10006, 198, 27313, 25, 2481, 301, 12, 14792, 6638, 7602]
10.0.2.2: b'Gerry McCarthy\n\nGerald Francis "Gerry" McCarthy (born 30 June 1958) is an Australian politician. He is a Labor member of the Northern Territory Legislative Assembly, having held his seat of Barkly since the 2008 general election. He has been Minister for Housing and Community Development, Minister for Essential Services and Minister for Public Employment in the Gunner Ministry since September 2016.\n\nMcCarthy also held several ministries in Paul Henderson\'s cabinet: on 9 February 2009 he joined the cabinet as Minister for Transport and Minister for Correctional Services. On 6 August, he gained the additional portfolios of Arts and Museums, Senior Territorians, Young Territorians, and Minister Assisting the Chief Minister on Multicultural Affairs and Education. From 4 December 2009 to the defeat of the Henderson government at the 2012 Northern Territory election, he was Minister for Lands and Planning, Transport, Construction, Correctional Services, and Arts and Museums.\n\nReferences\n\nCategory:1958 births\nCategory:Living people\nCategory:Members of the Northern Territory Legislative Assembly\nCategory:Australian Labor Party members of the Northern Territory Legislative Assembly\nCategory:21st-century Australian politicians'
10.0.2.2: [50, 2978, 1010, 290, 9992, 2628, 4839, 11206, 540, 6844, 290, 37793, 1973, 262, 1499, 790, 1110, 287, 7176, 286, 8914, 517, 3160, 13, 27213, 11, 287, 262, 1339, 286, 1440, 4044, 6844, 290, 1105, 37793, 11, 644, 815, 423, 587, 257, 3772, 7464, 287, 649, 5682, 468, 1716, 257, 17123, 13, 198, 198, 13689, 428, 1227, 11, 262, 6844, 290, 37793, 550, 587, 9272, 422, 2972, 7064, 287, 262, 25701, 290, 4624, 1978, 287, 257, 7779, 329, 3067, 284, 10140, 329, 12695, 13, 833, 39066, 11, 262, 6811, 42178, 750, 407, 1061, 33078, 5044, 8435, 13, 2448, 262, 2732, 286, 19717, 11, 477, 6844, 290, 11875, 329, 5466, 393, 12695, 1276, 307, 11557, 329, 257, 5288, 4764, 2250, 788, 11068, 290, 16293, 5448, 416, 257, 11971, 41404, 878, 9358, 13, 198, 198, 1212, 15662, 8724, 287, 262, 4104, 286, 262, 4047, 43944, 1582, 85, 709, 19397, 11, 262, 7195, 290, 1918, 286, 1440, 37793, 11, 290, 281, 9812, 10538, 319, 262, 3085, 290, 3176, 4133, 286, 337, 4303, 8141, 12, 31710, 13, 198, 198, 464, 11772, 4477, 284, 1337, 329, 262, 13644, 11, 508, 3520, 287, 48827, 13, 36845, 423, 1541, 20672, 4138, 286, 5054, 13, 1002, 345, 561, 588, 284, 16565, 393, 4691, 355, 257, 17016, 2560, 11, 3387, 467, 284, 337, 4303, 8141, 13, 198, 198, 1639, 460, 635, 1037, 416, 22868, 257, 4273, 287, 534, 898, 2055, 11, 523, 4695, 645, 2392, 423, 284, 307, 18665, 1049, 18868, 287, 1502, 284, 1064, 5682, 13, 4222, 2800, 534, 1957, 11772, 393, 9992, 1448, 832, 4767, 22805, 284, 1064, 503, 703, 345, 460, 1037, 13, 198, 198, 16980, 544, 12670, 893, 89, 15016, 11, 4870, 286, 15899, 1550, 509, 24, 7092, 290, 14329, 5464, 379, 383, 20511, 11, 318, 262, 1772, 286, 3294, 968, 12255, 3067, 17555, 11, 1390, 3574, 647, 338, 3791, 12255, 3596, 416, 3596, 357, 18, 4372, 5061, 737, 2332, 670, 468, 635, 4120, 287, 383, 968, 1971, 3782, 11175, 11, 7695, 1039, 290, 37732, 290, 43403, 18168, 13]
10.0.2.2: b"Shelters and rescue groups transport adoptable dogs and puppies across the country every day in hopes of saving more lives. Sadly, in the case of four adult dogs and 12 puppies, what should have been a happy ending in new homes has become a nightmare.\n\nEarlier this month, the dogs and puppies had been gathered from various locations in the Midwest and placed together in a truck for travel to Massachusetts for adoption. Tragically, the rescuers did not follow interstate animal protocol. Per the Department of Agriculture, all dogs and cats for sale or adoption must be isolated for a minimum 48 hours then examined and pronounced healthy by a licensed veterinarian before transportation.\n\nThis oversight resulted in the spread of the highly contagious parvovirus, the suffering and death of four puppies, and an enormous burden on the staff and financial resources of MSPCA-Boston.\n\nThe shelter continues to care for the survivors, who remain in quarantine. Costs have already exceeded thousands of dollars. If you would like to donate or serve as a foster parent, please go to MSPCA.\n\nYou can also help by adopting a pet in your own community, so animals no longer have to be transported great distances in order to find homes. Please contact your local shelter or rescue group through Petfinder to find out how you can help.\n\nJulia Kamysz Lane, owner of Spot On K9 Sports and contributing editor at The Bark, is the author of multiple New Orleans travel guides, including Frommer'sNew Orleans Day by Day (3rd Edition). Her work has also appeared in The New York Times Magazine, Poets and Writers and Publishers Weekly."
10.0.2.2: [19844, 8849, 262, 1679, 400, 11162, 286, 383, 15502, 11, 290, 356, 1541, 760, 644, 345, 821, 3612, 1399, 198, 198, 2990, 2716, 428, 3489, 16032, 25629, 287, 22601, 259, 6, 1737, 12248, 198, 198, 21197, 25, 1446, 560, 27151, 921, 1680, 6305, 43124, 198, 198, 47834, 11, 329, 617, 1738, 262, 308, 849, 9961, 15827, 9048, 1492, 2646, 1625, 503, 319, 1737, 1367, 11, 9162, 851, 287, 257, 32034, 3931, 351, 6918, 588, 383, 21660, 28750, 11, 38355, 402, 931, 11, 290, 383, 15218, 2677, 13, 628, 198, 198, 37280, 11, 379, 1551, 340, 373, 319, 1363, 2008, 357, 1477, 448, 503, 284, 569, 7998, 329, 705, 3829, 82, 3988, 8133, 416, 2693, 30, 198, 198, 6104, 991, 11, 340, 373, 257, 46594, 2277, 326, 517, 621, 15229, 663, 4466, 379, 262, 3091, 2607, 290, 2627, 257, 8768, 2285, 6833, 13, 198, 198, 2396, 6189, 26925, 2984, 27312, 547, 407, 262, 3807, 10135, 428, 2646, 561, 307, 1900, 329, 1399, 198, 198, 36725, 11, 428, 373, 262, 2646, 810, 356, 2626, 7396, 3491, 14328, 5741, 11, 3367, 286, 9764, 9071, 8177, 11088, 5741, 13, 198, 198, 1870, 326, 2125, 470, 262, 691, 835, 287, 543, 428, 2646, 373, 3177, 284, 307, 25155, 13, 198, 198, 27535, 417, 15615, 198, 198, 11380, 11, 6041, 286, 7328, 423, 284, 1730, 351, 428, 851, 340, 338, 6209, 262, 4094, 5002, 345, 655, 460, 470, 22668, 1410, 329, 13, 198, 198, 1537, 618, 345, 2646, 287, 2258, 5913, 287, 2805, 11, 345, 836, 470, 6032, 1607, 36780, 312, 10101, 13, 198, 198, 4864, 11, 326, 338, 3446, 644, 484, 1392, 13, 383, 900, 373, 7478, 7558, 20884, 13, 198, 198, 1870, 326, 338, 407, 477, 851, 257, 23964, 2277, 1141, 3227, 290, 6572, 617, 286, 262, 5621, 11, 6666, 16119, 287, 17691, 13, 198, 198, 1026, 373, 257, 3807, 319, 543, 2279, 655, 3947, 284, 467, 2642, 13, 198, 198, 3351, 560, 6366, 3231, 198, 198, 4863, 262, 3726, 286, 262, 2686, 11, 262, 2646, 373, 27909, 351, 17390, 13, 198, 198, 2202, 262, 717, 1110, 11, 262, 4639, 286, 257, 23612, 12, 79, 15799, 357, 64, 7779, 351, 281, 16610, 41175, 810, 257, 5462, 2888, 460, 1302, 8, 373, 1742, 12204, 7241, 3016, 284, 1918, 0, 198, 198, 27275, 8674, 5966, 7793, 78, 11, 508, 2826, 29649, 6128, 4870, 46606, 11, 12433, 262, 5778, 284, 383, 317, 13, 53, 13, 6289, 287, 2813, 25, 198, 198, 1, 1135, 547, 4395, 379, 1755, 290, 262, 845, 717, 1110, 484, 547, 4634, 510, 262, 7588, 11, 612, 373, 257, 3516, 508, 373, 5059, 257, 23612, 12, 79, 15799, 4291, 262, 1256, 13, 317, 23612, 12, 79, 15799, 326, 345, 1234, 7588, 319, 13, 843, 262, 23612, 12, 79, 15799, 3214, 656, 257, 308, 2132, 11, 290, 356, 13663, 262, 736, 286, 262, 23612, 12, 79, 15799, 810, 262, 3516, 373, 13663, 510, 11, 290, 1816, 826, 656, 281, 12278, 16825, 290, 339, 373, 1742, 12204, 7241, 13, 843, 339, 373, 1474, 1918, 13, 1439, 286, 465, 16613, 547, 11544, 13, 679, 373, 546, 2608, 13, 2399, 3656, 373, 10423, 13, 632, 373, 257, 2089, 8458, 4756, 284, 257, 2646, 526, 198, 198, 5703, 734, 1528, 706, 326, 27767, 11, 612, 373, 3537, 31858, 1742, 12204, 1009, 13, 198, 198, 32, 1097, 26419, 373, 7478, 1762, 319, 257, 6147, 37370, 1082, 618, 340, 12615, 617, 1176, 3951, 13, 679, 6989, 20246, 319, 465, 1986, 11, 2832, 11, 290, 7721, 13, 198, 198, 18602, 326, 976, 1110, 11, 612, 373, 281, 43286, 2046, 13, 7793, 78, 3767, 25, 198, 198, 1, 1870, 788, 262, 2368, 1755, 618, 356, 547, 4395, 11, 314, 3505, 262, 2632, 7779, 4978, 319, 2046, 290, 8168, 2993, 644, 326, 373, 546, 526, 198, 198, 1532, 257, 9298, 8674, 588, 7793, 78, 1807, 262, 2646, 373, 25155, 1399, 628, 198, 198, 1537, 326, 2492, 470, 477, 1399, 198, 198, 818, 530, 286, 262, 749, 34837, 17390, 11, 257, 5462, 2888, 7478, 1392, 257, 9580, 26230, 21512, 832, 511, 1021, 0, 198, 198, 3673, 691, 373, 340, 257, 14702, 11, 12399, 5778, 851, 340, 22353, 530, 286, 262, 6821, 7651, 6698, 287, 262, 2646, 13, 628, 198, 198, 35854, 286, 599, 29655, 11, 826, 30, 198, 198, 48464, 6402, 644, 4191, 3022, 1399, 198, 198, 49, 13, 40, 13, 47, 13, 14328, 5741, 198, 198, 3633, 17691, 383, 15502, 11, 14328, 5741, 373, 2923, 287, 262, 6994, 286, 465, 1204, 13, 198, 198, 1212, 318, 925, 477, 262, 517, 15444, 780, 340, 561, 423, 587, 465, 336, 1670, 868, 2854, 13, 7413, 428, 11, 477, 339, 750, 547, 2223, 5513, 5354, 347, 12, 76, 20526, 13, 1002, 339, 1549, 925, 340, 11, 339, 561, 423, 550, 465, 2298, 286, 9961, 290, 3783, 10165, 277, 49191, 13, 5783, 11, 772, 9267, 401, 82, 13, 198, 198, 32, 8768, 2994, 329, 8502, 1399, 628, 198, 198, 2396, 644, 3022, 6949, 3548, 198, 198, 818, 262, 2646, 11, 7651, 12458, 574, 318, 41914, 416, 22239, 3386, 284, 651, 465, 15827, 706, 339, 290, 465, 11077, 389, 30040, 12864, 416, 257, 7706, 13, 1320, 1724, 287, 262, 41963, 3715, 543, 28539, 465, 45183, 11, 339, 468, 284, 1011, 257, 29444, 966, 9178, 13, 198, 198, 13898, 11, 262, 319, 9612, 5170, 2627, 257, 1103, 12, 6042, 530, 618, 262, 2485, 6294, 281, 4036, 10492, 656, 14328, 338, 11384, 13, 198, 198, 1544, 373, 13999, 284, 262, 4436, 11, 475, 706, 2237, 2250, 286, 8185, 339, 373, 16293, 2636, 13, 198, 198, 1544, 373, 655, 2579, 812, 1468, 13, 628, 198, 198, 35027, 373, 2048, 4423, 866, 706, 262, 5778, 13, 198, 198, 1537, 780, 5741, 550, 18976, 749, 286, 465, 8188, 329, 262, 3807, 379, 326, 966, 11, 340, 373, 3066, 484, 561, 5461, 290, 39383, 262, 4286, 284, 683, 13, 198, 198, 12814, 257, 1178, 4226, 2458, 290, 617, 1181, 286, 262, 1242, 4875, 1986, 9014, 319, 655, 257, 3155, 6934, 11, 484, 547, 1498, 284, 5461, 262, 3807, 13, 198, 198, 464, 6366, 738, 198, 198, 464, 1561, 286, 257, 17328, 2192, 1625, 422, 655, 703, 326, 5778, 3022, 13, 632, 338, 884, 281, 14855, 11, 7485, 2168, 286]
10.0.2.2: b'Saturday marks the 25th anniversary of The Crow, and we already know what you\'re thinking\xe2\x80\xa6\n\nThey released this obvious Halloween staple in freakin\' May?!\n\nPhotos: Scary Movies You Can Watch Streaming\n\nYep, for some reason the goth horror revenge comic book film came out on May 11, 1994 \xe2\x80\x94 in a blockbuster summer with movies like The Flintstones, Forrest Gump, and The Lion King.\n\n\n\nUm, at least it was on home video (shout out to VHS for \'90s kids!) by September?\n\nEven still, it was a sleeper hit that more than doubled its budget at the box office and became a genuine cult classic.\n\nSo obviously scheduling misfires were not the movie mistakes this film would be known for\xe2\x80\xa6\n\nSadly, this was the film where we lost rising star Brandon Lee, son of Hong Kong legend Bruce Lee.\n\nAnd that isn\'t the only way in which this film was considered to be cursed.\n\nCruel Weather\n\nOK, lots of films have to deal with this \xe2\x80\x94 it\'s basically the biggest element you just can\'t adequately plan for.\n\nBut when you film in North Carolina in March, you don\'t typically expect frigid temperatures.\n\nHowever, that\'s exactly what they got. The set was reportedly constantly freezing.\n\nAnd that\'s not all \xe2\x80\x94 a hurricane hit during production and destroyed some of the sets, causing delays in filming.\n\nIt was a movie on which everything just seemed to go wrong.\n\nScary Accidents\n\nFrom the beginning of the shoot, the film was plagued with accidents.\n\nOn the first day, the driver of a cherry-picker (a truck with an extending crane where a crew member can stand) was electrocuted nearly to death!\n\nCharacter actor Jon Polito, who played pawn shop owner Gideon, recalled the accident to The A.V. Club in 2011:\n\n"We were shooting at night and the very first day they were setting up the lights, there was a guy who was driving a cherry-picker onto the lot. A cherry-picker that you put lights on. And the cherry-picker fell into a gully, and we lifted the back of the cherry-picker where the guy was lifted up, and went right into an electrical pole and he was electrocuted. And he was near death. All of his organs were burned. He was about 26. His wife was pregnant. It was a bad luck opening to a film."\n\nJust two days after that catastrophe, there was ANOTHER electrocution.\n\nA carpenter was reportedly working on a metal girder when it touched some power lines. He suffered burns on his face, hands, and chest.\n\nLater that same day, there was an unexplained fire. Polito continued:\n\n"And then the third night when we were shooting, I remember the prop truck caught on fire and nobody knew what that was about."\n\nIf a veteran actor like Polito thought the film was cursed\xe2\x80\xa6\n\n\n\nBut that wasn\'t all\xe2\x80\xa6\n\nIn one of the most ominous accidents, a crew member reportedly got a screwdriver stabbed through their hand!\n\nNot only was it a shocking, brutal accident \xe2\x80\x94 it mirrors one of the injuries Eric faces in the film.\n\n\n\nKind of spooky, right?\n\nEspecially considering what eventually happened\xe2\x80\xa6\n\nR.I.P. Brandon Lee\n\nWhile filming The Crow, Brandon Lee was killed in the prime of his life.\n\nThis is made all the more tragic because it would have been his starmaking performance. Before this, all he did were action schlock B-movies. If he\'d made it, he would have had his pick of horror and science fiction flicks. Hell, even rom coms.\n\nA genuine loss for Hollywood\xe2\x80\xa6\n\n\n\nSo what happened anyway??\n\nIn the film, Eric Draven is resurrected by supernatural forces to get his revenge after he and his girlfriend are brutally murdered by a gang. That means in the flashback scene which depicts his slaying, he has to take a gunshot point blank.\n\nUnfortunately, the onscreen killing became a real-life one when the gun fired an actual bullet into Brandon\'s stomach.\n\nHe was rushed to the hospital, but after six hours of surgery he was pronounced dead.\n\nHe was just 28 years old.\n\n\n\nProduction was almost shut down after the accident.\n\nBut because Lee had filmed most of his scenes for the movie at that point, it was decided they would finish and dedicate the picture to him.\n\nUsing a few script changes and some state of the art digital face replacement on just a couple shots, they were able to finish the movie.\n\nThe Accident\n\nThe talk of a curse probably came from just how that accident happened. It\'s such an unfortunate, unlikely series of'
10.0.2.2: [34500, 4111, 47926, 12, 400, 273, 330, 291, 19560, 351, 13182, 38, 12, 38, 515, 5598, 12, 48369, 1963, 312, 316, 9250, 12, 808, 16356, 25, 4238, 6373, 287, 22169, 3871, 13, 198, 464, 4007, 286, 428, 2050, 373, 284, 9161, 262, 5885, 286, 24171, 262, 10238, 22949, 4369, 355, 880, 355, 26077, 2163, 1141, 13182, 38, 12, 70, 515, 16356, 3550, 72, 4867, 286, 262, 7721, 351, 5598, 12, 48369, 1963, 312, 316, 9250, 12, 808, 16356, 357, 12740, 4177, 737, 1881, 3470, 12277, 12, 15542, 12785, 3871, 287, 7813, 385, 18662, 351, 1900, 393, 9885, 7435, 41001, 19327, 25289, 281, 13182, 38, 12, 70, 515, 16356, 3550, 72, 6826, 12452, 286, 262, 7721, 1231, 12159, 12, 9967, 364, 1262, 262, 1708, 10007, 25, 357, 16, 8, 2927, 18991, 25, 3933, 2124, 657, 13, 21, 8085, 351, 1976, 12, 45928, 25397, 4136, 329, 262, 12673, 286, 5598, 32997, 657, 13, 21, 12, 3020, 24314, 357, 50, 25742, 5598, 26, 45196, 641, 1776, 13179, 640, 25, 657, 13, 2091, 264, 26, 7078, 25, 657, 13, 18, 26, 7982, 479, 53, 26, 939, 285, 1722, 26, 13182, 38, 12, 14401, 10742, 39007, 357, 2943, 38, 12, 79, 5753, 278, 8, 290, 357, 17, 8, 7982, 25962, 286, 257, 3439, 4, 6273, 5797, 13, 6060, 547, 49594, 25, 357, 16, 8, 284, 13446, 262, 10238, 22949, 4369, 357, 16, 12, 3020, 6546, 12317, 290, 16957, 459, 1292, 23824, 49594, 379, 5996, 4, 286, 262, 371, 12, 49, 16654, 26, 1312, 13, 68, 1539, 366, 24503, 20781, 23824, 4943, 290, 357, 17, 8, 284, 5004, 826, 357, 49, 6089, 37, 8, 290, 1364, 357, 43, 6089, 37, 8, 7435, 41001, 22189, 295, 49876, 357, 19509, 12, 22704, 827, 301, 4160, 290, 2566, 459, 4160, 4263, 26, 20559, 385, 3788, 26, 1312, 13, 68, 1539, 366, 45124, 23824, 11074, 383, 1612, 2612, 2494, 373, 8854, 275, 4426, 357, 9521, 25, 5433, 12, 10232, 8, 290, 262, 1612, 9367, 640, 373, 1248, 13, 1157, 29694, 362, 13, 3134, 264, 357, 9521, 25, 838, 12, 1983, 737, 317, 2472, 286, 17031, 37305, 357, 5892, 4407, 550, 1111, 12317, 290, 16957, 459, 1292, 4263, 13178, 355, 23584, 23824, 11, 9472, 838, 37305, 357, 23, 4407, 550, 1729, 12, 47356, 15132, 4263, 14294, 416, 262, 4931, 286, 22949, 12, 38714, 20316, 357, 77, 796, 604, 8, 393, 11700, 291, 20316, 3519, 284, 262, 779, 286, 257, 7078, 1988, 286, 657, 13, 18, 287, 3871, 351, 257, 845, 1877, 2612, 2494, 1141, 1366, 12673, 357, 77, 796, 718, 737, 25809, 286, 826, 290, 1364, 7435, 41001, 2163, 373, 47729, 287, 19755, 3871, 357, 6052, 7441, 6957, 4, 14514, 25, 9193, 12, 5607, 18823, 1114, 777, 19755, 37305, 11, 262, 1612, 371, 6089, 37, 373, 6337, 13, 940, 4, 11502, 16327, 860, 13, 20, 26, 2837, 25, 1160, 12, 4761, 8, 290, 262, 1612, 406, 6089, 37, 373, 7618, 13, 1954, 4, 11502, 16327, 838, 13, 3459, 26, 2837, 25, 1160, 12, 5999, 737, 554, 262, 5637, 5193, 3871, 11, 281, 848, 8344, 786, 10618, 341, 286, 262, 826, 290, 1364, 7435, 41001, 22357, 871, 373, 3177, 355, 257, 15637, 5766, 329, 7141, 17952, 286, 886, 12, 1837, 301, 4160, 290, 886, 12, 10989, 459, 4160, 7435, 41001, 15343, 13, 383, 1612, 11502, 16327, 9834, 8, 360, 19930, 1988, 286, 262, 37305, 373, 39466, 13, 4521, 11502, 16327, 19048, 13, 1120, 8, 285, 44802, 13, 11215, 13, 25809, 286, 10238, 22949, 4369, 290, 26077, 2163, 422, 262, 976, 1366, 900, 373, 47729, 287, 10190, 4, 286, 262, 3871, 351, 13182, 38, 12, 70, 515, 5598, 12, 48369, 10670, 4177, 13]
10.0.2.2: b'Integrated cardio-thoracic imaging with ECG-Gated 64-slice multidetector-row CT: initial findings in 133 patients.\nThe purpose of this study was to investigate the possibility of assessing the underlying respiratory disease as well as cardiac function during ECG-gated CT angiography of the chest with 64-slice multidetector-row CT (MDCT). One hundred thirty-three consecutive patients in sinus rhythm with known or suspected ventricular dysfunction underwent an ECG-gated CT angiographic examination of the chest without beta-blockers using the following parameters: (1) collimation: 32 x 0.6 mm with z-flying focal spot for the acquisition of 64 overlapping 0.6-mm slices (Sensation 64; Siemens); rotation time: 0.33 s; pitch: 0.3; 120 kV; 200 mAs; ECG-controlled dose modulation (ECG-pulsing) and (2) 120 ml of a 35% contrast agent. Data were reconstructed: (1) to evaluate the underlying respiratory disease (1-mm thick lung and mediastinal scans reconstructed at 55% of the R-R interval; i.e., "morphologic scans") and (2) to determine right (RVEF) and left (LVEF) ventricular ejection fractions (short-axis systolic and diastolic images; Argus software; i.e., "functional scans"). The mean heart rate was 73 bpm (range: 42-120) and the mean scan time was 18.11 +/- 2.67 s (range: 10-27). A total of 123 examinations (92%) had both lung and mediastinal images rated as diagnostic scans, whereas 10 examinations (8%) had non-diagnostic images altered by the presence of respiratory-motion artifacts (n = 4) or cyclic artifacts related to the use of a pitch value of 0.3 in patients with a very low heart rate during data acquisition (n = 6). Assessment of right and left ventricular function was achievable in 124 patients (93%, 95% CI: 88-97%). For these 124 examinations, the mean RVEF was 46.10% (+/- 9.5; range: 20-72) and the mean LVEF was 58.23% (+/- 10.88; range: 20-83). In the remaining nine patients, an imprecise segmentation of the right and left ventricular cavities was considered as a limiting factor for precise calculation of end-systolic and end-diastolic ventricular volumes. The mean (+/- SD) DLP value of the examinations was 279.86 (+/- 117.50) mGy.cm. Assessment of underlying respiratory disease and cardiac function from the same data set was achievable in 92% of the patients with ECG-gated 64-slice MDCT.'
10.0.2.2: Create dataset pile at scatter 0 with 2509316 documents
10.0.2.2: [1026, 338, 407, 17458, 28149, 284, 17920, 462, 4351, 4324, 8718, 45693, 284, 910, 326, 340, 338, 1327, 284, 892, 286, 257, 517, 2227, 357, 392, 39659, 651, 11487, 8, 44185, 287, 2031, 826, 783, 621, 17109, 7335, 11951, 84, 391, 13, 1318, 338, 3729, 645, 19099, 3058, 287, 257, 517, 7215, 292, 12, 2339, 1057, 286, 1296, 621, 262, 36399, 11, 508, 287, 262, 2479, 286, 23510, 945, 290, 2208, 18664, 82, 5983, 262, 15549, 338, 9689, 15907, 981, 7463, 23017, 656, 6159, 9987, 13, 198, 198, 15262, 1142, 22418, 423, 683, 287, 511, 3272, 71, 3468, 290, 11, 355, 318, 511, 28329, 11, 423, 18359, 645, 3663, 284, 7365, 511, 29708, 2340, 44896, 287, 511, 2496, 338, 4571, 13, 5953, 4640, 15415, 12, 1544, 259, 89, 371, 20080, 328, 469, 468, 587, 262, 3452, 284, 1011, 284, 1702, 465, 40221, 532, 290, 339, 338, 407, 3436, 13, 366, 5189, 477, 262, 3215, 1938, 287, 41530, 317, 826, 783, 11, 11951, 84, 391, 318, 616, 12507, 553, 18205, 276, 4706, 12, 259, 12, 10247, 1780, 40089, 1052, 5276, 26380, 938, 1227, 13, 198, 198, 1890, 783, 11, 996, 11, 339, 318, 1016, 12062, 532, 290, 4346, 318, 477, 262, 1365, 329, 340, 13, 1052, 36399, 2651, 11, 38186, 287, 326, 18778, 5156, 4171, 10147, 290, 9645, 14332, 11106, 3812, 257, 3670, 4427, 318, 257, 6504, 11462, 284, 5814, 262, 15625, 286, 262, 9267, 29320, 13, 887, 11951, 84, 391, 338, 1621, 318, 3385, 306, 2612, 3101, 287, 1909, 338, 10747, 11, 810, 2048, 790, 1353, 7401, 318, 17298, 1626, 257, 7009, 4097, 286, 1637, 276, 2208, 30132, 13, 198, 198, 32423, 2188, 338, 10655, 198, 198, 1890, 262, 749, 636, 11, 597, 17909, 351, 1526, 324, 4450, 389, 35010, 290, 2642, 15353, 11, 475, 262, 8177, 40293, 416, 465, 18476, 318, 991, 2383, 11, 290, 407, 691, 780, 339, 21272, 1705, 286, 11951, 84, 391, 338, 1445, 284, 465, 44358, 20994, 351, 262, 14305, 326, 339, 373, 366, 67, 12944, 306, 625, 2633, 276, 1, 198, 198, 14698, 7334, 10032, 286, 1204, 319, 6416, 14708, 338, 18375, 444, 736, 287, 2211, 11, 11951, 84, 391, 468, 587, 1088, 326, 2512, 1752, 1541, 13, 6498, 783, 356, 460, 19096, 683, 287, 326, 28790, 6536, 286, 1938, 508, 389, 517, 379, 1363, 7662, 803, 281, 44288, 621, 47399, 3929, 262, 9323, 13, 2399, 22581, 1028, 48572, 84, 14057, 938, 5041, 407, 691, 1718, 465, 898, 26767, 329, 262, 1622, 284, 1160, 287, 355, 867, 1830, 475, 4504, 14332, 11106, 284, 262, 1353, 286, 41530, 317, 355, 262, 1923, 338, 1597, 886, 46234, 13, 198, 198, 1890, 262, 749, 636, 11, 597, 17909, 351, 1526, 324, 4450, 389, 35010, 290, 2642, 15353, 11, 475, 262, 8177, 40293, 416, 465, 18476, 318, 991, 2383, 11, 290, 407, 691, 780, 339, 21272, 1705, 286, 11951, 84, 391, 338, 1445, 284, 465, 44358, 20994, 351, 262, 14305, 326, 339, 373, 366, 67, 12944, 306, 625, 2633, 276, 1911, 1526, 324, 4450, 338, 3671, 499, 2367, 287, 49404, 357, 1169, 4652, 8714, 339, 1718, 606, 284, 287, 12923, 290, 6303, 3520, 262, 3430, 338, 691, 734, 8, 407, 691, 2087, 281, 24056, 5002, 286, 19661, 284, 262, 3430, 338, 8507, 1973, 262, 13342, 532, 290, 48662, 4193, 35537, 262, 13666, 286, 11951, 84, 391, 338, 4351, 287, 262, 717, 1295, 532, 475, 635, 33966, 1739, 262, 3430, 351, 262, 845, 38216, 3716, 326, 468, 4193, 663, 649, 21580, 698, 25548, 2402, 10325, 13, 198, 198, 2200, 9858, 44, 49361, 807, 1938, 508, 547, 5749, 621, 262, 3430, 357, 273, 379, 1551, 1807, 484, 547, 8, 198, 198, 12510, 4647, 706, 1526, 324, 4450, 12615, 866, 319, 3169, 499, 12977, 29553, 11, 11951, 84, 391, 2622, 6974, 284, 46804, 465, 6614, 4058, 286, 465, 1263, 1445, 284, 6152, 262, 717, 6529, 286, 11892, 422, 512, 3255, 17205, 13, 383, 8216, 286, 40380, 373, 4999, 257, 3155, 286, 1933, 1568, 11, 618, 465, 717, 6662, 4041, 4661, 550, 257, 11856, 2986, 11243, 14057, 894, 889, 503, 465, 1438, 757, 290, 757, 379, 21815, 3101, 6115, 532, 257, 25023, 11, 36219, 50131, 284, 262, 867, 9687, 286, 262, 5336, 270, 1417, 3722, 286, 1909, 338, 4336, 3968, 13, 198, 198, 3118, 785, 12065, 2858, 198, 198, 36124, 84, 391, 338, 640, 287, 14708, 635, 20534, 276, 281, 14855, 5182, 543, 339, 468, 1865, 284, 13279, 572, 25, 326, 465, 1241, 286, 13293, 468, 19960, 284, 1057, 287, 34062, 9823, 284, 465, 1241, 286, 9750, 198, 198, 3646, 3787, 286, 5701, 3653, 804, 284, 7716, 23594, 290, 23775, 1042, 284, 1394, 2405, 13338, 11, 475, 11951, 84, 391, 318, 257, 2137, 508, 19531, 1158, 319, 36374, 13, 366, 32, 19099, 1464, 7832, 284, 1254, 1593, 553, 339, 1752, 531, 11, 543, 5983, 284, 262, 287, 50141, 540, 7664, 326, 339, 373, 1239, 1884, 284, 1064, 257, 24568, 10115, 36309, 12, 34475, 6416, 14708, 262, 749, 22516, 2858, 13, 198, 198, 3673, 326, 465, 640, 287, 8602, 373, 416, 597, 1724, 23993, 11, 475, 257, 2137, 508, 614, 5907, 329, 7284, 35924, 19163, 373, 1464, 1016, 284, 1064, 2241, 29044, 1028, 257, 5405, 13387, 618, 8754, 656, 257, 5636, 506, 286, 44280, 418, 13, 366, 40, 373, 845, 3772, 287, 14708, 1141, 617, 7188, 11, 475, 1342, 3772, 287, 1854, 13, 921, 8659, 618, 345, 836, 470, 651, 284, 711, 329, 617, 640, 553, 339, 1568, 6848, 13, 198, 198, 36124, 84, 391, 338, 640, 287, 14708, 635, 20534, 276, 281, 14855, 5182, 543, 339, 468, 1865, 284, 13279, 572, 25, 326, 465, 1241, 286, 13293, 468, 19960, 284, 1057, 287, 34062, 9823, 284, 465, 1241, 286, 9750, 13, 2399, 1903, 2712, 812, 379, 5866, 23648, 41085, 351, 262, 3430, 338, 18598, 422, 16519, 338, 1176, 2779, 11, 290, 981, 339, 7723, 12785, 4652, 14591, 6, 28057, 2402, 9679, 6416, 14708, 11, 484, 1625, 287, 9964, 287, 543, 339, 7781, 734, 290, 3624, 4661, 8148, 11, 3599, 655, 1160, 7466, 1973, 1111, 7028, 13, 383, 1708, 614, 11, 1719, 1288, 65, 6972, 465, 835, 284, 262, 2166, 286, 262, 3430, 338, 613, 44377, 1502, 11, 24226, 46990, 338, 33545, 3988, 1816, 290, 8169, 2557, 510]
10.0.2.2: b'It\'s not succumbing to tiresome transfer window hyperbole to say that it\'s hard to think of a more wanted (and realistically gettable) footballer in Europe right now than Gonzalo Higuain. There\'s certainly no striker currently in a more Midas-like run of form than the Argentine, who in the age of superstars and superclubs leads the continent\'s scoring charts while falling proudly into neither criteria.\n\nBayern Munich have him in their crosshairs and, as is their wont, have wasted no opportunity to bat their eyelids furiously in their target\'s direction. Chief executive Karl-Heinz Rumenigge has been the latest to take to sing his praises - and he\'s not alone. "Of all the foreign players in Serie A right now, Higuain is my favourite," chimed manager-in-waiting Carlo Ancelotti last month.\n\nFor now, though, he is going nowhere - and football is all the better for it. An Argentine forward, clad in that distinctive baby blue shirt and firing Napoli toward a title challenge is a sight guaranteed to warm the souls of the romantics. But Higuain\'s story is doubly heartening in today\'s landscape, where almost every top talent is concentrated within a tiny band of moneyed superpowers.\n\nDiego\'s legacy\n\nFor the most part, any comparisons with Maradona are simplistic and wrongheaded, but the legend bestowed by his predecessor is still significant, and not only because he greeted news of Higuain\'s move to his honorary hometown with the declaration that he was "doubly overjoyed"\n\nHaving grown tired of life on Real Madrid\'s peripheries back in 2013, Higuain has been around that block once already. Right now we can bracket him in that pleasing category of players who are more at home elevating an underdog than embellishing the establishment. His brace against Sassuolo last weekend not only took his own tally for the season to 20 in as many games but returned Napoli to the top of Serie A as the campaign\'s business end looms.\n\nFor the most part, any comparisons with Maradona are simplistic and wrongheaded, but the legend bestowed by his predecessor is still significant, and not only because he greeted news of Higuain\'s move to his honorary hometown with the declaration that he was "doubly overjoyed". Maradona\'s escapades in Naples (the league titles he took them to in 1987 and 1990 remain the club\'s only two) not only added an enduring element of romance to the club\'s reputation across the globe - and doubtless helped grease the wheels of Higuain\'s transfer in the first place - but also imbued the club with the very Messiah complex that has helped its new idol blossom upon arrival.\n\nRECOMMENDED 8 players who were bigger than the club (or at least thought they were)\n\nThree decades after Maradona touched down on Neapolitan turf, Higuain needed merely to disembark his plane ahead of his big move to prompt the first acts of worship from adoring locals. The tone of reverence was confirmed a couple of months later, when his first Champions League goals had a packed San Paolo belting out his name again and again at deafening volume - a magnificent, defiant antidote to the many complaints of the sanitised status of today\'s fan culture.\n\nUncomfortable environment\n\nHiguain\'s time in Madrid also cemented an unfortunate trend which he has yet to shake off: that his level of achievement has tended to run in inverse proportion to his level of involvement\n\nPlenty of sportsmen look to generate hostility and antagonism to keep themselves motivated, but Higuain is a player who thrives on goodwill. "A striker always likes to feel important," he once said, which leads to the inescapable conclusion that he was never likely to find a Cristiano Ronaldo-dominated Real Madrid the most welcoming environment.\n\nNot that his time in Spain was by any means unsuccessful, but a player who yearns for bespoke appreciation was always going to find himself punching against a glass ceiling when thrown into a throng of galacticos. "I was very happy in Madrid during some moments, but less happy in others. You suffer when you don\'t get to play for some time," he later admitted.\n\nHiguain\'s time in Madrid also cemented an unfortunate trend which he has yet to shake off: that his level of achievement has tended to run in inverse proportion to his level of involvement. His early playing years at River Plate coincided with the club\'s descent from Argentina\'s power base, and while he collected consecutive league winners\' medals upon joining Real Madrid, they came in campaigns in which he scored two and eight goals respectively, starting just 20 matches across both seasons. The following year, having elbowed his way to the front of the club\'s pecking order, Pep Guardiola\'s meddling kids went and hoovered up'
10.0.2.2: [51, 18534, 17036, 31918, 366, 14304, 2926, 5488, 1, 21753, 66, 12427, 481, 1986, 5891, 1041, 16158, 9298, 19116, 4345, 805, 287, 257, 8096, 1657, 32489, 18222, 329, 12282, 3174, 366, 16966, 1754, 3691, 13, 30512, 1, 319, 2795, 718, 379, 262, 4746, 27585, 3337, 287, 520, 13, 5593, 13]
10.0.2.2: b'Touted Brazilian Rafael "Feijao" Cavalcante will face fellow Pro Elite veteran Jared Hamman in a featured light heavyweight bout for Strikeforce "Lawler vs. Shields" on June 6 at the Scottrade Center in St. Louis.'
10.0.2.2: [26950, 958, 434, 286, 1535, 290, 3081, 286, 1204, 1262, 649, 1294, 2717, 9949, 329, 262, 11795, 286, 13825, 13, 198, 22362, 39204, 2472, 27127, 286, 4369, 3917, 351, 22652, 290, 13825, 468, 587, 42629, 416, 257, 3092, 286, 6414, 3199, 1366, 1262, 25713, 1767, 2347, 6376, 357, 33, 8895, 393, 2264, 14471, 1616, 6376, 685, 9948, 49262, 355, 3463, 287, 37075, 9086, 416, 262, 6616, 286, 262, 6001, 287, 10700, 25, 3463, 357, 10025, 20679, 58, 17015, 2124, 357, 76, 8, 17, 11907, 8, 23584, 9987, 11, 290, 416, 13455, 25713, 4941, 9684, 13, 49559, 286, 22949, 1035, 1648, 19777, 11, 1877, 736, 2356, 11, 1729, 12, 1040, 11599, 12, 21186, 12593, 33748, 17506, 11, 21134, 2526, 5087, 11, 290, 3518, 15025, 1262, 14362, 12, 2623, 28301, 547, 5295, 287, 257, 3272, 12, 44330, 8852, 5526, 286, 642, 46660, 1450, 290, 767, 29159, 1466, 9722, 1160, 284, 7863, 812, 422, 262, 12671, 290, 15475, 1262, 22456, 9987, 286, 262, 2351, 33656, 286, 3893, 290, 262, 2159, 3893, 12275, 9949, 13, 383, 11967, 3007, 286, 21134, 7476, 547, 2440, 287, 1450, 621, 1466, 11, 475, 262, 584, 1535, 10906, 547, 517, 10792, 287, 1466, 13, 11285, 935, 477, 1535, 10906, 3177, 547, 5566, 12824, 416, 22456, 13, 317, 22456, 286, 1679, 284, 1542, 14211, 14, 76, 17, 550, 257, 4143, 3744, 2928, 319, 10402, 22423, 329, 1535, 10906, 287, 1466, 621, 287, 1450, 13, 4380, 351, 22456, 2174, 1679, 14211, 14, 76, 17, 547, 3177, 262, 4941, 1448, 11, 351, 1877, 16815, 286, 7460, 286, 13825, 12, 5363, 10040, 290, 922, 3081, 286, 1204, 13, 14307, 1679, 284, 1542, 14211, 14, 76, 17, 11, 262, 11967, 3007, 286, 777, 547, 477, 3220, 11, 290, 2029, 1542, 14211, 14, 76, 17, 9257, 3220, 13, 2293, 16895, 329, 2479, 290, 12263, 5087, 11, 10402, 22423, 357, 3865, 4, 6628, 20016, 685, 3865, 4, 14514, 12962, 287, 883, 351, 257, 22456, 286, 1542, 14211, 14, 76, 17, 393, 2440, 547, 513, 13, 20, 357, 3865, 4, 14514, 11, 362, 13, 23, 12, 19, 13, 19, 8, 287, 1450, 290, 513, 13, 18, 357, 3865, 4, 14514, 11, 362, 13, 23, 12, 18, 13, 24, 8, 287, 1466, 329, 1790, 1108, 286, 8033, 618, 6155, 26148, 11, 604, 13, 21, 357, 3865, 4, 14514, 11, 362, 13, 19, 12, 23, 13, 23, 8, 287, 1450, 290, 642, 13, 19, 357, 3865, 4, 14514, 11, 362, 13, 23, 12, 940, 13, 20, 8, 287, 1466, 329, 1729, 12, 1040, 11599, 12, 21186, 12593, 33748, 17506, 11, 642, 13, 20, 357, 3865, 4, 14514, 11, 604, 13, 20, 12, 21, 13, 21, 8, 287, 1450, 290, 362, 13, 24, 357, 3865, 4, 14514, 11, 362, 13, 19, 12, 18, 13, 19, 8, 287, 1466, 329, 1719, 379, 1551, 352, 1688, 21134, 2526, 5766, 13, 5747, 1450, 290, 1466, 351, 22456, 286, 1542, 14211, 14, 76, 17, 393, 2440, 547, 5403, 355, 1884, 284, 423, 13156, 287, 9489, 257, 2837, 286, 4096, 4445, 3518, 4568, 13, 27492, 351, 1466, 351, 22456, 2793, 621, 1679, 14211, 14, 76, 17, 11, 883, 351, 22456, 286, 1542, 14211, 14, 76, 17, 393, 2440, 547, 352, 13, 20, 1661, 517, 1884, 284, 423, 7460, 286, 987, 332, 660, 24427, 11898, 607, 77, 3920, 13, 5865, 42491, 517, 22652, 1466, 550, 2761, 3917, 351, 1877, 736, 2356, 11, 1390, 16222, 8132, 284, 511, 4445, 1597, 11, 8889, 422, 670, 11, 290, 3315, 18103, 13, 3893, 7476, 329, 257, 2837, 286, 2761, 389, 5545, 1262, 262, 3210, 22456, 45616, 2173, 13, 3827, 6551, 290, 13825, 389, 3917, 351, 3220, 7476, 286, 10726, 10040, 11, 9233, 7460, 11, 290, 23992, 286, 3081, 286, 1204, 13]
10.0.2.2: b'Impairment of health and quality of life using new US federal guidelines for the identification of obesity.\nEstimating total burdens of disease associated with overweight and obesity has been hampered by a lack of consistent published data using standardized body mass index (BMI or Quetelet index [calculated as weight in kilograms divided by the square of the height in meters: weight (kg)/[height x (m)2]]) diagnostic criteria, and by poorly standardized reference populations. Symptoms of respiratory insufficiency, low back pain, non-insulin-dependent diabetes mellitus, cardiovascular risk factors, and physical functioning using SF-36 questionnaire were determined in a cross-sectional representative survey of 5887 men and 7018 women aged 20 to 59 years from the Netherlands and analyzed using BMI criteria of the National Institutes of Health and the World Health Organization guidelines. The prevalences of cardiovascular risks were higher in men than women, but the other health outcomes were more frequent in women. Virtually all health outcomes considered were significantly influenced by BMI. A BMI of 25 to 30 kg/m2 had a generally greater impact on odds ratios for health outcomes in women than in men. People with BMI below 25 kg/m2 were considered the reference group, with low prevalence of symptoms of obesity-related diseases and good quality of life. Between 25 to 30 kg/m2, the prevalences of these were all increased, and above 30 kg/m2 greatly increased. After adjustments for age and lifestyle factors, odds ratios (95% confidence intervals [95% CI]) in those with a BMI of 30 kg/m2 or higher were 3.5 (95% CI, 2.8-4.4) in men and 3.3 (95% CI, 2.8-3.9) in women for shortness of breath when walking upstairs, 4.6 (95% CI, 2.4-8.8) in men and 5.4 (95% CI, 2.8-10.5) in women for non-insulin-dependent diabetes mellitus, 5.5 (95% CI, 4.5-6.6) in men and 2.9 (95% CI, 2.4-3.4) in women for having at least 1 major cardiovascular risk factor. Both men and women with BMI of 30 kg/m2 or higher were twice as likely to have difficulties in performing a range of basic daily physical activities. Compared with women with BMI lower than 25 kg/m2, those with BMI of 30 kg/m2 or higher were 1.5 times more likely to have symptoms of intervertebral disk herniation. Significantly more overweight women had problems associated with low back pain, including hindrance to their daily business, absence from work, and medical consultation. Health risks for a range of problems are presented using the standard BMI cutoff points. Overweight and obesity are associated with increased risks of chronic diseases, secondary symptoms, and impairment of quality of life.'
10.0.2.2: [49, 34889, 1717, 68, 11, 2039, 45284, 198, 198, 18843, 2929, 286, 262, 2039, 45284, 1812, 4287, 9455, 423, 5169, 1679, 9885, 2285, 1023, 16686, 284, 262, 2619, 28570, 705, 32, 8505, 6, 3200, 3592, 1141, 281, 30122, 10542, 287, 2039, 45284, 13, 198, 198, 28545, 3825, 9272, 326, 262, 11826, 547, 5169, 319, 3261, 301, 286, 3035, 11, 2864, 618, 262, 27269, 286, 262, 12297, 12, 34, 586, 1042, 11801, 286, 262, 9455, 31158, 440, 1350, 11433, 38561, 11072, 5851, 28374, 272, 707, 24484, 810, 262, 11826, 11, 531, 284, 423, 1282, 422, 2972, 6712, 286, 1029, 4673, 290, 9233, 4266, 1626, 262, 26015, 11, 550, 9272, 329, 30122, 41302, 656, 262, 3200, 3592, 13, 198, 198, 464, 1812, 4287, 5094, 13883, 10391, 11, 1770, 12119, 567, 1703, 3301, 47775, 11, 4999, 262, 3251, 618, 11237, 319, 11426, 13, 198, 198, 5840, 3301, 47775, 11, 257, 34058, 286, 4287, 11, 531, 262, 14794, 547, 636, 286, 257, 4025, 1923, 290, 36847, 1028, 2285, 1042, 290, 584, 410, 1063, 12077, 28247, 287, 262, 1181, 13, 198, 198, 464, 11826, 11, 339, 531, 11, 547, 5169, 832, 257, 880, 12, 37652, 3898, 9513, 326, 1718, 1295, 379, 546, 352, 25, 405, 257, 13, 76, 13, 2641, 262, 24484, 2157, 2039, 45284, 1812, 1979, 21629, 2254, 11, 5851, 28374, 272, 707, 319, 3321, 1755, 13, 198, 198, 1, 464, 11826, 3407, 2444, 286, 617, 12411, 7514, 23873, 873, 11, 11155, 290, 9233, 4266, 1626, 262, 26015, 355, 880, 355, 617, 1242, 26100, 290, 3259, 422, 2972, 2706, 287, 262, 1181, 553, 262, 1644, 6523, 531, 13, 198, 198, 1, 23022, 11911, 422, 262, 11826, 2291, 257, 15726, 925, 2060, 9036, 16790, 11, 257, 15726, 925, 2005, 284, 2546, 2060, 9036, 2485, 11, 1440, 2107, 37852, 290, 2237, 37328, 37852, 553, 339, 531, 13, 198, 198, 5840, 3301, 47775, 531, 262, 11826, 389, 1541, 5742, 262, 1644, 287, 511, 10986, 13, 198, 198, 818, 257, 3519, 2478, 11, 262, 350, 31190, 2252, 16404, 326, 734, 9885, 7394, 5937, 326, 39868, 287, 1509, 521, 1359, 511, 41975, 4970, 286, 1637, 290, 1188, 84, 2977, 423, 587, 5169, 416, 262, 27269, 286, 262, 12297, 12, 34, 586, 1042, 11801, 13, 198, 198, 1544, 531, 262, 11826, 11, 508, 2921, 511, 3891, 355, 530, 48857, 440, 7145, 325, 11632, 290, 911, 1437, 12301, 5708, 422, 314, 73, 1765, 84, 28818, 2127, 440, 7145, 1812, 11, 547, 5169, 319, 1737, 352, 301, 2864, 379, 5065, 86, 20895, 7285, 16488, 286, 2295, 1734, 11, 2039, 45284, 11, 355, 257, 1255, 286, 4430, 4560, 13, 198, 198, 4821, 284, 1703, 3301, 47775, 11, 262, 11826, 953, 385, 1515, 26800, 373, 284, 12705, 355, 35867, 1023, 287, 1502, 284, 1509, 42343, 4970, 286, 511, 23309, 13, 198, 198, 1, 23022, 11911, 422, 606, 389, 2291, 9885, 8390, 19247, 286, 617, 2678, 11, 617, 12585, 14396, 10559, 290, 584, 9885, 29981, 5696, 13, 1119, 389, 5742, 262, 27269, 287, 511, 10986, 553, 339, 531, 13]
10.0.2.2: b'Raphael Ede, Enugu\n\nOperatives of the Enugu State Police Command have arrested 25 suspected cultists belonging to the Black Axe \'Ayes\' secret society during an initiation ceremony in Enugu.\n\nDaily Sun gathered that the suspects were arrested on 31st of April, 2018 when the operatives of the Anti-Cultism Unit of the Command raided Obeagu Amechi Awkunanaw bush where the suspects, said to have come from various institutions of high learning and secondary schools within the southeast, had gathered for initiation rites into the secret society.\n\nThe State Police Public Relations Officer, Mr Ebere Amaraizu, confirmed the arrest when contacted on telephone.\n\nAmaraizu, a Superintendent of Police, said the arrests were part of a larger campaign and onslaught against cultism and other vices amongst youths in the state.\n\nThe suspects, he said, were arrested through a well-coordinated raid that took place at about 1:00 a.m. inside the bush behind Enugu State Centenary City, Awkunanaw on Monday night.\n\n"The suspects includes students of some notable polytechnics, universities and secondary schools within the southeast as well as some artisans and workers from various companies in the state," the police spokesman said.\n\n"Items recovered from the suspects include a locally made single barrel pistol, a locally made cut to size single barrel gun, four live cartridges and six expended cartridges," he said.\n\nAmaraizu said the suspects are already helping the police in their investigations.\n\nIn a related development, the PPRO further disclosed that two suspected fraudsters that specialize in swindling their unsuspecting victims of money and valuables have been arrested by the operatives of the Anti-Cultism Unit.\n\nHe said the suspects, who gave their names as one Akin Ogunsemi and Shina Fatola from Ijebu Igbo Ogun State, were arrested on May 1st 2018 at Ukwuoji axis of Emene, Enugu, as a result of intelligence operations.\n\nAccording to Amaraizu, the suspects modus operandi was to pose as herbalists in order to swindle victims of their possessions.\n\n"Items recovered from them are include suspected fake currencies of some countries, some Juju boxes and other suspected shrine materials. They are helping the operatives in their investigations," he said.'
10.0.2.2: [19221, 4611, 677, 7408, 43614, 22577, 39766, 2882, 287, 6076, 49235, 739, 1029, 5951, 5503, 13, 198, 11922, 5951, 318, 530, 286, 262, 1593, 5503, 5087, 326, 2689, 14450, 287, 19690, 2678, 13, 39269, 466, 18101, 393, 11206, 1180, 11701, 284, 10980, 884, 5503, 329, 9441, 13, 632, 318, 281, 3499, 2426, 290, 468, 12725, 867, 4837, 284, 670, 2402, 13, 3423, 11, 356, 9713, 262, 1245, 286, 3664, 4611, 677, 7408, 357, 4090, 8, 319, 9403, 1359, 3349, 290, 28360, 876, 3761, 1080, 287, 734, 6076, 49235, 357, 57, 18213, 285, 592, 406, 2014, 2429, 13567, 48569, 1539, 327, 5805, 12, 2624, 357, 2411, 9404, 4894, 33435, 8, 290, 37125, 12, 1157, 357, 2411, 9404, 4894, 22084, 828, 739, 1029, 5951, 5503, 13, 3334, 5951, 18268, 3744, 7741, 287, 5894, 42584, 286, 37125, 12, 16, 352, 9403, 17783, 355, 3688, 284, 883, 286, 327, 5805, 12, 2624, 13, 1318, 373, 257, 10730, 2620, 287, 355, 10215, 65, 378, 583, 1140, 312, 589, 290, 25276, 776, 7935, 2027, 310, 589, 4568, 287, 262, 11135, 286, 327, 5805, 12, 2624, 9403, 17783, 13, 2102, 11, 262, 4568, 286, 3797, 282, 589, 290, 2208, 28885, 18099, 315, 589, 11832, 290, 262, 10154, 286, 367, 19004, 11, 386, 1370, 290, 6428, 261, 1940, 498, 2934, 39175, 357, 44, 5631, 8, 3220, 287, 9403, 17783, 286, 1111, 262, 2429, 13567, 13, 15678, 286, 14719, 357, 7029, 31324, 44, 8, 2957, 284, 3220, 5894, 42584, 287, 4894, 15033, 327, 5805, 12, 2624, 9403, 17783, 13, 632, 6596, 262, 9332, 286, 4789, 72, 4053, 12, 1722, 4763, 21182, 287, 11135, 286, 327, 5805, 12, 2624, 9403, 17783, 355, 373, 30204, 416, 262, 13105, 355, 10215, 65, 378, 583, 1140, 312, 589, 290, 25276, 776, 7935, 2027, 310, 589, 4568, 13, 383, 4568, 286, 3797, 282, 589, 290, 2208, 28885, 18099, 315, 589, 3220, 287, 1111, 262, 21379, 286, 37125, 12, 1157, 9403, 17783, 11, 9472, 287, 327, 5805, 12, 2624, 11, 340, 373, 691, 287, 20611, 11, 706, 14719, 3586, 13, 2448, 1140, 312, 589, 3842, 3220, 287, 14719, 5716, 9403, 17783, 286, 1111, 262, 2429, 13567, 11, 996, 262, 2620, 373, 31188, 2440, 287, 327, 5805, 12, 2624, 13, 383, 10154, 286, 367, 158, 224, 224, 46, 158, 224, 224, 290, 337, 5631, 11832, 290, 326, 286, 386, 1370, 3220, 287, 14719, 5716, 9403, 17783, 286, 1111, 262, 2429, 13567, 11, 739, 5503, 3403, 13, 632, 743, 307, 8391, 326, 14719, 18268, 22577, 39766, 2882, 416, 510, 2301, 8306, 4789, 72, 4053, 12, 1722, 4763, 21182, 287, 11135, 290, 708, 1397, 1029, 19922, 55, 3842, 287, 1111, 262, 21379, 286, 327, 5805, 12, 2624, 9403, 17783, 11, 739, 1029, 5951, 5503, 13]
10.0.2.2: b'Salicylic acid induces differential antioxidant response in spring maize under high temperature stress.\nHigh temperature is one of the important stress factors that affect crops in tropical countries. Plants do evolve or adopt different mechanisms to overcome such stress for survival. It is an interesting subject and has attracted many researchers to work upon. Here, we studied the effect of salicylic acid (SA) on seedling growth and antioxidative defense system in two spring maize (Zea mays L.) genotypes viz., CML-32 (relatively heat tolerant) and LM-11 (relatively heat susceptible), under high temperature stress. High temperature induced greater reduction in dry biomass of LM-1 1 seedlings as compared to those of CML-32. There was a parallel increase in ascorbate peroxidase and glutathione reductase activities in the roots of CML-32 seedlings. However, the activities of catalase and superoxide dismutase decreased and the contents of H202, proline and malonaldialdehyde (MDA) increased in seedlings of both the genotypes. Application of SA (400 \xc2\xb5M) led to increased dry biomass in heat stressed CML-32 seedlings. It improved the efficiency of Halliwell-Asada pathway in roots of CML-32 seedlings as was evidenced by the enhanced ascorbate peroxidase and glutathione reductase activities. The activities of catalase and superoxide dismutase increased in both the tissues of LM-11 seedlings, whereas in CML-32, it was only in shoots, after SA application. Peroxidase activity increased in SA treated seedlings of both the genotypes, though the increase was comparatively higher in CML-32. The contents of H\xe2\x82\x82O\xe2\x82\x82 and MDA decreased and that of proline increased in SA treated seedlings of both the genotypes, under stress conditions. It may be concluded that SA induced differential antioxidant response by upregulating Halliwell-Asada pathway in roots and attaining high POX activity in both the tissues of CML-32 seedlings, under high temperature stress.'
10.0.2.2: [32, 1310, 625, 734, 1933, 2084, 11, 1966, 5940, 3950, 3491, 11, 1966, 4640, 3437, 286, 376, 7397, 2223, 11, 290, 1459, 1714, 14321, 6623, 8518, 27436, 4563, 373, 530, 286, 262, 717, 1688, 21305, 7362, 287, 20899, 14909, 338, 13029, 1366, 10285, 13, 1629, 262, 640, 11, 356, 547, 1498, 284, 6216, 465, 734, 3432, 5504, 1262, 262, 3917, 3884, 2657, 7508, 13, 887, 783, 11, 5176, 284, 8171, 1706, 18106, 15406, 11, 356, 635, 423, 465, 7034, 5986, 13, 685, 6214, 5992, 2174, 8183, 198, 198, 4711, 5504, 547, 407, 13140, 11, 523, 597, 2888, 286, 20899, 14909, 2045, 329, 257, 2872, 287, 262, 976, 4736, 777, 5504, 389, 6823, 287, 561, 307, 1498, 284, 766, 262, 7034, 4286, 13, 357, 1174, 10260, 25, 20899, 14909, 10212, 6945, 326, 2985, 389, 1498, 284, 1630, 543, 5986, 319, 511, 1848, 389, 1171, 290, 2839, 26, 1171, 7034, 5986, 884, 355, 777, 460, 307, 1043, 416, 477, 1866, 10342, 262, 2524, 11, 407, 655, 832, 511, 1321, 326, 373, 19957, 13, 4091, 5992, 379, 262, 886, 286, 428, 1281, 737, 198, 198, 464, 7034, 5986, 286, 262, 3318, 11129, 1513, 5504, 547, 5954, 510, 416, 674, 8171, 1706, 351, 257, 1479, 11, 555, 47684, 1848, 1262, 1366, 422, 262, 8156, 13, 198, 198, 5189, 1781, 597, 3275, 345, 3758, 1839, 470, 1682, 307, 6793, 284, 27436, 4563, 2241, 13, 887, 428, 857, 4545, 514, 379, 1551, 734, 1243, 25, 352, 8, 8518, 27436, 4563, 5729, 973, 1103, 357, 2016, 4622, 40157, 8, 5205, 329, 1111, 286, 465, 3432, 20899, 14909, 16545, 11, 290, 362, 8, 2687, 508, 468, 281, 20899, 14909, 1848, 290, 973, 511, 4036, 5205, 743, 991, 423, 2728, 329, 2328, 13, 198, 198, 10260, 838, 14, 1983, 11, 838, 25, 2548, 257, 13, 76, 11207, 198, 198, 26754, 1636, 14909, 338, 11079, 1992, 286, 14620, 11, 3362, 3873, 540, 468, 2810, 262, 1708, 2882, 25, 198, 198, 3633, 356, 460, 470, 6216, 3025, 5504, 345, 1053, 1534, 3902, 287, 534, 2708, 11, 356, 460, 6216, 379, 645, 966, 547, 262, 734, 16545, 287, 1808, 7104, 11, 390, 33106, 393, 13140, 416, 262, 2836, 13, 198, 198, 13828, 561, 1612, 326, 428, 9412, 857, 407, 6646, 966, 284, 257, 649, 2324, 11804, 5115, 20899, 14909, 338, 1366, 39948, 1429, 11, 355, 5081, 4271, 13, 383, 1281, 468, 587, 6153, 284, 4079, 428, 13, 775, 13721, 262, 4049, 13, 198, 198, 10260, 352, 25, 3510, 279, 13, 76, 11207, 198, 198, 1212, 1281, 468, 587, 6153, 284, 4079, 262, 1109, 326, 674, 8171, 1706, 1043, 262, 5205, 1262, 1366, 422, 262, 2656, 8156, 13]
10.0.2.2: b"A little over two months ago, former conservative reality star, former executive director of FRC action, and current sex rehab resident Josh Duggar was one of the first major celebrities exposed in Ashley Madison's unprecedented data dump. At the time, we were able to confirm his two paid accounts using the associated credit card info. But now, thanks to tipster Nathan Turner, we also have his profile pictures. [See updates below.]\n\nThese accounts were not deleted, so any member of Ashley Madison looking for a match in the same cities these accounts are registered in would be able to see the profile picture. (**Update: Ashley Madison clarifies that users are able to control which pictures on their account are public and private; public profile pictures such as these can be found by all members searching the site, not just through their information that was hacked. See updates at the end of this post).\n\nThe profile pictures of the undeleted accounts were pulled up by our tipster with a free, unverified account using data from the hack.\n\nOf course any message you send won't actually be delivered to Duggar himself. But this does teach us at least two things: 1) Josh Duggar apparently used real (though slightly obscured) photos for both of his paid Ashley Madison profiles, and 2) anyone who has an Ashley Madison account and used their actual photos may still have cause for concern.\n\nUpdate 10/27, 10:38 a.m.:\n\nAshley Madison's Vice President of Communications, Paul Keable has provided the following response:\n\nWhile we can't confirm whose accounts you've profiled in your article, we can confirm at no point were the two profiles in question hidden, deactivated or deleted by the user.\n\nWhich would mean that this discovery does not necessarily point to a new security flaw regarding Ashley Madison's data deletion process, as stated previously. The post has been updated to reflect this. We regret the error.\n\nUpdate 1:46 p.m.:\n\nThis post has been updated to reflect the fact that our tipster found the photos using data from the original hack."
10.0.2.2: [8888, 319, 14044, 968, 2032, 557, 11, 356, 821, 7348, 262, 995, 338, 14162, 3227, 12170, 11, 9008, 510, 617, 19967, 283, 306, 9813, 351, 281, 1167, 49009, 11, 5186, 9053, 3526, 11, 290, 1976, 30602, 832, 22600, 319, 257, 5584, 1143, 3283, 12, 261, 13]
10.0.2.2: b"Today on Engineering Newswire, we're flying the world's fastest production drone, hitting up some gnarly waves with an inflatable, electric surfboard, and zooming through airports on a motorized carry-on."
10.0.2.2: [14214, 2550, 26718, 286, 26306, 8268, 39270, 13, 198, 14214, 2550, 26718, 286, 41080, 485, 11, 475, 407, 286, 40311, 393, 269, 274, 1505, 39270, 11, 318, 6515, 416, 257, 6087, 286, 44678, 540, 2928, 11538, 5444, 45943, 11081, 357, 8895, 1546, 8, 290, 49961, 4590, 9509, 1313, 5444, 45943, 11081, 357, 52, 3705, 7, 1544, 40, 4008, 286, 716, 13425, 516, 4735, 1660, 7362, 284, 327, 82, 40, 393, 327, 82, 37, 20199, 13, 383, 976, 4417, 22088, 291, 4069, 318, 635, 10944, 422, 18955, 17262, 357, 12740, 8, 27785, 286, 262, 11188, 257, 4188, 516, 8268, 8136, 13, 383, 337, 11015, 2482, 905, 262, 41121, 286, 41080, 485, 11, 475, 407, 40311, 11, 329, 262, 4417, 286, 262, 716, 13425, 516, 4735, 1660, 2646, 11, 4955, 4145, 1913, 2370, 329, 262, 5220, 4931, 286, 20140, 10284, 1460, 357, 2101, 485, 11, 275, 398, 485, 11, 290, 284, 257, 14494, 6287, 44921, 8, 379, 262, 1353, 1712, 7679, 286, 257, 4188, 516, 16649, 13, 554, 6273, 11, 645, 5763, 540, 4417, 26718, 286, 39270, 318, 6515, 287, 1138, 7637, 349, 11, 6159, 287, 262, 6306, 4249, 287, 262, 18640, 13, 11399, 11, 262, 1944, 2482, 7603, 326, 11, 355, 1290, 355, 262, 21969, 34743, 7612, 286, 1540, 10473, 286, 32915, 7344, 10284, 1460, 389, 5213, 11, 716, 13425, 516, 4735, 1660, 290, 1138, 7637, 349, 16649, 17438, 12470, 355, 16649, 286, 262, 11188, 38236, 13]
10.0.2.2: b'Surface segregation of dissolved salt ions.\nSurface segregation of iodide, but not of fluoride or cesium ions, is observed by a combination of metastable impact electron spectroscopy (MIES) and ultraviolet photoelectron spectroscopy (UPS(HeI)) of amorphous solid water exposed to CsI or CsF vapor. The same surface ionic behavior is also derived from molecular dynamics (MD) simulations of the corresponding aqueous salt solutions. The MIES results show the propensity of iodide, but not fluoride, for the surface of the amorphous solid water film, providing thus strong evidence for the suggested presence of heavier halides (iodide, bromide, and to a lesser extent chloride) at the topmost layer of aqueous surfaces. In contrast, no appreciable surface segregation of ions is observed in methanol, neither in the experiment nor in the simulation. Furthermore, the present results indicate that, as far as the thermodynamic aspects of solvation of alkali halides are concerned, amorphous solid water and methanol surfaces behave similarly as surfaces of the corresponding liquids.'
10.0.2.2: [14967, 14863, 8121, 318, 281, 17911, 7451, 290, 16713, 7451, 1912, 379, 28199, 2059, 13, 554, 465, 649, 1492, 11, 383, 35941, 317, 431, 11, 339, 11673, 326, 262, 2589, 674, 2471, 8952, 18668, 2540, 442, 4501, 379, 300, 8142, 286, 7815, 284, 2251, 511, 717, 4899, 11, 484, 2716, 257, 2700, 532, 3037, 532, 326, 468, 2826, 257, 28992, 2597, 287, 23610, 262, 1692, 4693, 13, 8013, 25438, 423, 14294, 262, 835, 356, 43414, 674, 20791, 11, 8335, 674, 2057, 11, 779, 674, 4202, 290, 4474, 13817, 13, 775, 750, 407, 8067, 3037, 11, 428, 2026, 12, 1941, 12, 727, 11444, 11673, 13, 8987, 15646, 514, 13, 198, 198, 2396, 644, 17218, 466, 356, 651, 656, 1692, 3450, 618, 356, 804, 379, 262, 2597, 286, 3037, 287, 674, 6954, 30, 198, 198, 1858, 318, 257, 11202, 326, 3037, 532, 422, 262, 7593, 5854, 284, 262, 3644, 2479, 532, 468, 6451, 1234, 514, 656, 257, 649, 995, 11, 530, 326, 318, 257, 1643, 14343, 13, 775, 5490, 326, 9061, 1244, 1011, 514, 625, 11, 329, 1672, 13, 887, 340, 373, 1683, 4145, 13, 383, 34306, 41500, 318, 257, 1720, 286, 262, 13360, 286, 3037, 13, 632, 42185, 2817, 674, 6954, 290, 2900, 514, 656, 257, 4047, 12661, 7185, 13, 1320, 318, 1521, 314, 6901, 41500, 31841, 10465, 355, 281, 11666, 43835, 13, 198, 198, 2215, 750, 428, 1429, 2221, 30, 198, 198, 1135, 460, 766, 422, 262, 30971, 2370, 326, 416, 362, 13, 21, 76, 812, 11, 674, 38132, 1373, 404, 31470, 66, 500, 2471, 8952, 18668, 550, 4499, 703, 284, 11594, 379, 14966, 290, 787, 4899, 13, 7413, 788, 11, 484, 550, 973, 14966, 284, 2005, 1243, 475, 783, 484, 547, 1682, 23610, 10340, 286, 7815, 284, 6050, 2972, 3544, 13, 1320, 373, 262, 8780, 2589, 11, 262, 530, 326, 13973, 257, 1919, 5854, 13, 198, 198, 818, 644, 835, 30, 198, 198, 5779, 11, 530, 1593, 2478, 561, 423, 587, 262, 5103, 286, 262, 717, 1017, 654, 329, 6872, 1088, 23856, 11903, 13, 9170, 606, 11, 1466, 561, 423, 37328, 517, 10685, 2568, 6872, 511, 1751, 287, 511, 5101, 621, 484, 561, 423, 973, 319, 4955, 606, 351, 7545, 11, 319, 26222, 341, 13, 887, 783, 11, 611, 345, 550, 4899, 284, 787, 46842, 11, 345, 714, 1494, 4695, 290, 4781, 511, 25873, 351, 262, 23110, 345, 550, 4499, 703, 284, 787, 290, 685, 6738, 262, 25873, 60, 345, 714, 787, 257, 46704, 351, 543, 284, 3283, 534, 5156, 13, 198, 198, 464, 10939, 286, 428, 2478, 547, 9812, 13, 632, 4001, 326, 11903, 714, 2555, 284, 1205, 2354, 262, 31514, 706, 4082, 290, 326, 511, 14290, 714, 2555, 284, 1663, 13, 1119, 547, 407, 31070, 416, 262, 2546, 286, 511, 12289, 6, 16176, 85, 2696, 290, 714, 1663, 5749, 290, 5749, 329, 812, 13, 632, 2921, 514, 8354, 329, 9028, 7118, 13, 775, 714, 1577, 4082, 284, 1751, 508, 547, 39186, 739, 33082, 475, 3025, 14290, 714, 2555, 284, 1205, 2354, 262, 31514, 13, 198, 198, 1135, 460, 691, 13249, 326, 11, 286, 1781, 13, 383, 25873, 393, 1490, 2189, 64, 326, 1244, 423, 587, 973, 355, 1017, 654, 423, 890, 1201, 875, 16548, 13, 198, 198, 818, 3090, 11, 996, 11, 4899, 2810, 514, 351, 262, 3777, 356, 973, 284, 1494, 4695, 3025, 6174, 2810, 262, 7532, 12, 7527, 18977, 326, 547, 3306, 329, 674, 14290, 284, 4292, 625, 262, 304, 684, 13, 6660, 3037, 1309, 9155, 7767, 326, 2957, 284, 514, 21568, 4025, 290, 4025, 14290, 13, 632, 857, 407, 4727, 1521, 356, 4166, 1263, 14290, 11, 475, 340, 2523, 703, 3037, 2727, 262, 2272, 287, 543, 326, 7118, 714, 3051, 13, 198, 198, 1135, 4398, 470, 3114, 736, 1201, 788, 30, 198, 198, 5779, 11, 645, 11, 407, 2407, 13, 554, 1109, 11, 3632, 2546, 468, 11832, 4622, 625, 262, 1613, 1542, 11, 830, 812, 290, 314, 892, 326, 468, 257, 1256, 284, 466, 351, 3037, 13, 2750, 326, 2278, 287, 674, 6954, 11, 257, 11527, 805, 645, 2392, 2622, 284, 3505, 703, 867, 44268, 256, 385, 591, 339, 373, 21057, 416, 1194, 11527, 805, 13, 679, 714, 1317, 326, 319, 262, 7714, 286, 465, 11527, 351, 7521, 13, 775, 550, 4251, 262, 3800, 810, 356, 550, 4499, 284, 779, 14354, 13, 1406, 3037, 468, 2904, 2067, 284, 1011, 1497, 257, 1310, 286, 674, 761, 329, 1588, 14290, 13, 9676, 11, 356, 389, 783, 44333, 674, 10878, 3007, 379, 257, 3744, 290, 3744, 2866, 11, 351, 262, 2478, 286, 3665, 2614, 9061, 11, 329, 1672, 13, 1406, 314, 561, 4331, 326, 11, 287, 262, 890, 1057, 11, 5384, 389, 1016, 284, 2555, 284, 651, 1342, 44479, 12661, 13, 770, 318, 407, 6646, 257, 2089, 1517, 11, 2158, 13, 198, 198, 19626, 262, 1672, 286, 2951, 432, 13, 1550, 2811, 11, 340, 318, 2192, 43229, 329, 674, 4693, 13, 1002, 314, 550, 284, 7866, 416, 852, 1498, 284, 4136, 257, 20096, 290, 788, 2686, 340, 866, 351, 257, 9563, 290, 15452, 11, 314, 2192, 3636, 470, 307, 994, 13, 887, 314, 460, 407, 691, 766, 20096, 11, 314, 460, 766, 33712, 290, 12899, 27982, 532, 1262, 21145, 13920, 290, 42067, 532, 780, 286, 616, 5307, 16357, 2776, 351, 3037, 13, 1406, 287, 867, 2842, 616, 2951, 432, 318, 1365, 621, 257, 11527, 805, 338, 393, 257, 19177, 12, 41268, 48386, 338, 11, 475, 691, 287, 2846, 286, 502, 852, 257, 3182, 1258, 1349, 2770, 7185, 13, 9561, 326, 616, 1176, 468, 7083, 13, 1320, 318, 1521, 314, 1561, 546, 514, 852, 262, 34968, 43835, 287, 262, 28690, 2565, 475, 11, 351, 3037, 11, 262, 12841, 13, 198, 198, 19926, 356, 307, 7960, 546, 674, 3957, 21403, 319, 3037, 30, 198, 198, 464, 3280, 318, 3763, 393, 645, 11, 6906, 319, 534, 24323, 393, 32277, 1042, 13, 632, 714, 307, 326, 287, 262, 12899, 2003, 3668, 481, 1716, 26329, 5976, 4674, 329, 9265, 290, 611, 262, 3037, 284, 1037, 514, 2666, 857, 407, 2152, 11, 356, 743, 4191, 33505, 284, 257, 36972, 1918, 13, 1114, 1672, 11, 1231, 3037, 11, 356, 481, 407, 423, 262, 1724, 284, 1730, 351, 262, 1306, 3236, 19999, 578, 326, 6665, 674, 835, 13, 1550]
10.0.2.2: b"Timothy Taylor is an anthropologist and archaeologist based at Bradford University. In his new book, The Artificial Ape, he argues that the moment our apemen ancestors began chipping at lumps of stone to create their first tools, they released a force - technology - that has played a pivotal role in shaping the human species. Such innovations have altered the way we nurture our offspring, prepare our food, use our strength and establish cultures. We did not invent technology, this 50-year-old scientist argues. Technology invented us.\n\nSo what insights do we get into human nature when we look at the role of technology in our evolution?\n\nThere is a perception that technology - from the industrial revolution to the computer age - has suddenly put us into a new world, one that is a bit scary. We worry that computers might take us over, for example. But it was ever thus. The genus Homo is a product of the realm of technology. It underpinned our evolution and turned us into a highly intelligent creature. That is why I describe Homo sapiens as an artificial ape.\n\nWhen did this process begin?\n\nWe can see from the archaeological evidence that by 2.6m years, our australopithecine apemen ancestors had learned how to chip at stones and make tools. Before then, they had used stones to cut things but now they were actually shaping bits of stone to suit various uses. That was the crucial moment, the one that triggered a social revolution.\n\nIn what way?\n\nWell, one important development would have been the construction of the first slings for carrying around newborn babies. Without them, women would have expended more biological energy carrying their children in their arms than they would have used on providing them with milk, on lactation. But now, if you had tools to make spears, you could kill animals and remove their skins with the knives you had learned how to make and [from the skins] you could make a sling with which to carry your baby.\n\nThe implications of this development were enormous. It meant that babies could continue to develop outside the womb after birth and that their brains could continue to grow. They were not constrained by the size of their mothers' pelvises and could grow bigger and bigger for years. It gave us scope for intellectual expansion. We could give birth to children who were intellectually underdeveloped but whose brains could continue to develop outside the womb.\n\nWe can only infer that, of course. The skins or viscera that might have been used as slings have long since decayed.\n\nIn addition, though, tools provided us with the weapons we used to kill animals whose meat provided the protein-rich diets that were necessary for our brains to expand over the eons. Thus technology let loose processes that led to us evolving larger and larger brains. It does not explain why we developed big brains, but it shows how technology created the space in which that expansion could occur.\n\nWe haven't looked back since then?\n\nWell, no, not quite. In fact, brain size has decreased slightly over the past 30,000 years and I think that has a lot to do with technology. By that period in our evolution, a caveman no longer needed to remember how many mammoth tusks he was owed by another caveman. He could mark that on the walls of his cave with paint. We had reached the stage where we had learned to use symbols. So technology has recently started to take away a little of our need for large brains. Indeed, we are now outsourcing our intelligences at a greater and greater speed, with the development of powerful personal computers, for example. So I would predict that, in the long run, humans are going to continue to get less biologically intelligent. This is not necessarily a bad thing, however.\n\nConsider the example of eyesight. On average, it is probably deteriorating for our species. If I had to survive by being able to spot a deer and then shoot it down with a bow and arrow, I probably wouldn't be here. But I can not only see deer, I can see microbes and distant galaxies - using microscopes and telescopes - because of my symbiotic relationship with technology. So in many ways my eyesight is better than a caveman's or a hunter-gatherer's, but only in terms of me being a biotechnological creature. Through that my power has extended. That is why I talk about us being the weakest ape in the innate sense but, with technology, the strongest.\n\nShould we be worried about our growing dependence on technology?\n\nThe answer is yes or no, depending on your optimism or pessimism. It could be that in the distant future Earth will become uninhabitable for humanity and if the technology to help us leave does not exist, we may eventually succumb to a dusty death. For example, without technology, we will not have the means to deal with the next huge meteorite that heads our way. On"
10.0.2.2: [1, 464, 3188, 2492, 470, 1760, 287, 640, 329, 262, 38180, 553, 36184, 6523, 8078, 520, 13, 1757, 531, 287, 14409, 284, 262, 10362, 2708, 13, 366, 464, 2635, 2097, 1239, 2497, 340, 11, 523, 484, 1422, 470, 2512, 340, 526, 198, 198, 818, 262, 649, 1339, 11, 10362, 531, 6193, 6154, 379, 262, 2351, 10692, 291, 290, 41516, 8694, 851, 636, 286, 262, 16127, 2732, 851, 287, 3945, 900, 510, 257, 3598, 12, 19522, 6103, 284, 8335, 257, 11529, 989, 319, 262, 5009, 286, 4086, 5519, 546, 3298, 9917, 290, 40370, 13, 198, 198, 818, 1737, 11, 618, 262, 989, 373, 2938, 284, 307, 2716, 11, 6103, 5118, 3738, 82, 1004, 316, 2611, 64, 2722, 281, 304, 12, 4529, 422, 257, 16127, 1743, 2282, 262, 989, 2622, 284, 307, 925, 1342, 6276, 290, 373, 407, 284, 307, 2716, 11, 10362, 2098, 13, 198, 198, 15285, 3838, 22998, 39708, 406, 2306, 268, 65, 3493, 318, 3058, 503, 286, 262, 1499, 11, 475, 10362, 10947, 683, 355, 2282, 262, 989, 373, 6974, 281, 5387, 3188, 290, 714, 407, 307, 2716, 780, 262, 4086, 714, 407, 1011, 281, 1743, 2292, 319, 262, 2071, 13, 198, 198, 464, 989, 9859, 257, 6152, 2882, 422, 2311, 13, 5278, 406, 2306, 23140, 360, 12, 45, 13, 41, 1539, 508, 5047, 326, 366, 1169, 3662, 468, 6840, 6875, 1175, 319, 3783, 290, 3872, 284, 5963, 663, 3098, 12, 38986, 8666, 2644, 262, 5511, 3662, 4477, 284, 42081, 5519, 508, 423, 12395, 262, 1459, 12751, 286, 3298, 9917, 526, 198, 198, 5703, 734, 2745, 2084, 11, 4837, 531, 326, 749, 286, 262, 2620, 287, 9151, 5951, 326, 21318, 517, 8157, 40370, 318, 257, 1255, 286, 1692, 12, 17223, 3298, 9917, 11, 257, 2050, 530, 13453, 531, 366, 565, 4629, 262, 9052, 1, 1022, 4258, 1487, 290, 3665, 20228, 588, 33251, 13, 198, 198, 464, 5885, 286, 3298, 9917, 13891, 40370, 318, 11889, 8564, 780, 262, 3662, 468, 26643, 11628, 284, 4239, 2650, 286, 21678, 326, 460, 2728, 9917, 3403, 13]
10.0.2.2: b'"The document wasn\'t done in time for the rollout," NOAA spokesman Jordan St. John said in responding to the Nature article. "The White House never saw it, so they didn\'t block it."\n\nIn the new case, Nature said weather experts at the National Oceanic and Atmospheric Administration \xe2\x80\x94 part of the Commerce Department \xe2\x80\x94 in February set up a seven-member panel to prepare a consensus report on the views of agency scientists about global warming and hurricanes.\n\nIn May, when the report was expected to be released, panel chair Ants Leetmaa received an e-mail from a Commerce official saying the report needed to be made less technical and was not to be released, Nature reported.\n\nNOAA Administrator Conrad Lautenbacher is currently out of the country, but Nature quoted him as saying the report was merely an internal document and could not be released because the agency could not take an official position on the issue.\n\nThe report drew a prompt response from Sen. Frank Lautenberg D-N.J., who charged that "the administration has effectively declared war on science and truth to advance its anti-environment agenda ... the Bush administration continues to censor scientists who have documented the current impacts of global warming."\n\nJust two weeks ago, researchers said that most of the increase in ocean temperature that feeds more intense hurricanes is a result of human-induced global warming, a study one researcher said "closes the loop" between climate change and powerful storms like Katrina.\n\nThe possibility of global warming affecting hurricanes is politically sensitive because the administration has resisted proposals to restrict release of gases that can cause warming conditions.'
10.0.2.2: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.2: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.2: Load language detection model
10.0.2.8: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.8: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.8: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.8: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.2: Dataset document count 2381340, token count 3258369020, non sentence start0.0
10.0.2.2: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.2: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.2: Load language detection model
10.0.2.2: Dataset document count 125466, token count 171431539, non sentence start0.0
10.0.2.2: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.2: Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
10.0.2.2: Load language detection model
10.0.2.2: Dataset document count 2510, token count 4247522, non sentence start0.0
10.0.2.2: BERT prob 0.5, gap sent prob 0.3, GPT prob 0.2, infill prob 0.5
10.0.2.2: generation min ratio 0.25, block ratio 0.15, gap sent ratio 0.15
10.0.2.2: block length distribution [0.14936120510359185, 0.22404180765538775, 0.22404180765538775, 0.16803135574154085, 0.10081881344492458, 0.05040940672246224, 0.02160403145248382, 0.008101511794681432, 0.002700503931560479, 0.0008101511794681439, 0.00022095032167312998, 5.523758041828258e-05, 1.2747133942680616e-05, 2.7315287020029775e-06, 5.463057404005949e-07, 1.0243232632511163e-07, 1.807629288090209e-08, 3.0127154801503496e-09, 4.756919179184761e-10, 7.135378768777153e-11, 1.01933982411102e-11, 1.3900088510604783e-12, 1.8130550231223652e-13, 2.26631877890296e-14, 2.719582534683569e-15, 3.1379798477117986e-16, 3.486644275235373e-17, 3.735690294894986e-18, 3.8645072016155465e-19, 3.864507201615528e-20, 3.739845678982777e-21, 3.5061053240463625e-22, 3.18736847640571e-23, 2.8123839497698246e-24, 2.410614814088421e-25, 2.0088456784069862e-26, 1.6287937933030137e-27, 1.285889836818143e-28, 9.89146028321656e-30]
10.0.2.2: block mask prob 0.1, context mask ratio 0.0
10.0.2.2: BERT prob 0.5, gap sent prob 0.3, GPT prob 0.2, infill prob 0.5
10.0.2.2: generation min ratio 0.25, block ratio 0.15, gap sent ratio 0.15
10.0.2.2: block length distribution [0.14936120510359185, 0.22404180765538775, 0.22404180765538775, 0.16803135574154085, 0.10081881344492458, 0.05040940672246224, 0.02160403145248382, 0.008101511794681432, 0.002700503931560479, 0.0008101511794681439, 0.00022095032167312998, 5.523758041828258e-05, 1.2747133942680616e-05, 2.7315287020029775e-06, 5.463057404005949e-07, 1.0243232632511163e-07, 1.807629288090209e-08, 3.0127154801503496e-09, 4.756919179184761e-10, 7.135378768777153e-11, 1.01933982411102e-11, 1.3900088510604783e-12, 1.8130550231223652e-13, 2.26631877890296e-14, 2.719582534683569e-15, 3.1379798477117986e-16, 3.486644275235373e-17, 3.735690294894986e-18, 3.8645072016155465e-19, 3.864507201615528e-20, 3.739845678982777e-21, 3.5061053240463625e-22, 3.18736847640571e-23, 2.8123839497698246e-24, 2.410614814088421e-25, 2.0088456784069862e-26, 1.6287937933030137e-27, 1.285889836818143e-28, 9.89146028321656e-30]
10.0.2.2: block mask prob 0.1, context mask ratio 0.0
10.0.2.2: BERT prob 0.5, gap sent prob 0.3, GPT prob 0.2, infill prob 0.5
10.0.2.2: generation min ratio 0.25, block ratio 0.15, gap sent ratio 0.15
10.0.2.2: block length distribution [0.14936120510359185, 0.22404180765538775, 0.22404180765538775, 0.16803135574154085, 0.10081881344492458, 0.05040940672246224, 0.02160403145248382, 0.008101511794681432, 0.002700503931560479, 0.0008101511794681439, 0.00022095032167312998, 5.523758041828258e-05, 1.2747133942680616e-05, 2.7315287020029775e-06, 5.463057404005949e-07, 1.0243232632511163e-07, 1.807629288090209e-08, 3.0127154801503496e-09, 4.756919179184761e-10, 7.135378768777153e-11, 1.01933982411102e-11, 1.3900088510604783e-12, 1.8130550231223652e-13, 2.26631877890296e-14, 2.719582534683569e-15, 3.1379798477117986e-16, 3.486644275235373e-17, 3.735690294894986e-18, 3.8645072016155465e-19, 3.864507201615528e-20, 3.739845678982777e-21, 3.5061053240463625e-22, 3.18736847640571e-23, 2.8123839497698246e-24, 2.410614814088421e-25, 2.0088456784069862e-26, 1.6287937933030137e-27, 1.285889836818143e-28, 9.89146028321656e-30]
10.0.2.2: block mask prob 0.1, context mask ratio 0.0
10.0.2.2: building GPT2 model ...
10.0.2.2:  > number of parameters on model parallel rank 4: 1243471872
10.0.2.2:  > number of parameters on model parallel rank 6: 1243471872
10.0.2.2:  > number of parameters on model parallel rank 5: 1243471872
10.0.2.2:  > number of parameters on model parallel rank 2: 1243471872
10.0.2.2:  > number of parameters on model parallel rank 0: 1243471872
10.0.2.2:  > number of parameters on model parallel rank 3: 1243471872
10.0.2.2:  > number of parameters on model parallel rank 7: 1243471872
10.0.2.2:  > number of parameters on model parallel rank 1: 1243471872
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:59:57] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:59:57] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Warning]: Can not build MLU-Link HAT ring, use PCIe or network instead.
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 14:59:57] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 14:59:58] [LOG_CNCL] [Warning]: Do not support all-connect topo with multi-node communication.
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Warning]: Can not build MLU-Link HAT ring, use PCIe or network instead.
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Warning]: Do not support all-connect topo with multi-node communication.
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 14:59:59] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Warning]: Can not build MLU-Link HAT ring, use PCIe or network instead.
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 14:59:59] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Do not support all-connect topo with multi-node communication.
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: DeepSpeed is enabled.
10.0.2.2: [2022-04-01 15:00:00,047] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.3.13+unknown, git-hash=unknown, git-branch=unknown
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Can not build MLU-Link HAT ring, use PCIe or network instead.
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Do not support all-connect topo with multi-node communication.
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Can not build MLU-Link HAT ring, use PCIe or network instead.
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:0] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Can not build MLU-Link HAT ring, use PCIe or network instead.
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Do not support all-connect topo with multi-node communication.
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Do not support all-connect topo with multi-node communication.
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Can not build MLU-Link HAT ring, use PCIe or network instead.
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Do not support all-connect topo with multi-node communication.
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Can not build MLU-Link HAT ring, use PCIe or network instead.
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 3--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->0
10.0.2.8: Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...
10.0.2.8: Building extension module utils...
10.0.2.8: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
10.0.2.2: [4-1 15:0:0] [LOG_CNCL] [Warning]: Do not support all-connect topo with multi-node communication.
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...
10.0.2.2: Building extension module utils...
10.0.2.2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
10.0.2.8: ninja: no work to do.
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.30760836601257324 seconds
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0004761219024658203 seconds
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.3017268180847168 seconds
10.0.2.2: ninja: no work to do.
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.2871525287628174 seconds
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.20166563987731934 seconds
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.00047850608825683594 seconds
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0004980564117431641 seconds
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0004267692565917969 seconds
10.0.2.2: [2022-04-01 15:00:00,994] [INFO] [engine.py:608:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer
10.0.2.2: [2022-04-01 15:00:00,995] [INFO] [engine.py:612:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam
10.0.2.2: Checking ZeRO support for optimizer=FusedAdam type=<class 'cnmix.torch.optimizers.fused_adam.FusedAdam'>
10.0.2.2: [2022-04-01 15:00:00,995] [INFO] [logging.py:60:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...
10.0.2.8: Building extension module utils...
10.0.2.8: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
10.0.2.2: Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...
10.0.2.2: Building extension module utils...
10.0.2.2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
10.0.2.2: ninja: no work to do.
10.0.2.8: ninja: no work to do.
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.2827110290527344 seconds
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.2976672649383545 seconds
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.30171966552734375 seconds
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.3020048141479492 seconds
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.00051116943359375 seconds
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0005247592926025391 seconds
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.00045680999755859375 seconds
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0005071163177490234 seconds
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...
10.0.2.8: Building extension module utils...
10.0.2.8: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Building extension module utils...
10.0.2.2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: ninja: no work to do.
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.30127406120300293 seconds
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: ninja: no work to do.
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Loading extension module utils...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Time to load utils op: 0.3144824504852295 seconds
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0005388259887695312 seconds
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.3016507625579834 seconds
10.0.2.2: Loading extension module utils...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.10155582427978516 seconds
10.0.2.2: Time to load utils op: 0.10139918327331543 seconds
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0005517005920410156 seconds
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0005185604095458984 seconds
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0004425048828125 seconds
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0004677772521972656 seconds
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...
10.0.2.8: Building extension module utils...
10.0.2.8: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...
10.0.2.2: Building extension module utils...
10.0.2.2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: ninja: no work to do.
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.3005373477935791 seconds
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.40180301666259766 seconds
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.1014699935913086 seconds
10.0.2.8: Loading extension module utils...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.6026515960693359 seconds
10.0.2.8: Time to load utils op: 0.3015604019165039 seconds
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.40163087844848633 seconds
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.3015108108520508 seconds
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.30167078971862793 seconds
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: ninja: no work to do.
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.3049459457397461 seconds
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.00047206878662109375 seconds
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0004165172576904297 seconds
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.4023301601409912 seconds
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.5017552375793457 seconds
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Loading extension module utils...
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Time to load utils op: 0.5019540786743164 seconds
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.00046372413635253906 seconds
10.0.2.2: Loading extension module utils...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: Loading extension module utils...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.00040221214294433594 seconds
10.0.2.2: Time to load utils op: 0.2014307975769043 seconds
10.0.2.2: Time to load utils op: 0.20144319534301758 seconds
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0004367828369140625 seconds
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0004210472106933594 seconds
10.0.2.8: Loading extension module utils...
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0003924369812011719 seconds
10.0.2.8: Time to load utils op: 0.20147705078125 seconds
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0004031658172607422 seconds
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.2015674114227295 seconds
10.0.2.2: [2022-04-01 15:00:02,292] [INFO] [stage2.py:136:__init__] Reduce bucket size 50000000
10.0.2.2: [2022-04-01 15:00:02,292] [INFO] [stage2.py:137:__init__] Allgather bucket size 500000000
10.0.2.2: [2022-04-01 15:00:02,292] [INFO] [stage2.py:138:__init__] CPU Offload: False
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0004734992980957031 seconds
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.30164432525634766 seconds
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.20164036750793457 seconds
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0003962516784667969 seconds
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.4020071029663086 seconds
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0003993511199951172 seconds
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.00043892860412597656 seconds
10.0.2.8: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.8:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0003948211669921875 seconds
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.00044989585876464844 seconds
10.0.2.8: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.8: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.8: Loading extension module utils...
10.0.2.8: Time to load utils op: 0.0003981590270996094 seconds
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: [2022-04-01 15:00:02,354] [INFO] [stage2.py:399:__init__] optimizer state initialized
10.0.2.2: [2022-04-01 15:00:02,354] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
10.0.2.2: [2022-04-01 15:00:02,354] [INFO] [engine.py:449:_configure_lr_scheduler] DeepSpeed using client LR scheduler
10.0.2.2: [2022-04-01 15:00:02,354] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
10.0.2.2: [2022-04-01 15:00:02,354] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 15:00:02,354] [INFO] [config.py:737:print] DeepSpeedEngine configuration:
10.0.2.2: [2022-04-01 15:00:02,354] [INFO] [config.py:741:print]   activation_checkpointing_config  {
10.0.2.2:     "contiguous_memory_optimization": false,
10.0.2.2:     "cpu_checkpointing": false,
10.0.2.2:     "number_checkpoints": null,
10.0.2.2:     "partition_activations": false,
10.0.2.2:     "profile": false,
10.0.2.2:     "synchronize_checkpoint_boundary": false
10.0.2.2: }
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   allreduce_always_fp32 ........ False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   amp_enabled .................. False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   amp_params ................... False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   checkpoint_tag_validation_enabled  True
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   checkpoint_tag_validation_fail  False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   disable_allgather ............ False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   dump_state ................... False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   elasticity_enabled ........... False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   flops_profiler_config ........ {
10.0.2.2:     "detailed": true,
10.0.2.2:     "enabled": false,
10.0.2.2:     "module_depth": -1,
10.0.2.2:     "profile_step": 1,
10.0.2.2:     "top_modules": 3
10.0.2.2: }
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   fp16_enabled ................. True
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   global_rank .................. 0
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   gradient_accumulation_steps .. 1
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   gradient_clipping ............ 1.0
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   gradient_predivide_factor .... 1.0
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   initial_dynamic_scale ........ 4294967296
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   loss_scale ................... 0
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   memory_breakdown ............. False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   optimizer_legacy_fusion ...... False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   optimizer_name ............... adam
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.95], 'eps': 1e-08, 'weight_decay': 0.1}
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   pld_enabled .................. False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   pld_params ................... False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   prescale_gradients ........... False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   scheduler_name ............... None
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   scheduler_params ............. None
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   sparse_attention ............. None
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   sparse_gradients_enabled ..... False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   steps_per_print .............. 50
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   tensorboard_enabled .......... False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   tensorboard_job_name ......... DeepSpeedJobName
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   tensorboard_output_path ......
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   train_batch_size ............. 84
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   train_micro_batch_size_per_gpu  21
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   wall_clock_breakdown ......... False
10.0.2.2: [2022-04-01 15:00:02,355] [INFO] [config.py:741:print]   world_size ................... 4
10.0.2.2: [2022-04-01 15:00:02,356] [INFO] [config.py:741:print]   zero_allow_untested_optimizer  False
10.0.2.2: [2022-04-01 15:00:02,356] [INFO] [config.py:741:print]   zero_config .................. {
10.0.2.2:     "allgather_bucket_size": 500000000,
10.0.2.2:     "allgather_partitions": true,
10.0.2.2:     "contiguous_gradients": true,
10.0.2.2:     "cpu_offload": false,
10.0.2.2:     "cpu_offload_params": false,
10.0.2.2:     "cpu_offload_use_pin_memory": false,
10.0.2.2:     "elastic_checkpoint": true,
10.0.2.2:     "load_from_fp32_weights": true,
10.0.2.2:     "max_live_parameters": 1000000000,
10.0.2.2:     "max_reuse_distance": 1000000000,
10.0.2.2:     "overlap_comm": true,
10.0.2.2:     "param_persistence_threshold": 100000,
10.0.2.2:     "prefetch_bucket_size": 50000000,
10.0.2.2:     "reduce_bucket_size": 50000000,
10.0.2.2:     "reduce_scatter": true,
10.0.2.2:     "stage": 2,
10.0.2.2:     "sub_group_size": 1000000000000
10.0.2.2: }
10.0.2.2: [2022-04-01 15:00:02,356] [INFO] [config.py:741:print]   zero_enabled ................. True
10.0.2.2: [2022-04-01 15:00:02,356] [INFO] [config.py:741:print]   zero_optimization_stage ...... 2
10.0.2.2: [2022-04-01 15:00:02,356] [INFO] [config.py:748:print]   json = {
10.0.2.2:     "activation_checkpointing":{
10.0.2.2:         "contiguous_memory_optimization":false,
10.0.2.2:         "partition_activations":false
10.0.2.2:     },
10.0.2.2:     "fp16":{
10.0.2.2:         "enabled":true,
10.0.2.2:         "hysteresis":2,
10.0.2.2:         "loss_scale":0,
10.0.2.2:         "loss_scale_window":1000,
10.0.2.2:         "min_loss_scale":1
10.0.2.2:     },
10.0.2.2:     "gradient_accumulation_steps":1,
10.0.2.2:     "gradient_clipping":1.0,
10.0.2.2:     "optimizer":{
10.0.2.2:         "params":{
10.0.2.2:             "betas":[
10.0.2.2:                 0.9,
10.0.2.2:                 0.95
10.0.2.2:             ],
10.0.2.2:             "eps":1e-08,
10.0.2.2:             "lr":0.0001,
10.0.2.2:             "weight_decay":0.1
10.0.2.2:         },
10.0.2.2:         "type":"Adam"
10.0.2.2:     },
10.0.2.2:     "steps_per_print":50,
10.0.2.2:     "train_micro_batch_size_per_gpu":21,
10.0.2.2:     "wall_clock_breakdown":false,
10.0.2.2:     "zero_optimization":{
10.0.2.2:         "allgather_bucket_size":500000000,
10.0.2.2:         "contiguous_gradients":true,
10.0.2.2:         "overlap_comm":true,
10.0.2.2:         "reduce_bucket_size":50000000,
10.0.2.2:         "reduce_scatter":true,
10.0.2.2:         "stage":2
10.0.2.2:     }
10.0.2.2: }
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.00033020973205566406 seconds
10.0.2.2: learning rate decaying style cosine, ratio 10.0
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.8: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0004215240478515625 seconds
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: /torch/venv3/pytorch/lib/python3.6/site-packages/torch_mlu/core/device/notifier.py:9: UserWarning: torch_mlu.core.device.Notifier is deprecated, please use torch.mlu.Event instead
10.0.2.2:   warnings.warn("torch_mlu.core.device.Notifier is deprecated, please use "
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.00041222572326660156 seconds
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: Using /root/.cache/torch_extensions as PyTorch extensions root...
10.0.2.2: No modifications detected for re-loaded extension module utils, skipping build step...
10.0.2.2: Loading extension module utils...
10.0.2.2: Time to load utils op: 0.0004322528839111328 seconds
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: CNCL_LOG_LEVEL: INFO
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Warning]: Can not build MLU-Link HAT ring, use PCIe or network instead.
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 31--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->4
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 3--[REMOTE_DMA]->4--[REMOTE_DMA]->5
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 4--[REMOTE_DMA]->5--[REMOTE_DMA]->6
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 5--[REMOTE_DMA]->6--[REMOTE_DMA]->7
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 6--[REMOTE_DMA]->7--[REMOTE_DMA]->8
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 7--[REMOTE_DMA]->8--[REMOTE_DMA]->9
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 8--[REMOTE_DMA]->9--[REMOTE_DMA]->10
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 9--[REMOTE_DMA]->10--[REMOTE_DMA]->11
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 10--[REMOTE_DMA]->11--[REMOTE_DMA]->12
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 11--[REMOTE_DMA]->12--[REMOTE_DMA]->13
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 12--[REMOTE_DMA]->13--[REMOTE_DMA]->14
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 13--[REMOTE_DMA]->14--[REMOTE_DMA]->15
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 14--[REMOTE_DMA]->15--[REMOTE_DMA]->16
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 15--[REMOTE_DMA]->16--[REMOTE_DMA]->17
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 16--[REMOTE_DMA]->17--[REMOTE_DMA]->18
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 17--[REMOTE_DMA]->18--[REMOTE_DMA]->19
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 18--[REMOTE_DMA]->19--[REMOTE_DMA]->20
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 19--[REMOTE_DMA]->20--[REMOTE_DMA]->21
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 20--[REMOTE_DMA]->21--[REMOTE_DMA]->22
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 21--[REMOTE_DMA]->22--[REMOTE_DMA]->23
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 22--[REMOTE_DMA]->23--[REMOTE_DMA]->24
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 23--[REMOTE_DMA]->24--[REMOTE_DMA]->25
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 24--[REMOTE_DMA]->25--[REMOTE_DMA]->26
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 25--[REMOTE_DMA]->26--[REMOTE_DMA]->27
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 26--[REMOTE_DMA]->27--[REMOTE_DMA]->28
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 27--[REMOTE_DMA]->28--[REMOTE_DMA]->29
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 28--[REMOTE_DMA]->29--[REMOTE_DMA]->30
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 29--[REMOTE_DMA]->30--[REMOTE_DMA]->31
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 0: 30--[REMOTE_DMA]->31--[REMOTE_DMA]->0
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 31--[REMOTE_DMA]->0--[REMOTE_DMA]->1
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 0--[REMOTE_DMA]->1--[REMOTE_DMA]->2
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 1--[REMOTE_DMA]->2--[REMOTE_DMA]->3
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 2--[REMOTE_DMA]->3--[REMOTE_DMA]->4
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 3--[REMOTE_DMA]->4--[REMOTE_DMA]->5
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 4--[REMOTE_DMA]->5--[REMOTE_DMA]->6
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 5--[REMOTE_DMA]->6--[REMOTE_DMA]->7
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 6--[REMOTE_DMA]->7--[REMOTE_DMA]->8
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 7--[REMOTE_DMA]->8--[REMOTE_DMA]->9
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 8--[REMOTE_DMA]->9--[REMOTE_DMA]->10
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 9--[REMOTE_DMA]->10--[REMOTE_DMA]->11
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 10--[REMOTE_DMA]->11--[REMOTE_DMA]->12
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 11--[REMOTE_DMA]->12--[REMOTE_DMA]->13
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 12--[REMOTE_DMA]->13--[REMOTE_DMA]->14
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 13--[REMOTE_DMA]->14--[REMOTE_DMA]->15
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 14--[REMOTE_DMA]->15--[REMOTE_DMA]->16
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 15--[REMOTE_DMA]->16--[REMOTE_DMA]->17
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 16--[REMOTE_DMA]->17--[REMOTE_DMA]->18
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 17--[REMOTE_DMA]->18--[REMOTE_DMA]->19
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 18--[REMOTE_DMA]->19--[REMOTE_DMA]->20
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 19--[REMOTE_DMA]->20--[REMOTE_DMA]->21
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 20--[REMOTE_DMA]->21--[REMOTE_DMA]->22
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 21--[REMOTE_DMA]->22--[REMOTE_DMA]->23
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 22--[REMOTE_DMA]->23--[REMOTE_DMA]->24
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 23--[REMOTE_DMA]->24--[REMOTE_DMA]->25
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 24--[REMOTE_DMA]->25--[REMOTE_DMA]->26
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 25--[REMOTE_DMA]->26--[REMOTE_DMA]->27
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 26--[REMOTE_DMA]->27--[REMOTE_DMA]->28
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 27--[REMOTE_DMA]->28--[REMOTE_DMA]->29
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 28--[REMOTE_DMA]->29--[REMOTE_DMA]->30
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 29--[REMOTE_DMA]->30--[REMOTE_DMA]->31
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Info]: Build 2 rings. Ring 1: 30--[REMOTE_DMA]->31--[REMOTE_DMA]->0
10.0.2.2: [4-1 15:0:2] [LOG_CNCL] [Warning]: Do not support all-connect topo with multi-node communication.
10.0.2.2: Pretrain GPT2 model
10.0.2.2: arguments:
10.0.2.2:   transformer_xl ............... False
10.0.2.2:   pretrained_bert .............. False
10.0.2.2:   encoder_decoder .............. False
10.0.2.2:   attention_dropout ............ 0.1
10.0.2.2:   num_attention_heads .......... 64
10.0.2.2:   hidden_size .................. 4096
10.0.2.2:   intermediate_size ............ None
10.0.2.2:   num_layers ................... 48
10.0.2.2:   layernorm_epsilon ............ 1e-05
10.0.2.2:   hidden_dropout ............... 0.1
10.0.2.2:   output_dropout ............... 0.1
10.0.2.2:   max_position_embeddings ...... 1024
10.0.2.2:   vocab_size ................... 50304
10.0.2.2:   deep_init .................... False
10.0.2.2:   make_vocab_size_divisible_by . 128
10.0.2.2:   cpu_optimizer ................ False
10.0.2.2:   cpu_torch_adam ............... False
10.0.2.2:   fp16 ......................... True
10.0.2.2:   fp32_embedding ............... False
10.0.2.2:   fp32_layernorm ............... False
10.0.2.2:   fp32_tokentypes .............. False
10.0.2.2:   fp32_allreduce ............... False
10.0.2.2:   hysteresis ................... 2
10.0.2.2:   loss_scale ................... None
10.0.2.2:   loss_scale_window ............ 1000
10.0.2.2:   min_scale .................... 1
10.0.2.2:   attention_scale .............. 1.0
10.0.2.2:   experiment_name .............. blocklm-10b04-01-14-57
10.0.2.2:   batch_size ................... 21
10.0.2.2:   gradient_accumulation_steps .. 1
10.0.2.2:   weight_decay ................. 0.1
10.0.2.2:   checkpoint_activations ....... True
10.0.2.2:   checkpoint_num_layers ........ 1
10.0.2.2:   deepspeed_activation_checkpointing  True
10.0.2.2:   epochs ....................... None
10.0.2.2:   clip_grad .................... 1.0
10.0.2.2:   train_iters .................. 1000
10.0.2.2:   label_smoothing .............. 0.0
10.0.2.2:   log_interval ................. 1
10.0.2.2:   summary_dir ..................
10.0.2.2:   seed ......................... 1234
10.0.2.2:   reset_position_ids ........... False
10.0.2.2:   reset_attention_mask ......... False
10.0.2.2:   lr_decay_iters ............... 175000
10.0.2.2:   lr_decay_style ............... cosine
10.0.2.2:   lr_decay_ratio ............... 0.1
10.0.2.2:   lr ........................... 0.0001
10.0.2.2:   warmup ....................... 0.04
10.0.2.2:   switch_linear ................ False
10.0.2.2:   save ......................... ./checkpoints/blocklm-10b04-01-14-57
10.0.2.2:   new_save_directory ........... False
10.0.2.2:   save_epoch ................... 1
10.0.2.2:   save_interval ................ 2000
10.0.2.2:   no_save_optim ................ False
10.0.2.2:   no_save_rng .................. False
10.0.2.2:   load ......................... None
10.0.2.2:   no_load_optim ................ False
10.0.2.2:   no_load_rng .................. False
10.0.2.2:   no_load_lr_scheduler ......... False
10.0.2.2:   no_deepspeed_load ............ False
10.0.2.2:   finetune ..................... False
10.0.2.2:   resume_dataloader ............ True
10.0.2.2:   distributed_backend .......... cncl
10.0.2.2:   DDP_impl ..................... torch
10.0.2.2:   local_rank ................... 0
10.0.2.2:   block_lm ..................... True
10.0.2.2:   masked_lm .................... False
10.0.2.2:   bert_prob .................... 0.5
10.0.2.2:   gpt_infill_prob .............. 0.5
10.0.2.2:   gpt_min_ratio ................ 0.25
10.0.2.2:   gap_sentence_prob ............ 0.3
10.0.2.2:   gap_sentence_ratio ........... 0.15
10.0.2.2:   avg_block_length ............. 3
10.0.2.2:   short_seq_prob ............... 0.02
10.0.2.2:   single_span_prob ............. 0.0
10.0.2.2:   task_mask .................... True
10.0.2.2:   no_shuffle_block ............. False
10.0.2.2:   no_block_position ............ False
10.0.2.2:   sentinel_token ............... False
10.0.2.2:   block_mask_prob .............. 0.1
10.0.2.2:   context_mask_ratio ........... 0.0
10.0.2.2:   random_position .............. False
10.0.2.2:   eval_batch_size .............. None
10.0.2.2:   eval_iters ................... 100
10.0.2.2:   eval_interval ................ 1000
10.0.2.2:   eval_epoch ................... 1
10.0.2.2:   eval_seq_length .............. None
10.0.2.2:   eval_max_preds_per_seq ....... None
10.0.2.2:   overlapping_eval ............. 32
10.0.2.2:   temperature .................. 1.0
10.0.2.2:   top_p ........................ 0.0
10.0.2.2:   top_k ........................ 0
10.0.2.2:   out_seq_length ............... 256
10.0.2.2:   num_beams .................... 1
10.0.2.2:   length_penalty ............... 0.0
10.0.2.2:   no_repeat_ngram_size ......... 0
10.0.2.2:   min_tgt_length ............... 0
10.0.2.2:   select_topk .................. False
10.0.2.2:   blank_maskratio .............. 0.1
10.0.2.2:   model_parallel_size .......... 8
10.0.2.2:   shuffle ...................... False
10.0.2.2:   filter_english ............... True
10.0.2.2:   train_data ................... ['pile']
10.0.2.2:   valid_data ................... None
10.0.2.2:   test_data .................... None
10.0.2.2:   data_dir ..................... None
10.0.2.2:   input_data_sizes_file ........ sizes.txt
10.0.2.2:   delim ........................ ,
10.0.2.2:   text_key ..................... sentence
10.0.2.2:   eval_text_key ................ None
10.0.2.2:   split ........................ 949,50,1
10.0.2.2:   no_lazy_loader ............... False
10.0.2.2:   half_lazy_loader ............. False
10.0.2.2:   loader_scatter ............... 2
10.0.2.2:   loose_json ................... False
10.0.2.2:   presplit_sentences ........... False
10.0.2.2:   num_workers .................. 2
10.0.2.2:   tokenizer_model_type ......... None
10.0.2.2:   tokenizer_path ............... tokenizer.model
10.0.2.2:   tokenizer_type ............... GPT2BPETokenizer
10.0.2.2:   no_pre_tokenize .............. False
10.0.2.2:   cache_dir .................... None
10.0.2.2:   use_tfrecords ................ False
10.0.2.2:   seq_length ................... 512
10.0.2.2:   mem_length ................... 0
10.0.2.2:   max_preds_per_seq ............ None
10.0.2.2:   non_sentence_start ........... 0.0
10.0.2.2:   sample_one_document .......... False
10.0.2.2:   load_splits .................. None
10.0.2.2:   save_splits .................. None
10.0.2.2:   save_test_data ............... None
10.0.2.2:   multi_task_data .............. None
10.0.2.2:   multi_task_ratio ............. 0.0
10.0.2.2:   multi_seq_length ............. None
10.0.2.2:   multi_batch_size ............. None
10.0.2.2:   task ......................... None
10.0.2.2:   load_pretrained .............. None
10.0.2.2:   pool_token ................... cls
10.0.2.2:   cloze_eval ................... False
10.0.2.2:   multi_token .................. False
10.0.2.2:   segment_length ............... 0
10.0.2.2:   loss_func .................... cross_entropy
10.0.2.2:   block_lm_ratio ............... 0.0
10.0.2.2:   adapet ....................... False
10.0.2.2:   pattern_id ................... 0
10.0.2.2:   fast_decode .................. False
10.0.2.2:   few_superglue ................ False
10.0.2.2:   eval_valid ................... False
10.0.2.2:   validation_metric ............ None
10.0.2.2:   unidirectional ............... False
10.0.2.2:   src_seq_length ............... None
10.0.2.2:   tgt_seq_length ............... None
10.0.2.2:   adam_beta1 ................... 0.9
10.0.2.2:   adam_beta2 ................... 0.999
10.0.2.2:   adam_eps ..................... 1e-08
10.0.2.2:   optimizer .................... adam
10.0.2.2:   wsc_negative ................. False
10.0.2.2:   overwrite .................... False
10.0.2.2:   no_validation ................ False
10.0.2.2:   continuous_prompt ............ False
10.0.2.2:   num_prompt_tokens ............ 0
10.0.2.2:   prompt_func .................. lstm
10.0.2.2:   freeze_transformer ........... False
10.0.2.2:   tune_prefix_layers ........... None
10.0.2.2:   prefix_prompt ................ 0
10.0.2.2:   prompt_init .................. False
10.0.2.2:   deepspeed .................... True
10.0.2.2:   deepspeed_config ............. /home/glm_project/glm/config/config_block_10B.json
10.0.2.2:   deepscale .................... False
10.0.2.2:   deepscale_config ............. None
10.0.2.2:   deepspeed_mpi ................ False
10.0.2.2:   cuda ......................... False
10.0.2.2:   rank ......................... 0
10.0.2.2:   world_size ................... 32
10.0.2.2:   dynamic_loss_scale ........... True
10.0.2.2:   master_ip .................... 10.0.2.2
10.0.2.2:   master_port .................. 49204
10.0.2.2:   eod_token .................... 50256
10.0.2.2:   persist_state ................ 0
10.0.2.2:   lazy ......................... False
10.0.2.2:   transpose .................... False
10.0.2.2:   data_set_type ................ Block
10.0.2.2:   samples_per_shard ............ 100
10.0.2.2:   do_train ..................... 1
10.0.2.2:   do_valid ..................... 1
10.0.2.2:   do_test ...................... 1
10.0.2.2:   iteration .................... 0
10.0.2.2:   log_dir ...................... runs/blocklm-10b04-01-14-57
10.0.2.2: Resume dataloader
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.[2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.[2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2:
10.0.2.2:
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.[2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2:
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-04-01 15:00:03,219] [INFO] [checkpointing.py:429:forward] Activation Checkpointing Information
10.0.2.2: [2022-04-01 15:00:03,220] [INFO] [checkpointing.py:431:forward] ----Partition Activations False, CPU CHECKPOINTING False
10.0.2.2: [2022-04-01 15:00:03,220] [INFO] [checkpointing.py:434:forward] ----contiguous Memory Checkpointing False with None total layers
10.0.2.2: [2022-04-01 15:00:03,220] [INFO] [checkpointing.py:436:forward] ----Synchronization False
10.0.2.2: [2022-04-01 15:00:03,220] [INFO] [checkpointing.py:437:forward] ----Profiling False
10.0.2.2: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.[2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8:
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlFill] is deprecated and will be removed in the future release, please use [cnnlFill_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.8: [2022-4-1 15:0:3] [CNNL] [Warning]: [cnnlSetActivationDescriptor] is deprecated and will be removed in the future release, plsase use [cnnlSetActivationDescriptor_v3] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.[2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8:
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlExp] is deprecated and will be removed in the future release, please use [cnnlExp_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.8: [2022-4-1 15:0:4] [CNNL] [Warning]: [cnnlDiv] is deprecated and will be removed in the future release, please use [cnnlDiv_v2] instead.
10.0.2.2: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.8: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2: [2022-04-01 15:00:13,195] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
10.0.2.2:  iteration        1/    1000 | elapsed time per iteration (ms): 10248.7 | learning rate 0.000E+00 | lm loss 1.183861E+01 | loss scale 4294967296.0 |
10.0.2.2: after 1 iterations memory (MB) | allocated: 5930.6337890625 | max allocated: 13026.0419921875 | cached: 18324.0
10.0.2.2: time (ms) | forward: 3203.24 | backward: 6920.38 | optimizer: 15.55 | batch generator: 259.61 | data loader: 247.93
10.0.2.2: [2022-04-01 15:00:22,890] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,890] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,890] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,890] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,890] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,890] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,890] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,890] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,890] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,890] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.8: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2: [2022-04-01 15:00:22,891] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
10.0.2.2:  iteration        2/    1000 | elapsed time per iteration (ms): 9696.9 | learning rate 0.000E+00 | lm loss 1.176851E+01 | loss scale 2147483648.0 |
10.0.2.2: time (ms) | forward: 2867.17 | backward: 6815.30 | optimizer: 6.47 | batch generator: 2.97 | data loader: 0.36
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,625] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,625] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,624] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,625] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,625] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.8: [2022-04-01 15:00:32,625] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
10.0.2.2:  iteration        3/    1000 | elapsed time per iteration (ms): 9732.0 | learning rate 0.000E+00 | lm loss 1.182024E+01 | loss scale 1073741824.0 |
10.0.2.2: time (ms) | forward: 2882.07 | backward: 6824.58 | optimizer: 8.15 | batch generator: 4.08 | data loader: 0.80
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.8: [2022-04-01 15:00:41,633] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
10.0.2.2:  iteration        4/    1000 | elapsed time per iteration (ms): 9009.2 | learning rate 0.000E+00 | lm loss 1.172883E+01 | loss scale 536870912.0 |
10.0.2.2: time (ms) | forward: 2670.10 | backward: 6316.23 | optimizer: 6.79 | batch generator: 2.92 | data loader: 0.35
10.0.2.2: [2022-04-01 15:00:51,549] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,549] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,549] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,549] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,549] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,549] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,549] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.8: [2022-04-01 15:00:51,550] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
10.0.2.2:  iteration        5/    1000 | elapsed time per iteration (ms): 9916.1 | learning rate 0.000E+00 | lm loss 1.180736E+01 | loss scale 268435456.0 |
10.0.2.2: time (ms) | forward: 2946.33 | backward: 6944.32 | optimizer: 5.54 | batch generator: 3.36 | data loader: 0.59
10.0.2.2: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,403] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.8: [2022-04-01 15:01:01,402] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,403] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,403] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,403] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,403] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,403] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,403] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,403] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2: [2022-04-01 15:01:01,403] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
10.0.2.2:  iteration        6/    1000 | elapsed time per iteration (ms): 9853.2 | learning rate 0.000E+00 | lm loss 1.175143E+01 | loss scale 134217728.0 |
10.0.2.2: time (ms) | forward: 2742.94 | backward: 6883.80 | optimizer: 10.20 | batch generator: 2.79 | data loader: 0.43
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.8: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,269] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2: [2022-04-01 15:01:11,270] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
10.0.2.2:  iteration        7/    1000 | elapsed time per iteration (ms): 9866.1 | learning rate 0.000E+00 | lm loss 1.180609E+01 | loss scale 67108864.0 |
10.0.2.2: time (ms) | forward: 2905.96 | backward: 6927.51 | optimizer: 4.42 | batch generator: 4.22 | data loader: 0.44
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.8: [2022-04-01 15:01:21,142] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
10.0.2.2:  iteration        8/    1000 | elapsed time per iteration (ms): 9872.3 | learning rate 0.000E+00 | lm loss 1.175412E+01 | loss scale 33554432.0 |
10.0.2.2: time (ms) | forward: 2613.98 | backward: 6907.94 | optimizer: 5.13 | batch generator: 3.05 | data loader: 0.40
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2:  iteration        9/    1000 | elapsed time per iteration (ms): 9713.9 | learning rate 0.000E+00 | lm loss 1.175778E+01 | loss scale 16777216.0 |
10.0.2.8: [2022-04-01 15:01:30,856] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
10.0.2.2: time (ms) | forward: 2915.13 | backward: 6791.32 | optimizer: 4.52 | batch generator: 3.22 | data loader: 0.39
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.8: [2022-04-01 15:01:40,918] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
10.0.2.2:  iteration       10/    1000 | elapsed time per iteration (ms): 10062.3 | learning rate 0.000E+00 | lm loss 1.177229E+01 | loss scale 8388608.0 |
10.0.2.2: time (ms) | forward: 2556.25 | backward: 7098.50 | optimizer: 5.01 | batch generator: 2.54 | data loader: 0.38
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.8: [2022-04-01 15:01:50,990] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
10.0.2.2:  iteration       11/    1000 | elapsed time per iteration (ms): 10071.5 | learning rate 0.000E+00 | lm loss 1.190211E+01 | loss scale 4194304.0 |
10.0.2.2: time (ms) | forward: 2894.24 | backward: 7111.22 | optimizer: 8.06 | batch generator: 3.00 | data loader: 0.40
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,141] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,141] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,140] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.8: [2022-04-01 15:02:01,141] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2: [2022-04-01 15:02:01,141] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
10.0.2.2:  iteration       12/    1000 | elapsed time per iteration (ms): 10151.0 | learning rate 0.000E+00 | lm loss 1.180715E+01 | loss scale 2097152.0 |
10.0.2.2: time (ms) | forward: 2588.03 | backward: 7174.99 | optimizer: 6.21 | batch generator: 3.29 | data loader: 0.40
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2:  iteration       13/    1000 | elapsed time per iteration (ms): 10033.1 | learning rate 0.000E+00 | lm loss 1.182891E+01 | loss scale 1048576.0 |
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.8: [2022-04-01 15:02:11,174] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
10.0.2.2: time (ms) | forward: 2703.95 | backward: 7066.29 | optimizer: 6.82 | batch generator: 2.71 | data loader: 0.39
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,193] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.8: [2022-04-01 15:02:21,194] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
10.0.2.2:  iteration       14/    1000 | elapsed time per iteration (ms): 10019.5 | learning rate 0.000E+00 | lm loss 1.180837E+01 | loss scale 524288.0 |
10.0.2.2: time (ms) | forward: 2661.73 | backward: 7059.06 | optimizer: 34.32 | batch generator: 2.86 | data loader: 0.39
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.8: [2022-04-01 15:02:31,180] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
10.0.2.2:  iteration       15/    1000 | elapsed time per iteration (ms): 9987.2 | learning rate 0.000E+00 | lm loss 1.178798E+01 | loss scale 262144.0 |
10.0.2.2: time (ms) | forward: 2923.87 | backward: 7012.68 | optimizer: 38.40 | batch generator: 2.71 | data loader: 0.38
10.0.2.2: [2022-04-01 15:02:41,206] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,206] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,206] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2:  iteration       16/    1000 | elapsed time per iteration (ms): 10025.6 | learning rate 0.000E+00 | lm loss 1.174920E+01 | loss scale 131072.0 |
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.8: [2022-04-01 15:02:41,207] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
10.0.2.2: time (ms) | forward: 2940.06 | backward: 7047.79 | optimizer: 34.37 | batch generator: 2.92 | data loader: 0.48
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.8: [2022-04-01 15:02:51,242] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
10.0.2.2:  iteration       17/    1000 | elapsed time per iteration (ms): 10035.5 | learning rate 0.000E+00 | lm loss 1.181842E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2922.69 | backward: 7031.81 | optimizer: 33.35 | batch generator: 2.90 | data loader: 0.29
10.0.2.2:  iteration       18/    1000 | elapsed time per iteration (ms): 11398.5 | learning rate 1.429E-08 | lm loss 1.184177E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2651.23 | backward: 7134.50 | optimizer: 1277.68 | batch generator: 2.69 | data loader: 0.39
10.0.2.2:  iteration       19/    1000 | elapsed time per iteration (ms): 11094.1 | learning rate 2.857E-08 | lm loss 1.179515E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2525.50 | backward: 7144.02 | optimizer: 983.60 | batch generator: 2.85 | data loader: 0.36
10.0.2.2:  iteration       20/    1000 | elapsed time per iteration (ms): 11243.0 | learning rate 4.286E-08 | lm loss 1.186366E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2966.35 | backward: 7138.90 | optimizer: 984.78 | batch generator: 6.59 | data loader: 0.67
10.0.2.2:  iteration       21/    1000 | elapsed time per iteration (ms): 10820.8 | learning rate 5.714E-08 | lm loss 1.172958E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2676.06 | backward: 6920.01 | optimizer: 992.46 | batch generator: 6.34 | data loader: 0.67
10.0.2.2:  iteration       22/    1000 | elapsed time per iteration (ms): 11018.9 | learning rate 7.143E-08 | lm loss 1.177246E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2933.65 | backward: 7056.37 | optimizer: 989.01 | batch generator: 4.46 | data loader: 0.62
10.0.2.2:  iteration       23/    1000 | elapsed time per iteration (ms): 10075.2 | learning rate 8.571E-08 | lm loss 1.165803E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2603.08 | backward: 6444.65 | optimizer: 982.44 | batch generator: 7.41 | data loader: 0.80
10.0.2.2:  iteration       24/    1000 | elapsed time per iteration (ms): 11048.0 | learning rate 1.000E-07 | lm loss 1.161466E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2695.96 | backward: 7060.74 | optimizer: 995.96 | batch generator: 3.43 | data loader: 0.67
10.0.2.2:  iteration       25/    1000 | elapsed time per iteration (ms): 11087.1 | learning rate 1.143E-07 | lm loss 1.100919E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2944.22 | backward: 7128.06 | optimizer: 998.02 | batch generator: 4.78 | data loader: 0.69
10.0.2.2:  iteration       26/    1000 | elapsed time per iteration (ms): 11021.5 | learning rate 1.286E-07 | lm loss 1.119264E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2690.26 | backward: 7048.03 | optimizer: 996.61 | batch generator: 3.60 | data loader: 0.77
10.0.2.2:  iteration       27/    1000 | elapsed time per iteration (ms): 10855.0 | learning rate 1.429E-07 | lm loss 1.087708E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2724.12 | backward: 6969.63 | optimizer: 980.78 | batch generator: 3.71 | data loader: 0.68
10.0.2.2:  iteration       28/    1000 | elapsed time per iteration (ms): 10141.7 | learning rate 1.571E-07 | lm loss 1.102195E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2592.51 | backward: 6466.41 | optimizer: 986.00 | batch generator: 4.61 | data loader: 0.74
10.0.2.2:  iteration       29/    1000 | elapsed time per iteration (ms): 11027.8 | learning rate 1.714E-07 | lm loss 1.033000E+01 | loss scale 65536.0 |
10.0.2.2: time (ms) | forward: 2720.51 | backward: 7050.25 | optimizer: 987.75 | batch generator: 3.51 | data loader: 0.76
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2: [2022-04-01 15:05:12,226] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2:  iteration       30/    1000 | elapsed time per iteration (ms): 10151.6 | learning rate 1.714E-07 | lm loss 9.704139E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2648.60 | backward: 7111.72 | optimizer: 38.95 | batch generator: 5.05 | data loader: 0.85
10.0.2.8: [2022-04-01 15:05:12,227] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
10.0.2.2:  iteration       31/    1000 | elapsed time per iteration (ms): 11093.8 | learning rate 1.857E-07 | lm loss 9.338732E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2542.66 | backward: 7130.11 | optimizer: 994.29 | batch generator: 3.18 | data loader: 0.43
10.0.2.2:  iteration       32/    1000 | elapsed time per iteration (ms): 11019.2 | learning rate 2.000E-07 | lm loss 9.626313E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2911.70 | backward: 7056.47 | optimizer: 985.09 | batch generator: 3.39 | data loader: 0.68
10.0.2.2:  iteration       33/    1000 | elapsed time per iteration (ms): 11143.8 | learning rate 2.143E-07 | lm loss 9.436672E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2581.06 | backward: 7169.08 | optimizer: 991.45 | batch generator: 3.21 | data loader: 0.71
10.0.2.2:  iteration       34/    1000 | elapsed time per iteration (ms): 10343.6 | learning rate 2.286E-07 | lm loss 9.630325E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2582.37 | backward: 6619.29 | optimizer: 987.94 | batch generator: 6.87 | data loader: 0.71
10.0.2.2:  iteration       35/    1000 | elapsed time per iteration (ms): 10832.5 | learning rate 2.429E-07 | lm loss 9.208185E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2878.86 | backward: 6962.11 | optimizer: 987.55 | batch generator: 4.74 | data loader: 0.85
10.0.2.2:  iteration       36/    1000 | elapsed time per iteration (ms): 11119.6 | learning rate 2.571E-07 | lm loss 8.651358E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2922.10 | backward: 7149.08 | optimizer: 986.94 | batch generator: 5.53 | data loader: 0.69
10.0.2.2:  iteration       37/    1000 | elapsed time per iteration (ms): 10154.0 | learning rate 2.714E-07 | lm loss 9.789668E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2542.17 | backward: 6509.19 | optimizer: 984.67 | batch generator: 4.35 | data loader: 0.63
10.0.2.2:  iteration       38/    1000 | elapsed time per iteration (ms): 10972.5 | learning rate 2.857E-07 | lm loss 9.162799E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2551.97 | backward: 7043.57 | optimizer: 978.63 | batch generator: 3.69 | data loader: 0.80
10.0.2.2:  iteration       39/    1000 | elapsed time per iteration (ms): 11090.8 | learning rate 3.000E-07 | lm loss 8.724504E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2918.99 | backward: 7159.68 | optimizer: 980.46 | batch generator: 4.70 | data loader: 0.69
10.0.2.2:  iteration       40/    1000 | elapsed time per iteration (ms): 10964.9 | learning rate 3.143E-07 | lm loss 8.526272E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2915.93 | backward: 7018.32 | optimizer: 999.47 | batch generator: 5.75 | data loader: 0.48
10.0.2.2:  iteration       41/    1000 | elapsed time per iteration (ms): 11038.8 | learning rate 3.286E-07 | lm loss 8.252905E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2956.15 | backward: 7093.99 | optimizer: 985.46 | batch generator: 4.28 | data loader: 0.53
10.0.2.2:  iteration       42/    1000 | elapsed time per iteration (ms): 11093.0 | learning rate 3.429E-07 | lm loss 9.123638E+00 | loss scale 32768.0 |
10.0.2.2: time (ms) | forward: 2606.42 | backward: 7126.73 | optimizer: 994.17 | batch generator: 3.38 | data loader: 0.56
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 3 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 1 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 4 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 6 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 8 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 7 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 11 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 2 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 5 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 14 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 19 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 22 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 28 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 15 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 27 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 12 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 17 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 21 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 18 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 26 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 30 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 16 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 24 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 13 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 9 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 10 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,185] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 31 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 23 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,184] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 20 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2:  iteration       43/    1000 | elapsed time per iteration (ms): 10089.7 | learning rate 3.429E-07 | lm loss 9.787616E+00 | loss scale 16384.0 |
10.0.2.8: [2022-04-01 15:07:33,185] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 25 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.8: [2022-04-01 15:07:33,185] [INFO] [stage2.py:1394:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 29 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
10.0.2.2: time (ms) | forward: 2942.91 | backward: 7099.69 | optimizer: 43.60 | batch generator: 3.59 | data loader: 0.77
10.0.2.2:  iteration       44/    1000 | elapsed time per iteration (ms): 10926.2 | learning rate 3.571E-07 | lm loss 9.340295E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2573.62 | backward: 6997.83 | optimizer: 981.85 | batch generator: 2.65 | data loader: 0.33
10.0.2.2:  iteration       45/    1000 | elapsed time per iteration (ms): 11219.4 | learning rate 3.714E-07 | lm loss 9.946453E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 3024.87 | backward: 7203.74 | optimizer: 987.61 | batch generator: 6.94 | data loader: 0.74
10.0.2.2:  iteration       46/    1000 | elapsed time per iteration (ms): 11079.5 | learning rate 3.857E-07 | lm loss 8.305489E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2649.36 | backward: 7094.77 | optimizer: 992.29 | batch generator: 6.73 | data loader: 0.76
10.0.2.2:  iteration       47/    1000 | elapsed time per iteration (ms): 10918.0 | learning rate 4.000E-07 | lm loss 9.038996E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2592.35 | backward: 7012.66 | optimizer: 983.40 | batch generator: 3.57 | data loader: 0.59
10.0.2.2:  iteration       48/    1000 | elapsed time per iteration (ms): 11005.7 | learning rate 4.143E-07 | lm loss 8.798014E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2943.89 | backward: 7056.92 | optimizer: 987.61 | batch generator: 13.41 | data loader: 0.76
10.0.2.2:  iteration       49/    1000 | elapsed time per iteration (ms): 11023.5 | learning rate 4.286E-07 | lm loss 8.196603E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2978.63 | backward: 7047.21 | optimizer: 994.22 | batch generator: 4.78 | data loader: 0.67
10.0.2.2: [2022-04-01 15:08:50,239] [INFO] [logging.py:60:log_dist] [Rank 0] step=50, skipped=19, lr=[4.2857142857142857e-07, 4.2857142857142857e-07], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 15:08:50,440] [INFO] [timer.py:164:stop] 0/50, SamplesPerSec=7.948861322598675
10.0.2.2:  iteration       50/    1000 | elapsed time per iteration (ms): 11082.8 | learning rate 4.429E-07 | lm loss 8.271378E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2643.73 | backward: 7102.69 | optimizer: 988.67 | batch generator: 6.28 | data loader: 0.48
10.0.2.2:  iteration       51/    1000 | elapsed time per iteration (ms): 10917.6 | learning rate 4.571E-07 | lm loss 7.532577E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2653.07 | backward: 6993.61 | optimizer: 982.79 | batch generator: 3.91 | data loader: 0.38
10.0.2.2:  iteration       52/    1000 | elapsed time per iteration (ms): 10320.2 | learning rate 4.714E-07 | lm loss 8.686605E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2663.35 | backward: 6627.17 | optimizer: 987.92 | batch generator: 5.25 | data loader: 0.73
10.0.2.2:  iteration       53/    1000 | elapsed time per iteration (ms): 11112.7 | learning rate 4.857E-07 | lm loss 8.447105E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2917.97 | backward: 7118.73 | optimizer: 996.76 | batch generator: 4.18 | data loader: 0.77
10.0.2.2:  iteration       54/    1000 | elapsed time per iteration (ms): 10999.2 | learning rate 5.000E-07 | lm loss 7.813034E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2688.19 | backward: 7048.82 | optimizer: 983.23 | batch generator: 4.03 | data loader: 0.58
10.0.2.2:  iteration       55/    1000 | elapsed time per iteration (ms): 10999.0 | learning rate 5.143E-07 | lm loss 7.754728E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2661.82 | backward: 7037.24 | optimizer: 991.83 | batch generator: 7.77 | data loader: 0.85
10.0.2.2:  iteration       56/    1000 | elapsed time per iteration (ms): 11209.4 | learning rate 5.286E-07 | lm loss 7.664151E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2986.80 | backward: 7203.08 | optimizer: 1005.56 | batch generator: 7.03 | data loader: 0.65
10.0.2.2:  iteration       57/    1000 | elapsed time per iteration (ms): 11046.6 | learning rate 5.429E-07 | lm loss 8.070938E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2965.02 | backward: 7093.65 | optimizer: 984.66 | batch generator: 4.46 | data loader: 1.31
10.0.2.2:  iteration       58/    1000 | elapsed time per iteration (ms): 10822.7 | learning rate 5.571E-07 | lm loss 8.759953E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2876.04 | backward: 6948.54 | optimizer: 995.69 | batch generator: 3.62 | data loader: 0.66
10.0.2.2:  iteration       59/    1000 | elapsed time per iteration (ms): 11013.5 | learning rate 5.714E-07 | lm loss 8.241187E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2959.43 | backward: 7053.02 | optimizer: 998.47 | batch generator: 3.63 | data loader: 0.75
10.0.2.2:  iteration       60/    1000 | elapsed time per iteration (ms): 10977.1 | learning rate 5.857E-07 | lm loss 8.227206E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2750.85 | backward: 7038.28 | optimizer: 982.13 | batch generator: 3.84 | data loader: 0.70
10.0.2.2:  iteration       61/    1000 | elapsed time per iteration (ms): 10928.8 | learning rate 6.000E-07 | lm loss 8.011648E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2628.60 | backward: 7003.13 | optimizer: 997.01 | batch generator: 7.34 | data loader: 0.84
10.0.2.2:  iteration       62/    1000 | elapsed time per iteration (ms): 11059.3 | learning rate 6.143E-07 | lm loss 8.335280E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2663.47 | backward: 7106.69 | optimizer: 1004.81 | batch generator: 4.01 | data loader: 0.64
10.0.2.2:  iteration       63/    1000 | elapsed time per iteration (ms): 11040.1 | learning rate 6.286E-07 | lm loss 8.258962E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2686.69 | backward: 7095.65 | optimizer: 985.29 | batch generator: 9.10 | data loader: 0.60
10.0.2.2:  iteration       64/    1000 | elapsed time per iteration (ms): 10679.7 | learning rate 6.429E-07 | lm loss 8.786755E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2686.20 | backward: 6816.30 | optimizer: 997.10 | batch generator: 6.28 | data loader: 0.61
10.0.2.2:  iteration       65/    1000 | elapsed time per iteration (ms): 10946.3 | learning rate 6.571E-07 | lm loss 8.108166E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2929.24 | backward: 7017.67 | optimizer: 994.14 | batch generator: 3.19 | data loader: 0.62
10.0.2.2:  iteration       66/    1000 | elapsed time per iteration (ms): 11036.0 | learning rate 6.714E-07 | lm loss 7.790966E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2755.19 | backward: 7090.04 | optimizer: 990.14 | batch generator: 5.10 | data loader: 0.78
10.0.2.2:  iteration       67/    1000 | elapsed time per iteration (ms): 11015.3 | learning rate 6.857E-07 | lm loss 7.961719E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2719.89 | backward: 7049.96 | optimizer: 988.77 | batch generator: 3.08 | data loader: 0.60
10.0.2.2:  iteration       68/    1000 | elapsed time per iteration (ms): 11056.5 | learning rate 7.000E-07 | lm loss 8.068945E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2970.75 | backward: 7087.54 | optimizer: 995.43 | batch generator: 3.88 | data loader: 0.86
10.0.2.2:  iteration       69/    1000 | elapsed time per iteration (ms): 11024.6 | learning rate 7.143E-07 | lm loss 8.130625E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2696.76 | backward: 7088.99 | optimizer: 992.67 | batch generator: 3.22 | data loader: 0.56
10.0.2.2:  iteration       70/    1000 | elapsed time per iteration (ms): 11078.8 | learning rate 7.286E-07 | lm loss 7.606467E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2651.55 | backward: 7109.88 | optimizer: 990.11 | batch generator: 4.30 | data loader: 0.82
10.0.2.2:  iteration       71/    1000 | elapsed time per iteration (ms): 10857.0 | learning rate 7.429E-07 | lm loss 8.007169E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2588.54 | backward: 6959.98 | optimizer: 989.23 | batch generator: 3.25 | data loader: 0.49
10.0.2.2:  iteration       72/    1000 | elapsed time per iteration (ms): 11127.0 | learning rate 7.571E-07 | lm loss 6.946220E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2934.78 | backward: 7161.73 | optimizer: 991.70 | batch generator: 8.90 | data loader: 1.00
10.0.2.2:  iteration       73/    1000 | elapsed time per iteration (ms): 11010.5 | learning rate 7.714E-07 | lm loss 7.786328E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2954.58 | backward: 7048.67 | optimizer: 990.28 | batch generator: 3.04 | data loader: 0.53
10.0.2.2:  iteration       74/    1000 | elapsed time per iteration (ms): 11034.0 | learning rate 7.857E-07 | lm loss 7.935613E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2925.19 | backward: 7087.49 | optimizer: 983.01 | batch generator: 5.22 | data loader: 1.35
10.0.2.2:  iteration       75/    1000 | elapsed time per iteration (ms): 10990.1 | learning rate 8.000E-07 | lm loss 7.495328E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2942.50 | backward: 7036.84 | optimizer: 989.74 | batch generator: 4.47 | data loader: 0.98
10.0.2.2:  iteration       76/    1000 | elapsed time per iteration (ms): 11004.6 | learning rate 8.143E-07 | lm loss 7.335707E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2948.77 | backward: 7058.37 | optimizer: 983.31 | batch generator: 7.45 | data loader: 0.61
10.0.2.2:  iteration       77/    1000 | elapsed time per iteration (ms): 11037.5 | learning rate 8.286E-07 | lm loss 8.245026E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2579.97 | backward: 7083.56 | optimizer: 986.89 | batch generator: 3.62 | data loader: 0.99
10.0.2.2:  iteration       78/    1000 | elapsed time per iteration (ms): 11088.0 | learning rate 8.429E-07 | lm loss 6.970088E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2888.88 | backward: 7131.68 | optimizer: 987.31 | batch generator: 7.04 | data loader: 0.63
10.0.2.2:  iteration       79/    1000 | elapsed time per iteration (ms): 30347.1 | learning rate 8.571E-07 | lm loss 6.938336E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2956.99 | backward: 25968.08 | optimizer: 1403.33 | batch generator: 3.48 | data loader: 0.72
10.0.2.2:  iteration       80/    1000 | elapsed time per iteration (ms): 11318.4 | learning rate 8.714E-07 | lm loss 7.563208E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2963.24 | backward: 7268.15 | optimizer: 1029.50 | batch generator: 32.79 | data loader: 1.29
10.0.2.2:  iteration       81/    1000 | elapsed time per iteration (ms): 11131.4 | learning rate 8.857E-07 | lm loss 7.360029E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2901.39 | backward: 7108.79 | optimizer: 1052.31 | batch generator: 4.28 | data loader: 1.01
10.0.2.2:  iteration       82/    1000 | elapsed time per iteration (ms): 11055.0 | learning rate 9.000E-07 | lm loss 7.950745E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2609.23 | backward: 7038.54 | optimizer: 1037.23 | batch generator: 4.95 | data loader: 0.94
10.0.2.2:  iteration       83/    1000 | elapsed time per iteration (ms): 11234.6 | learning rate 9.143E-07 | lm loss 7.700028E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2941.48 | backward: 7217.44 | optimizer: 1051.49 | batch generator: 9.42 | data loader: 0.94
10.0.2.2:  iteration       84/    1000 | elapsed time per iteration (ms): 21013.2 | learning rate 9.286E-07 | lm loss 8.445389E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2687.86 | backward: 17227.13 | optimizer: 1044.04 | batch generator: 4.62 | data loader: 1.08
10.0.2.2:  iteration       85/    1000 | elapsed time per iteration (ms): 11034.7 | learning rate 9.429E-07 | lm loss 7.507418E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2176.03 | backward: 7107.46 | optimizer: 985.60 | batch generator: 3.93 | data loader: 0.74
10.0.2.2:  iteration       86/    1000 | elapsed time per iteration (ms): 10975.2 | learning rate 9.571E-07 | lm loss 7.908524E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2641.88 | backward: 7034.97 | optimizer: 990.20 | batch generator: 5.38 | data loader: 0.84
10.0.2.2:  iteration       87/    1000 | elapsed time per iteration (ms): 10093.4 | learning rate 9.714E-07 | lm loss 8.674132E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2581.66 | backward: 6502.79 | optimizer: 984.44 | batch generator: 3.85 | data loader: 0.68
10.0.2.2:  iteration       88/    1000 | elapsed time per iteration (ms): 11084.4 | learning rate 9.857E-07 | lm loss 7.348385E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2876.15 | backward: 7112.54 | optimizer: 990.26 | batch generator: 6.46 | data loader: 0.65
10.0.2.2:  iteration       89/    1000 | elapsed time per iteration (ms): 11056.7 | learning rate 1.000E-06 | lm loss 7.376349E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2899.22 | backward: 7103.91 | optimizer: 990.95 | batch generator: 3.62 | data loader: 0.63
10.0.2.2:  iteration       90/    1000 | elapsed time per iteration (ms): 11030.7 | learning rate 1.014E-06 | lm loss 7.414065E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2957.35 | backward: 7064.93 | optimizer: 990.88 | batch generator: 6.06 | data loader: 0.70
10.0.2.2:  iteration       91/    1000 | elapsed time per iteration (ms): 10533.7 | learning rate 1.029E-06 | lm loss 7.786896E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2588.41 | backward: 6753.04 | optimizer: 989.79 | batch generator: 3.72 | data loader: 0.83
10.0.2.2:  iteration       92/    1000 | elapsed time per iteration (ms): 11059.8 | learning rate 1.043E-06 | lm loss 7.893105E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2581.78 | backward: 7072.55 | optimizer: 990.43 | batch generator: 3.54 | data loader: 0.63
10.0.2.2:  iteration       93/    1000 | elapsed time per iteration (ms): 11018.4 | learning rate 1.057E-06 | lm loss 7.947406E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2972.32 | backward: 7044.94 | optimizer: 998.65 | batch generator: 8.75 | data loader: 0.83
10.0.2.2:  iteration       94/    1000 | elapsed time per iteration (ms): 10963.1 | learning rate 1.071E-06 | lm loss 7.789286E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2686.18 | backward: 7028.57 | optimizer: 994.77 | batch generator: 4.56 | data loader: 0.67
10.0.2.2:  iteration       95/    1000 | elapsed time per iteration (ms): 10865.9 | learning rate 1.086E-06 | lm loss 7.833344E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2879.52 | backward: 6984.27 | optimizer: 999.39 | batch generator: 3.73 | data loader: 0.68
10.0.2.2:  iteration       96/    1000 | elapsed time per iteration (ms): 11103.5 | learning rate 1.100E-06 | lm loss 7.232502E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2654.29 | backward: 7123.79 | optimizer: 996.76 | batch generator: 3.95 | data loader: 0.52
10.0.2.2:  iteration       97/    1000 | elapsed time per iteration (ms): 10969.6 | learning rate 1.114E-06 | lm loss 7.336174E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2685.53 | backward: 7035.20 | optimizer: 996.45 | batch generator: 3.52 | data loader: 0.63
10.0.2.2:  iteration       98/    1000 | elapsed time per iteration (ms): 10887.3 | learning rate 1.129E-06 | lm loss 7.465465E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2932.36 | backward: 6961.11 | optimizer: 991.32 | batch generator: 3.74 | data loader: 0.83
10.0.2.2:  iteration       99/    1000 | elapsed time per iteration (ms): 11095.4 | learning rate 1.143E-06 | lm loss 7.706882E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2697.92 | backward: 7132.95 | optimizer: 989.21 | batch generator: 4.48 | data loader: 1.05
10.0.2.2: [2022-04-01 15:18:28,494] [INFO] [logging.py:60:log_dist] [Rank 0] step=100, skipped=19, lr=[1.1428571428571428e-06, 1.1428571428571428e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 15:18:28,682] [INFO] [timer.py:164:stop] 0/100, SamplesPerSec=7.58638057384534
10.0.2.2:  iteration      100/    1000 | elapsed time per iteration (ms): 10933.1 | learning rate 1.157E-06 | lm loss 7.442376E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2640.01 | backward: 6999.73 | optimizer: 986.25 | batch generator: 3.59 | data loader: 0.66
10.0.2.2:  iteration      101/    1000 | elapsed time per iteration (ms): 10366.5 | learning rate 1.171E-06 | lm loss 8.067780E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2571.28 | backward: 6641.00 | optimizer: 995.13 | batch generator: 5.59 | data loader: 0.51
10.0.2.2:  iteration      102/    1000 | elapsed time per iteration (ms): 10957.8 | learning rate 1.186E-06 | lm loss 7.568303E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2964.03 | backward: 6995.16 | optimizer: 996.36 | batch generator: 3.42 | data loader: 0.60
10.0.2.2:  iteration      103/    1000 | elapsed time per iteration (ms): 11060.2 | learning rate 1.200E-06 | lm loss 7.594111E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2625.66 | backward: 7093.19 | optimizer: 986.82 | batch generator: 5.76 | data loader: 0.67
10.0.2.2:  iteration      104/    1000 | elapsed time per iteration (ms): 11055.9 | learning rate 1.214E-06 | lm loss 7.318209E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2987.05 | backward: 7081.72 | optimizer: 983.87 | batch generator: 3.49 | data loader: 0.89
10.0.2.2:  iteration      105/    1000 | elapsed time per iteration (ms): 11055.6 | learning rate 1.229E-06 | lm loss 7.122692E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2681.39 | backward: 7078.75 | optimizer: 997.82 | batch generator: 7.58 | data loader: 0.69
10.0.2.2:  iteration      106/    1000 | elapsed time per iteration (ms): 10819.8 | learning rate 1.243E-06 | lm loss 7.875843E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2913.52 | backward: 6918.08 | optimizer: 985.03 | batch generator: 4.03 | data loader: 0.89
10.0.2.2:  iteration      107/    1000 | elapsed time per iteration (ms): 11056.0 | learning rate 1.257E-06 | lm loss 7.323044E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2664.00 | backward: 7090.78 | optimizer: 1002.48 | batch generator: 6.62 | data loader: 0.66
10.0.2.2:  iteration      108/    1000 | elapsed time per iteration (ms): 11081.5 | learning rate 1.271E-06 | lm loss 7.177762E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2953.71 | backward: 7135.65 | optimizer: 984.30 | batch generator: 3.78 | data loader: 0.72
10.0.2.2:  iteration      109/    1000 | elapsed time per iteration (ms): 11089.5 | learning rate 1.286E-06 | lm loss 7.289209E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2880.19 | backward: 7109.07 | optimizer: 1004.63 | batch generator: 4.99 | data loader: 0.70
10.0.2.2:  iteration      110/    1000 | elapsed time per iteration (ms): 11038.5 | learning rate 1.300E-06 | lm loss 7.256090E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2971.14 | backward: 7078.44 | optimizer: 985.65 | batch generator: 4.89 | data loader: 0.68
10.0.2.2:  iteration      111/    1000 | elapsed time per iteration (ms): 11039.9 | learning rate 1.314E-06 | lm loss 7.595111E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2967.20 | backward: 7071.76 | optimizer: 994.13 | batch generator: 5.38 | data loader: 0.76
10.0.2.2:  iteration      112/    1000 | elapsed time per iteration (ms): 11071.2 | learning rate 1.329E-06 | lm loss 7.121171E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2940.49 | backward: 7086.29 | optimizer: 993.55 | batch generator: 3.87 | data loader: 0.48
10.0.2.2:  iteration      113/    1000 | elapsed time per iteration (ms): 11054.3 | learning rate 1.343E-06 | lm loss 7.465108E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2968.72 | backward: 7082.28 | optimizer: 995.00 | batch generator: 4.56 | data loader: 0.68
10.0.2.2:  iteration      114/    1000 | elapsed time per iteration (ms): 10900.8 | learning rate 1.357E-06 | lm loss 7.767406E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2632.09 | backward: 6970.15 | optimizer: 999.44 | batch generator: 10.30 | data loader: 0.66
10.0.2.2:  iteration      115/    1000 | elapsed time per iteration (ms): 11066.8 | learning rate 1.371E-06 | lm loss 7.361613E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2986.65 | backward: 7078.63 | optimizer: 997.98 | batch generator: 3.55 | data loader: 0.59
10.0.2.2:  iteration      116/    1000 | elapsed time per iteration (ms): 10493.6 | learning rate 1.386E-06 | lm loss 8.177444E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2717.88 | backward: 6707.79 | optimizer: 1007.76 | batch generator: 8.64 | data loader: 0.70
10.0.2.2:  iteration      117/    1000 | elapsed time per iteration (ms): 11005.3 | learning rate 1.400E-06 | lm loss 7.052293E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2693.68 | backward: 7066.49 | optimizer: 992.93 | batch generator: 3.80 | data loader: 0.77
10.0.2.2:  iteration      118/    1000 | elapsed time per iteration (ms): 11084.7 | learning rate 1.414E-06 | lm loss 7.288123E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2679.75 | backward: 7089.57 | optimizer: 987.17 | batch generator: 4.40 | data loader: 0.68
10.0.2.2:  iteration      119/    1000 | elapsed time per iteration (ms): 10928.8 | learning rate 1.429E-06 | lm loss 8.014288E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2593.05 | backward: 6980.54 | optimizer: 1000.11 | batch generator: 8.13 | data loader: 0.66
10.0.2.2:  iteration      120/    1000 | elapsed time per iteration (ms): 11060.6 | learning rate 1.443E-06 | lm loss 7.085104E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2614.85 | backward: 7087.79 | optimizer: 995.71 | batch generator: 3.27 | data loader: 0.69
10.0.2.2:  iteration      121/    1000 | elapsed time per iteration (ms): 10996.4 | learning rate 1.457E-06 | lm loss 6.928201E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2902.81 | backward: 7033.25 | optimizer: 990.07 | batch generator: 8.05 | data loader: 0.68
10.0.2.2:  iteration      122/    1000 | elapsed time per iteration (ms): 10994.9 | learning rate 1.471E-06 | lm loss 7.122736E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2948.96 | backward: 7061.80 | optimizer: 981.46 | batch generator: 3.36 | data loader: 0.59
10.0.2.2:  iteration      123/    1000 | elapsed time per iteration (ms): 11036.1 | learning rate 1.486E-06 | lm loss 7.525211E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2564.32 | backward: 7076.71 | optimizer: 995.73 | batch generator: 5.80 | data loader: 0.60
10.0.2.2:  iteration      124/    1000 | elapsed time per iteration (ms): 10308.3 | learning rate 1.500E-06 | lm loss 7.934256E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2598.87 | backward: 6603.76 | optimizer: 992.38 | batch generator: 4.84 | data loader: 0.52
10.0.2.2:  iteration      125/    1000 | elapsed time per iteration (ms): 10992.9 | learning rate 1.514E-06 | lm loss 7.282183E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2899.95 | backward: 7034.26 | optimizer: 995.58 | batch generator: 3.24 | data loader: 0.69
10.0.2.2:  iteration      126/    1000 | elapsed time per iteration (ms): 32856.2 | learning rate 1.529E-06 | lm loss 6.949323E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2670.06 | backward: 7110.24 | optimizer: 22759.48 | batch generator: 4.17 | data loader: 0.64
10.0.2.2:  iteration      127/    1000 | elapsed time per iteration (ms): 11466.6 | learning rate 1.543E-06 | lm loss 7.807222E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2602.31 | backward: 7433.49 | optimizer: 1018.46 | batch generator: 46.27 | data loader: 1.50
10.0.2.2:  iteration      128/    1000 | elapsed time per iteration (ms): 11237.2 | learning rate 1.557E-06 | lm loss 7.271970E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2958.94 | backward: 7205.69 | optimizer: 1046.67 | batch generator: 7.74 | data loader: 0.89
10.0.2.2:  iteration      129/    1000 | elapsed time per iteration (ms): 11061.2 | learning rate 1.571E-06 | lm loss 7.795054E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 1311.12 | backward: 7042.19 | optimizer: 1039.81 | batch generator: 12.91 | data loader: 0.77
10.0.2.2:  iteration      130/    1000 | elapsed time per iteration (ms): 11083.1 | learning rate 1.586E-06 | lm loss 7.913411E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2631.39 | backward: 7081.24 | optimizer: 1043.53 | batch generator: 3.92 | data loader: 0.80
10.0.2.2:  iteration      131/    1000 | elapsed time per iteration (ms): 19789.0 | learning rate 1.600E-06 | lm loss 7.697004E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 11758.83 | backward: 6972.51 | optimizer: 1052.82 | batch generator: 9205.03 | data loader: 1.23
10.0.2.2:  iteration      132/    1000 | elapsed time per iteration (ms): 11216.3 | learning rate 1.614E-06 | lm loss 7.143092E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2942.85 | backward: 7206.99 | optimizer: 1030.45 | batch generator: 5.00 | data loader: 1.29
10.0.2.2:  iteration      133/    1000 | elapsed time per iteration (ms): 10344.0 | learning rate 1.629E-06 | lm loss 8.436214E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2672.50 | backward: 6628.76 | optimizer: 985.84 | batch generator: 5.39 | data loader: 1.33
10.0.2.2:  iteration      134/    1000 | elapsed time per iteration (ms): 10960.7 | learning rate 1.643E-06 | lm loss 7.066994E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2949.83 | backward: 7020.93 | optimizer: 986.28 | batch generator: 7.76 | data loader: 0.87
10.0.2.2:  iteration      135/    1000 | elapsed time per iteration (ms): 10983.7 | learning rate 1.657E-06 | lm loss 7.189628E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2639.49 | backward: 7035.44 | optimizer: 994.74 | batch generator: 5.04 | data loader: 0.76
10.0.2.2:  iteration      136/    1000 | elapsed time per iteration (ms): 11061.5 | learning rate 1.671E-06 | lm loss 7.245095E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2944.13 | backward: 7087.27 | optimizer: 986.63 | batch generator: 4.35 | data loader: 0.87
10.0.2.2:  iteration      137/    1000 | elapsed time per iteration (ms): 11050.2 | learning rate 1.686E-06 | lm loss 7.432858E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2982.75 | backward: 7073.86 | optimizer: 989.76 | batch generator: 4.97 | data loader: 0.75
10.0.2.2:  iteration      138/    1000 | elapsed time per iteration (ms): 10931.3 | learning rate 1.700E-06 | lm loss 7.483842E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2702.76 | backward: 6990.20 | optimizer: 993.27 | batch generator: 4.37 | data loader: 1.32
10.0.2.2:  iteration      139/    1000 | elapsed time per iteration (ms): 11041.5 | learning rate 1.714E-06 | lm loss 6.854408E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2704.88 | backward: 7116.36 | optimizer: 991.04 | batch generator: 4.13 | data loader: 1.32
10.0.2.2:  iteration      140/    1000 | elapsed time per iteration (ms): 10226.1 | learning rate 1.729E-06 | lm loss 7.777529E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2709.64 | backward: 6520.56 | optimizer: 992.11 | batch generator: 3.44 | data loader: 0.62
10.0.2.2:  iteration      141/    1000 | elapsed time per iteration (ms): 11026.3 | learning rate 1.743E-06 | lm loss 7.716773E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2702.29 | backward: 7042.75 | optimizer: 981.11 | batch generator: 4.07 | data loader: 0.66
10.0.2.2:  iteration      142/    1000 | elapsed time per iteration (ms): 11035.0 | learning rate 1.757E-06 | lm loss 7.655574E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2980.09 | backward: 7066.85 | optimizer: 983.35 | batch generator: 3.70 | data loader: 0.74
10.0.2.2:  iteration      143/    1000 | elapsed time per iteration (ms): 10909.9 | learning rate 1.771E-06 | lm loss 7.691761E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2563.46 | backward: 6993.68 | optimizer: 994.18 | batch generator: 5.83 | data loader: 1.37
10.0.2.2:  iteration      144/    1000 | elapsed time per iteration (ms): 10934.0 | learning rate 1.786E-06 | lm loss 7.341336E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2923.02 | backward: 7009.51 | optimizer: 985.17 | batch generator: 3.74 | data loader: 0.89
10.0.2.2:  iteration      145/    1000 | elapsed time per iteration (ms): 11060.4 | learning rate 1.800E-06 | lm loss 7.146738E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2940.47 | backward: 7094.67 | optimizer: 990.52 | batch generator: 3.58 | data loader: 0.66
10.0.2.2:  iteration      146/    1000 | elapsed time per iteration (ms): 10946.3 | learning rate 1.814E-06 | lm loss 7.900552E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2634.87 | backward: 7029.36 | optimizer: 997.33 | batch generator: 4.51 | data loader: 0.67
10.0.2.2:  iteration      147/    1000 | elapsed time per iteration (ms): 10980.1 | learning rate 1.829E-06 | lm loss 7.195645E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2927.92 | backward: 7016.13 | optimizer: 989.28 | batch generator: 3.70 | data loader: 0.69
10.0.2.2:  iteration      148/    1000 | elapsed time per iteration (ms): 11063.3 | learning rate 1.843E-06 | lm loss 6.782638E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2935.31 | backward: 7093.23 | optimizer: 985.77 | batch generator: 4.76 | data loader: 0.71
10.0.2.2:  iteration      149/    1000 | elapsed time per iteration (ms): 10971.7 | learning rate 1.857E-06 | lm loss 7.512482E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2928.93 | backward: 7027.99 | optimizer: 988.46 | batch generator: 5.57 | data loader: 0.74
10.0.2.2: [2022-04-01 15:28:07,249] [INFO] [logging.py:60:log_dist] [Rank 0] step=150, skipped=19, lr=[1.8571428571428573e-06, 1.8571428571428573e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 15:28:07,446] [INFO] [timer.py:164:stop] 0/150, SamplesPerSec=7.515164521254046
10.0.2.2:  iteration      150/    1000 | elapsed time per iteration (ms): 10907.6 | learning rate 1.871E-06 | lm loss 7.521904E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2731.56 | backward: 6986.66 | optimizer: 994.53 | batch generator: 4.37 | data loader: 0.69
10.0.2.2:  iteration      151/    1000 | elapsed time per iteration (ms): 10982.1 | learning rate 1.886E-06 | lm loss 6.898791E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2931.21 | backward: 7052.66 | optimizer: 994.24 | batch generator: 3.06 | data loader: 0.38
10.0.2.2:  iteration      152/    1000 | elapsed time per iteration (ms): 11038.6 | learning rate 1.900E-06 | lm loss 6.878236E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2823.60 | backward: 7085.49 | optimizer: 983.43 | batch generator: 3.73 | data loader: 0.74
10.0.2.2:  iteration      153/    1000 | elapsed time per iteration (ms): 11100.7 | learning rate 1.914E-06 | lm loss 7.410840E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2726.47 | backward: 7118.62 | optimizer: 1005.57 | batch generator: 3.58 | data loader: 0.75
10.0.2.2:  iteration      154/    1000 | elapsed time per iteration (ms): 10853.0 | learning rate 1.929E-06 | lm loss 7.600753E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2640.09 | backward: 6954.68 | optimizer: 991.53 | batch generator: 3.63 | data loader: 0.62
10.0.2.2:  iteration      155/    1000 | elapsed time per iteration (ms): 11102.6 | learning rate 1.943E-06 | lm loss 7.194934E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2586.35 | backward: 7126.69 | optimizer: 989.54 | batch generator: 3.38 | data loader: 0.62
10.0.2.2:  iteration      156/    1000 | elapsed time per iteration (ms): 11101.1 | learning rate 1.957E-06 | lm loss 6.972494E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2686.91 | backward: 7131.76 | optimizer: 988.36 | batch generator: 4.68 | data loader: 0.78
10.0.2.2:  iteration      157/    1000 | elapsed time per iteration (ms): 10965.8 | learning rate 1.971E-06 | lm loss 6.632212E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2949.90 | backward: 7027.24 | optimizer: 984.70 | batch generator: 3.93 | data loader: 0.64
10.0.2.2:  iteration      158/    1000 | elapsed time per iteration (ms): 11043.6 | learning rate 1.986E-06 | lm loss 7.311269E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2693.23 | backward: 7083.36 | optimizer: 990.85 | batch generator: 4.48 | data loader: 0.63
10.0.2.2:  iteration      159/    1000 | elapsed time per iteration (ms): 10945.3 | learning rate 2.000E-06 | lm loss 7.389897E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2920.75 | backward: 7030.78 | optimizer: 990.75 | batch generator: 4.25 | data loader: 0.72
10.0.2.2:  iteration      160/    1000 | elapsed time per iteration (ms): 11051.1 | learning rate 2.014E-06 | lm loss 7.181283E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2995.38 | backward: 7062.71 | optimizer: 989.47 | batch generator: 7.51 | data loader: 0.67
10.0.2.2:  iteration      161/    1000 | elapsed time per iteration (ms): 10950.5 | learning rate 2.029E-06 | lm loss 7.027079E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 1293.12 | backward: 7014.99 | optimizer: 991.96 | batch generator: 3.37 | data loader: 0.78
10.0.2.2:  iteration      162/    1000 | elapsed time per iteration (ms): 11049.2 | learning rate 2.043E-06 | lm loss 7.062597E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2870.17 | backward: 7089.92 | optimizer: 994.79 | batch generator: 6.16 | data loader: 0.60
10.0.2.2:  iteration      163/    1000 | elapsed time per iteration (ms): 10923.6 | learning rate 2.057E-06 | lm loss 7.851603E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2795.47 | backward: 6990.49 | optimizer: 992.35 | batch generator: 4.24 | data loader: 0.89
10.0.2.2:  iteration      164/    1000 | elapsed time per iteration (ms): 10951.8 | learning rate 2.071E-06 | lm loss 7.526187E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2639.51 | backward: 7028.54 | optimizer: 985.02 | batch generator: 4.11 | data loader: 0.51
10.0.2.2:  iteration      165/    1000 | elapsed time per iteration (ms): 11094.0 | learning rate 2.086E-06 | lm loss 7.339248E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2730.79 | backward: 7138.22 | optimizer: 985.25 | batch generator: 4.42 | data loader: 0.66
10.0.2.2:  iteration      166/    1000 | elapsed time per iteration (ms): 10957.4 | learning rate 2.100E-06 | lm loss 7.352976E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 1769.47 | backward: 7016.80 | optimizer: 992.13 | batch generator: 4.71 | data loader: 0.69
10.0.2.2:  iteration      167/    1000 | elapsed time per iteration (ms): 11000.8 | learning rate 2.114E-06 | lm loss 7.513003E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2623.02 | backward: 7053.46 | optimizer: 990.75 | batch generator: 3.87 | data loader: 0.98
10.0.2.2:  iteration      168/    1000 | elapsed time per iteration (ms): 10976.6 | learning rate 2.129E-06 | lm loss 7.180560E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2942.56 | backward: 7041.09 | optimizer: 990.36 | batch generator: 3.83 | data loader: 0.74
10.0.2.2:  iteration      169/    1000 | elapsed time per iteration (ms): 10931.1 | learning rate 2.143E-06 | lm loss 7.010810E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2107.34 | backward: 6996.56 | optimizer: 990.85 | batch generator: 3.24 | data loader: 0.62
10.0.2.2:  iteration      170/    1000 | elapsed time per iteration (ms): 11038.3 | learning rate 2.157E-06 | lm loss 7.554823E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2701.66 | backward: 7057.52 | optimizer: 988.13 | batch generator: 3.20 | data loader: 0.63
10.0.2.2:  iteration      171/    1000 | elapsed time per iteration (ms): 11032.0 | learning rate 2.171E-06 | lm loss 6.989552E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2952.24 | backward: 7074.75 | optimizer: 989.88 | batch generator: 3.37 | data loader: 0.67
10.0.2.2:  iteration      172/    1000 | elapsed time per iteration (ms): 11030.9 | learning rate 2.186E-06 | lm loss 7.389838E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2599.69 | backward: 7052.65 | optimizer: 989.69 | batch generator: 7.74 | data loader: 0.57
10.0.2.2:  iteration      173/    1000 | elapsed time per iteration (ms): 11025.8 | learning rate 2.200E-06 | lm loss 6.821290E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2928.42 | backward: 7066.77 | optimizer: 998.04 | batch generator: 3.47 | data loader: 0.72
10.0.2.2:  iteration      174/    1000 | elapsed time per iteration (ms): 10185.0 | learning rate 2.214E-06 | lm loss 7.507577E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2588.44 | backward: 6505.85 | optimizer: 995.26 | batch generator: 4.39 | data loader: 0.65
10.0.2.2:  iteration      175/    1000 | elapsed time per iteration (ms): 10951.1 | learning rate 2.229E-06 | lm loss 7.631870E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2957.01 | backward: 7004.68 | optimizer: 985.32 | batch generator: 3.54 | data loader: 0.73
10.0.2.2:  iteration      176/    1000 | elapsed time per iteration (ms): 10968.8 | learning rate 2.243E-06 | lm loss 7.338133E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2669.83 | backward: 7034.71 | optimizer: 988.49 | batch generator: 6.64 | data loader: 0.91
10.0.2.2:  iteration      177/    1000 | elapsed time per iteration (ms): 11034.7 | learning rate 2.257E-06 | lm loss 6.964823E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2647.01 | backward: 7063.28 | optimizer: 998.86 | batch generator: 3.49 | data loader: 0.47
10.0.2.2:  iteration      178/    1000 | elapsed time per iteration (ms): 10958.0 | learning rate 2.271E-06 | lm loss 7.399020E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2551.28 | backward: 7033.12 | optimizer: 987.15 | batch generator: 3.12 | data loader: 0.53
10.0.2.2:  iteration      179/    1000 | elapsed time per iteration (ms): 11069.1 | learning rate 2.286E-06 | lm loss 6.722303E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2924.83 | backward: 7099.90 | optimizer: 987.97 | batch generator: 9.16 | data loader: 0.72
10.0.2.2:  iteration      180/    1000 | elapsed time per iteration (ms): 10923.6 | learning rate 2.300E-06 | lm loss 7.876777E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2951.83 | backward: 6979.39 | optimizer: 988.98 | batch generator: 5.47 | data loader: 0.48
10.0.2.2:  iteration      181/    1000 | elapsed time per iteration (ms): 11050.7 | learning rate 2.314E-06 | lm loss 7.278941E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2641.04 | backward: 7091.24 | optimizer: 989.45 | batch generator: 3.56 | data loader: 0.73
10.0.2.2:  iteration      182/    1000 | elapsed time per iteration (ms): 10860.6 | learning rate 2.329E-06 | lm loss 7.403409E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2917.38 | backward: 6951.03 | optimizer: 988.06 | batch generator: 6.16 | data loader: 0.59
10.0.2.2:  iteration      183/    1000 | elapsed time per iteration (ms): 10900.8 | learning rate 2.343E-06 | lm loss 7.675495E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2714.50 | backward: 6980.97 | optimizer: 986.82 | batch generator: 5.55 | data loader: 0.86
10.0.2.2:  iteration      184/    1000 | elapsed time per iteration (ms): 11004.7 | learning rate 2.357E-06 | lm loss 7.281384E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2804.33 | backward: 7075.66 | optimizer: 987.91 | batch generator: 3.96 | data loader: 0.73
10.0.2.2:  iteration      185/    1000 | elapsed time per iteration (ms): 11048.2 | learning rate 2.371E-06 | lm loss 7.319133E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2584.82 | backward: 7080.57 | optimizer: 982.47 | batch generator: 3.95 | data loader: 0.70
10.0.2.2:  iteration      186/    1000 | elapsed time per iteration (ms): 10096.7 | learning rate 2.386E-06 | lm loss 8.140538E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2590.03 | backward: 6482.87 | optimizer: 1001.61 | batch generator: 6.50 | data loader: 0.81
10.0.2.2:  iteration      187/    1000 | elapsed time per iteration (ms): 10962.8 | learning rate 2.400E-06 | lm loss 7.041422E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2942.64 | backward: 7031.54 | optimizer: 985.11 | batch generator: 3.36 | data loader: 0.53
10.0.2.2:  iteration      188/    1000 | elapsed time per iteration (ms): 11025.6 | learning rate 2.414E-06 | lm loss 6.805831E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2934.07 | backward: 7056.30 | optimizer: 993.64 | batch generator: 3.65 | data loader: 0.59
10.0.2.2:  iteration      189/    1000 | elapsed time per iteration (ms): 11049.0 | learning rate 2.429E-06 | lm loss 7.457470E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2590.72 | backward: 7057.01 | optimizer: 1000.64 | batch generator: 3.29 | data loader: 0.61
10.0.2.2:  iteration      190/    1000 | elapsed time per iteration (ms): 10942.8 | learning rate 2.443E-06 | lm loss 7.208090E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2604.81 | backward: 6988.49 | optimizer: 1000.01 | batch generator: 7.82 | data loader: 0.68
10.0.2.2:  iteration      191/    1000 | elapsed time per iteration (ms): 11135.3 | learning rate 2.457E-06 | lm loss 6.648154E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2960.17 | backward: 7132.21 | optimizer: 1009.92 | batch generator: 8.17 | data loader: 0.62
10.0.2.2:  iteration      192/    1000 | elapsed time per iteration (ms): 11000.8 | learning rate 2.471E-06 | lm loss 6.598356E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2939.11 | backward: 7030.20 | optimizer: 990.78 | batch generator: 3.77 | data loader: 0.77
10.0.2.2:  iteration      193/    1000 | elapsed time per iteration (ms): 11038.1 | learning rate 2.486E-06 | lm loss 6.899177E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2602.70 | backward: 7062.89 | optimizer: 994.48 | batch generator: 7.36 | data loader: 0.67
10.0.2.2:  iteration      194/    1000 | elapsed time per iteration (ms): 11007.7 | learning rate 2.500E-06 | lm loss 6.988790E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2663.96 | backward: 7079.35 | optimizer: 989.43 | batch generator: 3.40 | data loader: 0.77
10.0.2.2:  iteration      195/    1000 | elapsed time per iteration (ms): 11109.3 | learning rate 2.514E-06 | lm loss 7.276519E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2679.90 | backward: 7129.61 | optimizer: 985.69 | batch generator: 7.66 | data loader: 0.59
10.0.2.2:  iteration      196/    1000 | elapsed time per iteration (ms): 11091.3 | learning rate 2.529E-06 | lm loss 7.658097E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2631.11 | backward: 7106.42 | optimizer: 989.82 | batch generator: 8.87 | data loader: 0.87
10.0.2.2:  iteration      197/    1000 | elapsed time per iteration (ms): 11081.9 | learning rate 2.543E-06 | lm loss 6.509797E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2946.72 | backward: 7096.56 | optimizer: 993.39 | batch generator: 3.82 | data loader: 0.67
10.0.2.2:  iteration      198/    1000 | elapsed time per iteration (ms): 11128.1 | learning rate 2.557E-06 | lm loss 6.631793E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2983.84 | backward: 7148.30 | optimizer: 992.36 | batch generator: 3.28 | data loader: 0.68
10.0.2.2:  iteration      199/    1000 | elapsed time per iteration (ms): 11057.6 | learning rate 2.571E-06 | lm loss 6.936020E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2622.51 | backward: 7095.20 | optimizer: 985.86 | batch generator: 6.19 | data loader: 0.78
10.0.2.2: [2022-04-01 15:37:16,346] [INFO] [logging.py:60:log_dist] [Rank 0] step=200, skipped=19, lr=[2.5714285714285716e-06, 2.5714285714285716e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 15:37:16,542] [INFO] [timer.py:164:stop] 0/200, SamplesPerSec=7.549676857274893
10.0.2.2:  iteration      200/    1000 | elapsed time per iteration (ms): 11263.0 | learning rate 2.586E-06 | lm loss 6.918169E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2667.12 | backward: 7243.27 | optimizer: 984.06 | batch generator: 8.07 | data loader: 0.71
10.0.2.2:  iteration      201/    1000 | elapsed time per iteration (ms): 11027.6 | learning rate 2.600E-06 | lm loss 6.969851E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2964.00 | backward: 7055.87 | optimizer: 991.02 | batch generator: 3.48 | data loader: 0.37
10.0.2.2:  iteration      202/    1000 | elapsed time per iteration (ms): 11139.0 | learning rate 2.614E-06 | lm loss 7.041534E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2599.56 | backward: 7153.90 | optimizer: 987.43 | batch generator: 2.79 | data loader: 0.54
10.0.2.2:  iteration      203/    1000 | elapsed time per iteration (ms): 10326.2 | learning rate 2.629E-06 | lm loss 7.914761E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2708.25 | backward: 6575.82 | optimizer: 987.34 | batch generator: 12.42 | data loader: 0.60
10.0.2.2:  iteration      204/    1000 | elapsed time per iteration (ms): 11017.4 | learning rate 2.643E-06 | lm loss 7.112105E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2681.48 | backward: 7062.31 | optimizer: 988.45 | batch generator: 6.11 | data loader: 0.66
10.0.2.2:  iteration      205/    1000 | elapsed time per iteration (ms): 11067.2 | learning rate 2.657E-06 | lm loss 7.142021E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2936.59 | backward: 7085.23 | optimizer: 984.82 | batch generator: 3.57 | data loader: 0.66
10.0.2.2:  iteration      206/    1000 | elapsed time per iteration (ms): 11124.8 | learning rate 2.671E-06 | lm loss 6.503601E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2964.00 | backward: 7131.73 | optimizer: 992.43 | batch generator: 5.93 | data loader: 0.94
10.0.2.2:  iteration      207/    1000 | elapsed time per iteration (ms): 11105.2 | learning rate 2.686E-06 | lm loss 6.684216E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2918.41 | backward: 7123.70 | optimizer: 992.21 | batch generator: 4.05 | data loader: 0.67
10.0.2.2:  iteration      208/    1000 | elapsed time per iteration (ms): 11085.9 | learning rate 2.700E-06 | lm loss 6.772271E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2929.98 | backward: 7126.14 | optimizer: 993.62 | batch generator: 8.48 | data loader: 0.52
10.0.2.2:  iteration      209/    1000 | elapsed time per iteration (ms): 10344.2 | learning rate 2.714E-06 | lm loss 6.747124E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2710.73 | backward: 6614.83 | optimizer: 991.70 | batch generator: 3.57 | data loader: 0.69
10.0.2.2:  iteration      210/    1000 | elapsed time per iteration (ms): 11023.0 | learning rate 2.729E-06 | lm loss 7.466324E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2535.87 | backward: 7061.77 | optimizer: 984.23 | batch generator: 3.18 | data loader: 0.51
10.0.2.2:  iteration      211/    1000 | elapsed time per iteration (ms): 10341.7 | learning rate 2.743E-06 | lm loss 7.713015E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2672.80 | backward: 6629.40 | optimizer: 989.83 | batch generator: 3.81 | data loader: 1.00
10.0.2.2:  iteration      212/    1000 | elapsed time per iteration (ms): 10968.9 | learning rate 2.757E-06 | lm loss 6.718151E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2663.48 | backward: 7033.18 | optimizer: 985.74 | batch generator: 3.19 | data loader: 0.56
10.0.2.2:  iteration      213/    1000 | elapsed time per iteration (ms): 11060.2 | learning rate 2.771E-06 | lm loss 7.127619E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2587.18 | backward: 7096.48 | optimizer: 985.53 | batch generator: 4.26 | data loader: 1.58
10.0.2.2:  iteration      214/    1000 | elapsed time per iteration (ms): 11016.4 | learning rate 2.786E-06 | lm loss 7.133472E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2951.52 | backward: 7066.51 | optimizer: 994.20 | batch generator: 3.68 | data loader: 0.68
10.0.2.2:  iteration      215/    1000 | elapsed time per iteration (ms): 11065.2 | learning rate 2.800E-06 | lm loss 6.776639E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2757.69 | backward: 7088.89 | optimizer: 988.78 | batch generator: 5.34 | data loader: 0.81
10.0.2.2:  iteration      216/    1000 | elapsed time per iteration (ms): 11021.3 | learning rate 2.814E-06 | lm loss 6.961914E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2968.97 | backward: 7055.10 | optimizer: 994.56 | batch generator: 3.78 | data loader: 0.63
10.0.2.2:  iteration      217/    1000 | elapsed time per iteration (ms): 11036.9 | learning rate 2.829E-06 | lm loss 7.127565E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2959.44 | backward: 7080.77 | optimizer: 994.37 | batch generator: 3.14 | data loader: 0.51
10.0.2.2:  iteration      218/    1000 | elapsed time per iteration (ms): 32116.5 | learning rate 2.843E-06 | lm loss 7.397267E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2736.02 | backward: 7757.27 | optimizer: 21387.83 | batch generator: 3.50 | data loader: 0.73
10.0.2.2:  iteration      219/    1000 | elapsed time per iteration (ms): 21973.2 | learning rate 2.857E-06 | lm loss 6.798743E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2894.06 | backward: 17950.11 | optimizer: 1053.10 | batch generator: 6.80 | data loader: 1.25
10.0.2.2:  iteration      220/    1000 | elapsed time per iteration (ms): 11148.0 | learning rate 2.871E-06 | lm loss 7.236090E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2933.23 | backward: 7099.77 | optimizer: 1051.28 | batch generator: 4.99 | data loader: 0.97
10.0.2.2:  iteration      221/    1000 | elapsed time per iteration (ms): 11212.4 | learning rate 2.886E-06 | lm loss 7.149319E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2954.07 | backward: 7199.23 | optimizer: 1046.63 | batch generator: 5.38 | data loader: 1.71
10.0.2.2:  iteration      222/    1000 | elapsed time per iteration (ms): 11049.7 | learning rate 2.900E-06 | lm loss 7.088908E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2621.63 | backward: 7050.10 | optimizer: 1025.47 | batch generator: 3.87 | data loader: 0.96
10.0.2.2:  iteration      223/    1000 | elapsed time per iteration (ms): 11067.1 | learning rate 2.914E-06 | lm loss 7.396776E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2598.17 | backward: 7066.83 | optimizer: 1044.46 | batch generator: 5.00 | data loader: 0.92
10.0.2.2:  iteration      224/    1000 | elapsed time per iteration (ms): 10829.2 | learning rate 2.929E-06 | lm loss 6.904030E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2689.59 | backward: 6891.51 | optimizer: 1009.54 | batch generator: 4.70 | data loader: 0.88
10.0.2.2:  iteration      225/    1000 | elapsed time per iteration (ms): 10897.1 | learning rate 2.943E-06 | lm loss 6.998571E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2930.75 | backward: 6959.40 | optimizer: 1002.54 | batch generator: 3.85 | data loader: 0.95
10.0.2.2:  iteration      226/    1000 | elapsed time per iteration (ms): 11071.8 | learning rate 2.957E-06 | lm loss 6.653631E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2763.78 | backward: 7112.46 | optimizer: 984.63 | batch generator: 3.53 | data loader: 0.64
10.0.2.2:  iteration      227/    1000 | elapsed time per iteration (ms): 10926.3 | learning rate 2.971E-06 | lm loss 6.598304E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2942.44 | backward: 6998.03 | optimizer: 982.81 | batch generator: 4.44 | data loader: 0.66
10.0.2.2:  iteration      228/    1000 | elapsed time per iteration (ms): 10940.9 | learning rate 2.986E-06 | lm loss 7.458881E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2922.39 | backward: 7023.30 | optimizer: 992.07 | batch generator: 4.39 | data loader: 0.77
10.0.2.2:  iteration      229/    1000 | elapsed time per iteration (ms): 24876.2 | learning rate 3.000E-06 | lm loss 7.025633E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2927.00 | backward: 7038.60 | optimizer: 14849.51 | batch generator: 5.39 | data loader: 0.73
10.0.2.2:  iteration      230/    1000 | elapsed time per iteration (ms): 18343.1 | learning rate 3.014E-06 | lm loss 7.297554E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2609.31 | backward: 14259.45 | optimizer: 1093.57 | batch generator: 27.32 | data loader: 1.36
10.0.2.2:  iteration      231/    1000 | elapsed time per iteration (ms): 11184.8 | learning rate 3.029E-06 | lm loss 6.811965E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2923.13 | backward: 7171.86 | optimizer: 1049.62 | batch generator: 4.88 | data loader: 1.07
10.0.2.2:  iteration      232/    1000 | elapsed time per iteration (ms): 11658.7 | learning rate 3.043E-06 | lm loss 7.347750E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2965.68 | backward: 7552.53 | optimizer: 1135.56 | batch generator: 5.67 | data loader: 1.99
10.0.2.2:  iteration      233/    1000 | elapsed time per iteration (ms): 23213.1 | learning rate 3.057E-06 | lm loss 7.549400E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2635.70 | backward: 7342.37 | optimizer: 12891.93 | batch generator: 5.25 | data loader: 1.20
10.0.2.2:  iteration      234/    1000 | elapsed time per iteration (ms): 11257.6 | learning rate 3.071E-06 | lm loss 7.019359E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2684.78 | backward: 7162.29 | optimizer: 1013.99 | batch generator: 58.45 | data loader: 1.43
10.0.2.2:  iteration      235/    1000 | elapsed time per iteration (ms): 11030.0 | learning rate 3.086E-06 | lm loss 7.107695E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2587.85 | backward: 7038.22 | optimizer: 1027.65 | batch generator: 3.78 | data loader: 0.92
10.0.2.2:  iteration      236/    1000 | elapsed time per iteration (ms): 11217.8 | learning rate 3.100E-06 | lm loss 7.092045E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2968.52 | backward: 7216.17 | optimizer: 1029.37 | batch generator: 3.95 | data loader: 1.00
10.0.2.2:  iteration      237/    1000 | elapsed time per iteration (ms): 10817.4 | learning rate 3.114E-06 | lm loss 7.078996E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2883.35 | backward: 6903.20 | optimizer: 984.88 | batch generator: 5.03 | data loader: 1.20
10.0.2.2: b' major software challenges report to the chairman subcommittee on research and development on armed services house of representatives because this is pdf file\n\nno short description mission critical systems defense attempting to address major software challenges report to the chairman subcommittee on research and development on armed services house of representatives because this is doc file\n\nno short description new citizenship basics textbook dvd and audio cd us naturalization test study guide 100 civics questions with new updated n 400 application questions pass the citizenship interview with the new textbook cd and dvd because this is pdf file\n\nno short description new citizenship basics textbook dvd and audio cd us naturalization test study guide 100 civics questions with new updated n 400 application questions pass the citizenship interview with the new textbook cd and dvd because this is doc file\n\nno short description how to start a electronic record label never revealed secrets of starting a electronic record label electronic record label business guide how to a eletr record label never revealed secret because this is pdf file\n\nno short description how to start a electronic record label never revealed secrets of starting a electronic record label electronic record label business guide how to a eletr record label never revealed secret because this is doc file\n\nno short description national water quality assessment geological survey faces formidable data management challenges report to the chairman environment energy and operations house of representatives because this is pdf file\n\nno short description national water quality assessment geological survey faces formidable data management challenges report to the chairman environment energy and operations house of representatives because this is doc file\n\nno short description everything guide to pregnancy over 35 from conquering your fears to assessing health risks all you need to have a happy healthy nine months everything parenting and family because this is pdf file\n\nno short description everything guide to pregnancy over 35 from conquering your fears to assessing health risks all you need to have a happy healthy nine months everything parenting and family because this is doc file\n\nno short description algorithms and computation 22nd international symposium isaac 2011 yokohama japan december 5 8 2011 proceedings lecture notes in computer computer science and general issues because this is pdf file\n\nno short description algorithms and computation 22nd international symposium isaac 2011 yokohama japan december 5 8 2011 proceedings lecture notes in computer computer science and general issues because this is doc file<|endoftext|>'
10.0.2.2:  iteration      238/    1000 | elapsed time per iteration (ms): 11017.2 | learning rate 3.129E-06 | lm loss 7.187128E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2952.53 | backward: 7070.63 | optimizer: 989.12 | batch generator: 5.25 | data loader: 1.40
10.0.2.2:  iteration      239/    1000 | elapsed time per iteration (ms): 10847.9 | learning rate 3.143E-06 | lm loss 7.082127E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2925.68 | backward: 6932.24 | optimizer: 986.30 | batch generator: 3.61 | data loader: 0.69
10.0.2.2:  iteration      240/    1000 | elapsed time per iteration (ms): 11001.3 | learning rate 3.157E-06 | lm loss 6.925566E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2737.84 | backward: 7043.52 | optimizer: 993.40 | batch generator: 4.89 | data loader: 0.64
10.0.2.2:  iteration      241/    1000 | elapsed time per iteration (ms): 21538.2 | learning rate 3.171E-06 | lm loss 6.820738E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2618.60 | backward: 17583.27 | optimizer: 992.94 | batch generator: 4.24 | data loader: 0.92
10.0.2.2:  iteration      242/    1000 | elapsed time per iteration (ms): 10951.7 | learning rate 3.186E-06 | lm loss 7.013078E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2729.44 | backward: 7005.15 | optimizer: 990.36 | batch generator: 6.50 | data loader: 0.69
10.0.2.2:  iteration      243/    1000 | elapsed time per iteration (ms): 10417.3 | learning rate 3.200E-06 | lm loss 7.394909E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2631.15 | backward: 6654.75 | optimizer: 985.47 | batch generator: 9.14 | data loader: 0.47
10.0.2.2:  iteration      244/    1000 | elapsed time per iteration (ms): 11010.1 | learning rate 3.214E-06 | lm loss 6.919241E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2938.04 | backward: 7038.77 | optimizer: 1002.92 | batch generator: 4.39 | data loader: 1.33
10.0.2.2:  iteration      245/    1000 | elapsed time per iteration (ms): 10983.0 | learning rate 3.229E-06 | lm loss 6.759709E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2920.90 | backward: 7028.72 | optimizer: 998.21 | batch generator: 5.37 | data loader: 0.58
10.0.2.2:  iteration      246/    1000 | elapsed time per iteration (ms): 11027.4 | learning rate 3.243E-06 | lm loss 6.302578E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2933.09 | backward: 7060.17 | optimizer: 989.08 | batch generator: 6.32 | data loader: 0.54
10.0.2.2:  iteration      247/    1000 | elapsed time per iteration (ms): 11062.0 | learning rate 3.257E-06 | lm loss 6.605430E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2910.22 | backward: 7099.07 | optimizer: 987.95 | batch generator: 4.77 | data loader: 0.64
10.0.2.2:  iteration      248/    1000 | elapsed time per iteration (ms): 11099.7 | learning rate 3.271E-06 | lm loss 6.640992E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2665.34 | backward: 7126.03 | optimizer: 988.29 | batch generator: 3.25 | data loader: 0.61
10.0.2.2:  iteration      249/    1000 | elapsed time per iteration (ms): 10224.3 | learning rate 3.286E-06 | lm loss 7.390630E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2615.13 | backward: 6522.68 | optimizer: 988.45 | batch generator: 3.22 | data loader: 0.61
10.0.2.2: [2022-04-01 15:47:41,047] [INFO] [logging.py:60:log_dist] [Rank 0] step=250, skipped=19, lr=[3.2857142857142857e-06, 3.2857142857142857e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 15:47:41,241] [INFO] [timer.py:164:stop] 0/250, SamplesPerSec=7.368233178390445
10.0.2.2:  iteration      250/    1000 | elapsed time per iteration (ms): 10912.0 | learning rate 3.300E-06 | lm loss 7.153709E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2607.86 | backward: 6976.27 | optimizer: 992.45 | batch generator: 3.67 | data loader: 0.80
10.0.2.2:  iteration      251/    1000 | elapsed time per iteration (ms): 10967.1 | learning rate 3.314E-06 | lm loss 6.425161E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2946.42 | backward: 7034.05 | optimizer: 984.30 | batch generator: 5.98 | data loader: 0.33
10.0.2.2:  iteration      252/    1000 | elapsed time per iteration (ms): 10944.9 | learning rate 3.329E-06 | lm loss 6.547818E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2728.07 | backward: 7002.07 | optimizer: 990.60 | batch generator: 4.02 | data loader: 0.62
10.0.2.2:  iteration      253/    1000 | elapsed time per iteration (ms): 11025.1 | learning rate 3.343E-06 | lm loss 6.405589E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2951.64 | backward: 7054.02 | optimizer: 992.03 | batch generator: 3.72 | data loader: 0.64
10.0.2.2:  iteration      254/    1000 | elapsed time per iteration (ms): 10867.7 | learning rate 3.357E-06 | lm loss 6.883951E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2936.64 | backward: 6913.39 | optimizer: 1014.03 | batch generator: 4.95 | data loader: 1.82
10.0.2.2:  iteration      255/    1000 | elapsed time per iteration (ms): 11038.4 | learning rate 3.371E-06 | lm loss 6.963536E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2966.00 | backward: 7079.54 | optimizer: 989.06 | batch generator: 3.58 | data loader: 0.79
10.0.2.2:  iteration      256/    1000 | elapsed time per iteration (ms): 11107.2 | learning rate 3.386E-06 | lm loss 6.826523E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2994.53 | backward: 7123.03 | optimizer: 986.37 | batch generator: 4.38 | data loader: 0.89
10.0.2.2:  iteration      257/    1000 | elapsed time per iteration (ms): 11093.2 | learning rate 3.400E-06 | lm loss 6.245556E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2922.97 | backward: 7116.53 | optimizer: 988.07 | batch generator: 6.36 | data loader: 0.72
10.0.2.2:  iteration      258/    1000 | elapsed time per iteration (ms): 10211.0 | learning rate 3.414E-06 | lm loss 7.081230E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2637.96 | backward: 6530.46 | optimizer: 987.45 | batch generator: 3.22 | data loader: 0.63
10.0.2.2:  iteration      259/    1000 | elapsed time per iteration (ms): 31045.5 | learning rate 3.429E-06 | lm loss 6.709989E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2654.62 | backward: 12514.32 | optimizer: 15571.60 | batch generator: 3.67 | data loader: 0.71
10.0.2.2:  iteration      260/    1000 | elapsed time per iteration (ms): 10993.4 | learning rate 3.443E-06 | lm loss 7.408602E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2608.19 | backward: 6996.55 | optimizer: 1034.23 | batch generator: 31.17 | data loader: 1.22
10.0.2.2:  iteration      261/    1000 | elapsed time per iteration (ms): 11167.5 | learning rate 3.457E-06 | lm loss 6.584246E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2946.49 | backward: 7149.17 | optimizer: 1055.34 | batch generator: 3.57 | data loader: 0.71
10.0.2.2:  iteration      262/    1000 | elapsed time per iteration (ms): 11146.4 | learning rate 3.471E-06 | lm loss 6.965287E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2733.52 | backward: 7109.15 | optimizer: 1047.20 | batch generator: 6.65 | data loader: 2.45
10.0.2.2:  iteration      263/    1000 | elapsed time per iteration (ms): 11096.0 | learning rate 3.486E-06 | lm loss 7.105671E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2587.38 | backward: 7059.44 | optimizer: 1065.95 | batch generator: 6.17 | data loader: 1.20
10.0.2.2:  iteration      264/    1000 | elapsed time per iteration (ms): 11123.2 | learning rate 3.500E-06 | lm loss 6.576165E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2924.60 | backward: 7090.89 | optimizer: 1047.31 | batch generator: 4.57 | data loader: 1.06
10.0.2.2:  iteration      265/    1000 | elapsed time per iteration (ms): 11184.3 | learning rate 3.514E-06 | lm loss 6.772067E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2648.44 | backward: 7179.09 | optimizer: 1032.06 | batch generator: 4.35 | data loader: 1.03
10.0.2.2:  iteration      266/    1000 | elapsed time per iteration (ms): 10276.4 | learning rate 3.529E-06 | lm loss 7.579163E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2678.20 | backward: 6562.33 | optimizer: 1014.88 | batch generator: 6.12 | data loader: 0.85
10.0.2.2:  iteration      267/    1000 | elapsed time per iteration (ms): 11330.3 | learning rate 3.543E-06 | lm loss 6.668294E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2674.56 | backward: 7298.41 | optimizer: 986.98 | batch generator: 4.80 | data loader: 0.92
10.0.2.2:  iteration      268/    1000 | elapsed time per iteration (ms): 10975.9 | learning rate 3.557E-06 | lm loss 7.163286E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2680.65 | backward: 7039.97 | optimizer: 983.52 | batch generator: 4.31 | data loader: 0.73
10.0.2.2:  iteration      269/    1000 | elapsed time per iteration (ms): 21514.6 | learning rate 3.571E-06 | lm loss 6.482551E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2970.60 | backward: 17543.42 | optimizer: 990.36 | batch generator: 8.87 | data loader: 0.54
10.0.2.2:  iteration      270/    1000 | elapsed time per iteration (ms): 11034.3 | learning rate 3.586E-06 | lm loss 7.121560E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2655.25 | backward: 7075.56 | optimizer: 990.80 | batch generator: 4.30 | data loader: 0.74
10.0.2.2:  iteration      271/    1000 | elapsed time per iteration (ms): 10296.2 | learning rate 3.600E-06 | lm loss 7.235399E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2540.14 | backward: 6580.86 | optimizer: 985.95 | batch generator: 4.13 | data loader: 0.88
10.0.2.2:  iteration      272/    1000 | elapsed time per iteration (ms): 10872.0 | learning rate 3.614E-06 | lm loss 6.707294E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2926.06 | backward: 6951.06 | optimizer: 991.77 | batch generator: 3.87 | data loader: 0.79
10.0.2.2:  iteration      273/    1000 | elapsed time per iteration (ms): 10995.3 | learning rate 3.629E-06 | lm loss 6.778229E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2954.50 | backward: 7050.42 | optimizer: 986.64 | batch generator: 3.04 | data loader: 0.66
10.0.2.2:  iteration      274/    1000 | elapsed time per iteration (ms): 11048.2 | learning rate 3.643E-06 | lm loss 6.479197E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2937.06 | backward: 7078.73 | optimizer: 990.58 | batch generator: 4.21 | data loader: 0.58
10.0.2.2:  iteration      275/    1000 | elapsed time per iteration (ms): 11032.1 | learning rate 3.657E-06 | lm loss 7.008944E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2879.30 | backward: 7088.45 | optimizer: 995.15 | batch generator: 3.25 | data loader: 0.62
10.0.2.2:  iteration      276/    1000 | elapsed time per iteration (ms): 11035.2 | learning rate 3.671E-06 | lm loss 6.118545E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2992.44 | backward: 7038.47 | optimizer: 1000.96 | batch generator: 3.45 | data loader: 0.64
10.0.2.2:  iteration      277/    1000 | elapsed time per iteration (ms): 11009.9 | learning rate 3.686E-06 | lm loss 6.768843E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2954.91 | backward: 7057.49 | optimizer: 994.30 | batch generator: 4.13 | data loader: 0.65
10.0.2.2:  iteration      278/    1000 | elapsed time per iteration (ms): 10969.5 | learning rate 3.700E-06 | lm loss 6.892917E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2675.34 | backward: 7036.10 | optimizer: 987.96 | batch generator: 3.88 | data loader: 0.72
10.0.2.2:  iteration      279/    1000 | elapsed time per iteration (ms): 10924.8 | learning rate 3.714E-06 | lm loss 7.263257E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2736.77 | backward: 6983.13 | optimizer: 992.21 | batch generator: 3.93 | data loader: 0.72
10.0.2.2:  iteration      280/    1000 | elapsed time per iteration (ms): 11004.3 | learning rate 3.729E-06 | lm loss 7.143953E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2683.43 | backward: 7064.98 | optimizer: 990.32 | batch generator: 5.29 | data loader: 1.88
10.0.2.2:  iteration      281/    1000 | elapsed time per iteration (ms): 11002.1 | learning rate 3.743E-06 | lm loss 6.843214E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2967.91 | backward: 7032.74 | optimizer: 997.62 | batch generator: 9.10 | data loader: 0.73
10.0.2.2:  iteration      282/    1000 | elapsed time per iteration (ms): 10894.3 | learning rate 3.757E-06 | lm loss 7.129951E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2574.28 | backward: 6966.57 | optimizer: 983.24 | batch generator: 3.94 | data loader: 0.70
10.0.2.2:  iteration      283/    1000 | elapsed time per iteration (ms): 10999.9 | learning rate 3.771E-06 | lm loss 6.697313E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2949.56 | backward: 7060.42 | optimizer: 986.89 | batch generator: 3.75 | data loader: 0.90
10.0.2.2:  iteration      284/    1000 | elapsed time per iteration (ms): 11028.8 | learning rate 3.786E-06 | lm loss 6.709870E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2960.71 | backward: 7070.50 | optimizer: 993.31 | batch generator: 8.05 | data loader: 0.68
10.0.2.2:  iteration      285/    1000 | elapsed time per iteration (ms): 11047.7 | learning rate 3.800E-06 | lm loss 6.896572E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2592.27 | backward: 7091.06 | optimizer: 990.41 | batch generator: 7.41 | data loader: 0.77
10.0.2.2:  iteration      286/    1000 | elapsed time per iteration (ms): 30232.2 | learning rate 3.814E-06 | lm loss 6.898461E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2621.20 | backward: 25732.87 | optimizer: 1582.61 | batch generator: 3.67 | data loader: 0.70
10.0.2.2:  iteration      287/    1000 | elapsed time per iteration (ms): 11212.2 | learning rate 3.829E-06 | lm loss 6.782449E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2585.54 | backward: 7193.18 | optimizer: 1043.45 | batch generator: 6.64 | data loader: 1.20
10.0.2.2:  iteration      288/    1000 | elapsed time per iteration (ms): 11185.1 | learning rate 3.843E-06 | lm loss 6.305786E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2951.79 | backward: 7183.68 | optimizer: 1028.47 | batch generator: 4.30 | data loader: 0.88
10.0.2.2:  iteration      289/    1000 | elapsed time per iteration (ms): 10951.8 | learning rate 3.857E-06 | lm loss 7.275204E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2562.67 | backward: 7005.63 | optimizer: 1017.15 | batch generator: 5.30 | data loader: 1.42
10.0.2.2:  iteration      290/    1000 | elapsed time per iteration (ms): 11198.6 | learning rate 3.871E-06 | lm loss 6.697133E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2978.51 | backward: 7145.73 | optimizer: 1071.04 | batch generator: 4.97 | data loader: 1.29
10.0.2.2:  iteration      291/    1000 | elapsed time per iteration (ms): 11059.3 | learning rate 3.886E-06 | lm loss 6.503813E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2936.97 | backward: 7072.19 | optimizer: 1040.70 | batch generator: 5.47 | data loader: 1.16
10.0.2.2:  iteration      292/    1000 | elapsed time per iteration (ms): 11068.6 | learning rate 3.900E-06 | lm loss 6.531927E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2665.70 | backward: 7069.97 | optimizer: 1048.86 | batch generator: 4.64 | data loader: 0.91
10.0.2.2:  iteration      293/    1000 | elapsed time per iteration (ms): 11079.1 | learning rate 3.914E-06 | lm loss 6.704105E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2852.57 | backward: 7120.74 | optimizer: 997.73 | batch generator: 7.51 | data loader: 0.86
10.0.2.2:  iteration      294/    1000 | elapsed time per iteration (ms): 10954.2 | learning rate 3.929E-06 | lm loss 6.591156E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2564.71 | backward: 7035.39 | optimizer: 982.15 | batch generator: 3.41 | data loader: 0.78
10.0.2.2:  iteration      295/    1000 | elapsed time per iteration (ms): 11019.3 | learning rate 3.943E-06 | lm loss 6.764133E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2765.86 | backward: 7052.99 | optimizer: 985.29 | batch generator: 4.02 | data loader: 0.61
10.0.2.2:  iteration      296/    1000 | elapsed time per iteration (ms): 11008.7 | learning rate 3.957E-06 | lm loss 6.610802E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2964.50 | backward: 7050.04 | optimizer: 991.62 | batch generator: 7.05 | data loader: 0.68
10.0.2.2:  iteration      297/    1000 | elapsed time per iteration (ms): 11057.5 | learning rate 3.971E-06 | lm loss 6.750785E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2965.66 | backward: 7072.31 | optimizer: 991.54 | batch generator: 4.31 | data loader: 0.67
10.0.2.2:  iteration      298/    1000 | elapsed time per iteration (ms): 11022.0 | learning rate 3.986E-06 | lm loss 6.691081E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2972.56 | backward: 7059.49 | optimizer: 984.57 | batch generator: 4.60 | data loader: 1.08
10.0.2.2:  iteration      299/    1000 | elapsed time per iteration (ms): 11048.5 | learning rate 4.000E-06 | lm loss 6.159507E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2941.92 | backward: 7085.75 | optimizer: 990.33 | batch generator: 3.94 | data loader: 0.67
10.0.2.2: [2022-04-01 15:57:40,379] [INFO] [logging.py:60:log_dist] [Rank 0] step=300, skipped=19, lr=[4e-06, 4e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 15:57:40,572] [INFO] [timer.py:164:stop] 0/300, SamplesPerSec=7.306017973193369
10.0.2.2:  iteration      300/    1000 | elapsed time per iteration (ms): 10927.0 | learning rate 4.014E-06 | lm loss 6.782633E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2656.88 | backward: 6991.35 | optimizer: 985.84 | batch generator: 3.48 | data loader: 0.72
10.0.2.2:  iteration      301/    1000 | elapsed time per iteration (ms): 10942.6 | learning rate 4.029E-06 | lm loss 6.726707E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2932.39 | backward: 7011.97 | optimizer: 987.74 | batch generator: 3.92 | data loader: 0.43
10.0.2.2:  iteration      302/    1000 | elapsed time per iteration (ms): 21369.4 | learning rate 4.043E-06 | lm loss 7.007379E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2959.33 | backward: 17405.64 | optimizer: 1000.32 | batch generator: 5.99 | data loader: 0.75
10.0.2.2:  iteration      303/    1000 | elapsed time per iteration (ms): 11050.9 | learning rate 4.057E-06 | lm loss 6.615928E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2597.70 | backward: 7078.05 | optimizer: 981.64 | batch generator: 11.29 | data loader: 0.67
10.0.2.2:  iteration      304/    1000 | elapsed time per iteration (ms): 11047.7 | learning rate 4.071E-06 | lm loss 6.204093E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2951.60 | backward: 7088.83 | optimizer: 990.17 | batch generator: 7.59 | data loader: 0.88
10.0.2.2:  iteration      305/    1000 | elapsed time per iteration (ms): 11025.8 | learning rate 4.086E-06 | lm loss 6.737050E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2957.20 | backward: 7070.93 | optimizer: 990.34 | batch generator: 5.76 | data loader: 0.84
10.0.2.2:  iteration      306/    1000 | elapsed time per iteration (ms): 10961.9 | learning rate 4.100E-06 | lm loss 6.823486E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2899.48 | backward: 7037.87 | optimizer: 999.78 | batch generator: 4.58 | data loader: 0.68
10.0.2.2:  iteration      307/    1000 | elapsed time per iteration (ms): 10981.7 | learning rate 4.114E-06 | lm loss 6.689324E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2904.41 | backward: 7036.92 | optimizer: 991.93 | batch generator: 3.81 | data loader: 0.73
10.0.2.2:  iteration      308/    1000 | elapsed time per iteration (ms): 10899.5 | learning rate 4.129E-06 | lm loss 6.587352E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2912.25 | backward: 6962.11 | optimizer: 997.11 | batch generator: 4.50 | data loader: 0.65
10.0.2.2:  iteration      309/    1000 | elapsed time per iteration (ms): 10938.1 | learning rate 4.143E-06 | lm loss 6.809086E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2590.14 | backward: 6982.27 | optimizer: 1001.29 | batch generator: 4.12 | data loader: 0.87
10.0.2.2:  iteration      310/    1000 | elapsed time per iteration (ms): 11096.5 | learning rate 4.157E-06 | lm loss 6.800129E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2715.27 | backward: 7100.38 | optimizer: 994.72 | batch generator: 6.23 | data loader: 0.60
10.0.2.2:  iteration      311/    1000 | elapsed time per iteration (ms): 11010.9 | learning rate 4.171E-06 | lm loss 7.117714E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2649.12 | backward: 7067.62 | optimizer: 989.31 | batch generator: 3.65 | data loader: 0.80
10.0.2.2:  iteration      312/    1000 | elapsed time per iteration (ms): 11030.9 | learning rate 4.186E-06 | lm loss 6.170708E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2902.71 | backward: 7054.31 | optimizer: 1001.19 | batch generator: 3.57 | data loader: 0.77
10.0.2.2:  iteration      313/    1000 | elapsed time per iteration (ms): 10962.1 | learning rate 4.200E-06 | lm loss 6.569118E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2608.54 | backward: 7019.09 | optimizer: 986.82 | batch generator: 3.49 | data loader: 0.64
10.0.2.2:  iteration      314/    1000 | elapsed time per iteration (ms): 11210.8 | learning rate 4.214E-06 | lm loss 6.946811E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 3018.89 | backward: 7199.53 | optimizer: 988.03 | batch generator: 4.44 | data loader: 0.54
10.0.2.2:  iteration      315/    1000 | elapsed time per iteration (ms): 11019.5 | learning rate 4.229E-06 | lm loss 6.341163E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2958.30 | backward: 7067.61 | optimizer: 989.43 | batch generator: 5.74 | data loader: 1.50
10.0.2.2:  iteration      316/    1000 | elapsed time per iteration (ms): 10973.6 | learning rate 4.243E-06 | lm loss 6.290822E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2974.73 | backward: 7007.35 | optimizer: 988.77 | batch generator: 5.91 | data loader: 0.62
10.0.2.2:  iteration      317/    1000 | elapsed time per iteration (ms): 10890.8 | learning rate 4.257E-06 | lm loss 6.626313E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2925.00 | backward: 6958.35 | optimizer: 1003.51 | batch generator: 3.46 | data loader: 0.64
10.0.2.2:  iteration      318/    1000 | elapsed time per iteration (ms): 11051.8 | learning rate 4.271E-06 | lm loss 6.292963E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2725.97 | backward: 7076.90 | optimizer: 1001.25 | batch generator: 3.91 | data loader: 0.95
10.0.2.2:  iteration      319/    1000 | elapsed time per iteration (ms): 10225.5 | learning rate 4.286E-06 | lm loss 6.896200E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2670.58 | backward: 6552.49 | optimizer: 988.44 | batch generator: 3.33 | data loader: 0.61
10.0.2.2:  iteration      320/    1000 | elapsed time per iteration (ms): 11012.8 | learning rate 4.300E-06 | lm loss 6.420447E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2941.99 | backward: 7064.97 | optimizer: 985.10 | batch generator: 3.41 | data loader: 0.69
10.0.2.2:  iteration      321/    1000 | elapsed time per iteration (ms): 11055.1 | learning rate 4.314E-06 | lm loss 6.707069E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2931.12 | backward: 7086.57 | optimizer: 1000.48 | batch generator: 5.05 | data loader: 0.70
10.0.2.2:  iteration      322/    1000 | elapsed time per iteration (ms): 11010.7 | learning rate 4.329E-06 | lm loss 6.384131E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2955.09 | backward: 7062.24 | optimizer: 989.48 | batch generator: 3.46 | data loader: 0.81
10.0.2.2:  iteration      323/    1000 | elapsed time per iteration (ms): 11063.8 | learning rate 4.343E-06 | lm loss 6.314971E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2935.67 | backward: 7102.52 | optimizer: 987.45 | batch generator: 4.14 | data loader: 0.64
10.0.2.2:  iteration      324/    1000 | elapsed time per iteration (ms): 10859.6 | learning rate 4.357E-06 | lm loss 6.631219E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2699.35 | backward: 6942.41 | optimizer: 993.58 | batch generator: 3.53 | data loader: 0.68
10.0.2.2:  iteration      325/    1000 | elapsed time per iteration (ms): 11141.4 | learning rate 4.371E-06 | lm loss 6.470342E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2914.08 | backward: 7167.94 | optimizer: 986.75 | batch generator: 3.81 | data loader: 0.97
10.0.2.2:  iteration      326/    1000 | elapsed time per iteration (ms): 11563.5 | learning rate 4.386E-06 | lm loss 6.695720E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2710.99 | backward: 7558.21 | optimizer: 1035.90 | batch generator: 4.17 | data loader: 0.76
10.0.2.2:  iteration      327/    1000 | elapsed time per iteration (ms): 10989.3 | learning rate 4.400E-06 | lm loss 6.670585E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2662.10 | backward: 7028.49 | optimizer: 991.51 | batch generator: 5.24 | data loader: 1.13
10.0.2.2:  iteration      328/    1000 | elapsed time per iteration (ms): 11025.9 | learning rate 4.414E-06 | lm loss 6.399598E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2889.43 | backward: 7062.77 | optimizer: 988.39 | batch generator: 5.68 | data loader: 0.75
10.0.2.2:  iteration      329/    1000 | elapsed time per iteration (ms): 11103.9 | learning rate 4.429E-06 | lm loss 7.093848E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2722.66 | backward: 7115.63 | optimizer: 995.49 | batch generator: 3.46 | data loader: 0.70
10.0.2.2:  iteration      330/    1000 | elapsed time per iteration (ms): 17745.5 | learning rate 4.443E-06 | lm loss 6.429457E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2602.48 | backward: 9665.22 | optimizer: 5116.31 | batch generator: 6.38 | data loader: 0.77
10.0.2.2:  iteration      331/    1000 | elapsed time per iteration (ms): 11213.0 | learning rate 4.457E-06 | lm loss 6.406472E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2669.61 | backward: 7169.47 | optimizer: 991.94 | batch generator: 45.17 | data loader: 0.97
10.0.2.2:  iteration      332/    1000 | elapsed time per iteration (ms): 11088.5 | learning rate 4.471E-06 | lm loss 6.663433E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2589.14 | backward: 7117.07 | optimizer: 994.78 | batch generator: 6.02 | data loader: 0.99
10.0.2.2:  iteration      333/    1000 | elapsed time per iteration (ms): 11052.0 | learning rate 4.486E-06 | lm loss 6.217796E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2972.82 | backward: 7088.26 | optimizer: 988.79 | batch generator: 3.75 | data loader: 0.65
10.0.2.2:  iteration      334/    1000 | elapsed time per iteration (ms): 30301.7 | learning rate 4.500E-06 | lm loss 6.072929E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2953.04 | backward: 25986.99 | optimizer: 1336.12 | batch generator: 3.11 | data loader: 0.66
10.0.2.2:  iteration      335/    1000 | elapsed time per iteration (ms): 11151.3 | learning rate 4.514E-06 | lm loss 6.391921E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2601.10 | backward: 7145.57 | optimizer: 1038.79 | batch generator: 6.75 | data loader: 1.26
10.0.2.2:  iteration      336/    1000 | elapsed time per iteration (ms): 11059.4 | learning rate 4.529E-06 | lm loss 6.208888E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2914.68 | backward: 7109.21 | optimizer: 1018.68 | batch generator: 4.44 | data loader: 1.06
10.0.2.2:  iteration      337/    1000 | elapsed time per iteration (ms): 11170.7 | learning rate 4.543E-06 | lm loss 6.369413E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2985.97 | backward: 7141.72 | optimizer: 1039.54 | batch generator: 4.97 | data loader: 1.00
10.0.2.2:  iteration      338/    1000 | elapsed time per iteration (ms): 11163.8 | learning rate 4.557E-06 | lm loss 6.392330E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2974.67 | backward: 7139.04 | optimizer: 1046.25 | batch generator: 5.67 | data loader: 1.22
10.0.2.2:  iteration      339/    1000 | elapsed time per iteration (ms): 10541.3 | learning rate 4.571E-06 | lm loss 7.264695E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2603.84 | backward: 6752.13 | optimizer: 1025.71 | batch generator: 4.20 | data loader: 1.02
10.0.2.2:  iteration      340/    1000 | elapsed time per iteration (ms): 11061.4 | learning rate 4.586E-06 | lm loss 7.227832E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2596.55 | backward: 7069.05 | optimizer: 1024.88 | batch generator: 4.92 | data loader: 0.97
10.0.2.2:  iteration      341/    1000 | elapsed time per iteration (ms): 11107.9 | learning rate 4.600E-06 | lm loss 6.360750E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2874.13 | backward: 7151.14 | optimizer: 986.29 | batch generator: 3.67 | data loader: 0.85
10.0.2.2:  iteration      342/    1000 | elapsed time per iteration (ms): 11036.2 | learning rate 4.614E-06 | lm loss 6.482043E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2704.93 | backward: 7071.36 | optimizer: 991.61 | batch generator: 5.95 | data loader: 0.76
10.0.2.2:  iteration      343/    1000 | elapsed time per iteration (ms): 10991.6 | learning rate 4.629E-06 | lm loss 6.753307E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2593.59 | backward: 7058.62 | optimizer: 984.63 | batch generator: 3.56 | data loader: 0.80
10.0.2.2:  iteration      344/    1000 | elapsed time per iteration (ms): 21582.9 | learning rate 4.643E-06 | lm loss 6.950133E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2597.16 | backward: 17606.70 | optimizer: 995.99 | batch generator: 3.55 | data loader: 0.69
10.0.2.2:  iteration      345/    1000 | elapsed time per iteration (ms): 11053.8 | learning rate 4.657E-06 | lm loss 6.193824E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2640.44 | backward: 7103.50 | optimizer: 984.89 | batch generator: 5.45 | data loader: 0.79
10.0.2.2:  iteration      346/    1000 | elapsed time per iteration (ms): 10331.2 | learning rate 4.671E-06 | lm loss 6.700029E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2716.42 | backward: 6597.19 | optimizer: 990.11 | batch generator: 4.67 | data loader: 1.35
10.0.2.2:  iteration      347/    1000 | elapsed time per iteration (ms): 10238.4 | learning rate 4.686E-06 | lm loss 7.431873E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2675.81 | backward: 6573.89 | optimizer: 985.44 | batch generator: 4.38 | data loader: 0.77
10.0.2.2:  iteration      348/    1000 | elapsed time per iteration (ms): 10940.8 | learning rate 4.700E-06 | lm loss 6.545257E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2587.27 | backward: 7009.57 | optimizer: 991.03 | batch generator: 6.46 | data loader: 0.61
10.0.2.2:  iteration      349/    1000 | elapsed time per iteration (ms): 11054.4 | learning rate 4.714E-06 | lm loss 6.203224E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2929.97 | backward: 7077.65 | optimizer: 996.12 | batch generator: 6.74 | data loader: 0.78
10.0.2.2: [2022-04-01 16:07:36,760] [INFO] [logging.py:60:log_dist] [Rank 0] step=350, skipped=19, lr=[4.714285714285715e-06, 4.714285714285715e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 16:07:36,951] [INFO] [timer.py:164:stop] 0/350, SamplesPerSec=7.267675506042319
10.0.2.2:  iteration      350/    1000 | elapsed time per iteration (ms): 10972.0 | learning rate 4.729E-06 | lm loss 6.970907E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2576.89 | backward: 7008.57 | optimizer: 997.94 | batch generator: 4.85 | data loader: 0.60
10.0.2.2:  iteration      351/    1000 | elapsed time per iteration (ms): 10931.0 | learning rate 4.743E-06 | lm loss 6.564168E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2573.98 | backward: 6972.13 | optimizer: 997.17 | batch generator: 3.70 | data loader: 0.70
10.0.2.2:  iteration      352/    1000 | elapsed time per iteration (ms): 10973.6 | learning rate 4.757E-06 | lm loss 6.339869E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2959.10 | backward: 7022.68 | optimizer: 989.23 | batch generator: 3.66 | data loader: 0.62
10.0.2.2:  iteration      353/    1000 | elapsed time per iteration (ms): 11004.5 | learning rate 4.771E-06 | lm loss 6.232707E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2904.50 | backward: 7054.79 | optimizer: 990.88 | batch generator: 4.22 | data loader: 0.81
10.0.2.2:  iteration      354/    1000 | elapsed time per iteration (ms): 11056.0 | learning rate 4.786E-06 | lm loss 6.576270E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2650.51 | backward: 7091.42 | optimizer: 993.48 | batch generator: 4.06 | data loader: 0.52
10.0.2.2:  iteration      355/    1000 | elapsed time per iteration (ms): 11020.2 | learning rate 4.800E-06 | lm loss 6.211117E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2974.01 | backward: 7052.90 | optimizer: 989.52 | batch generator: 4.27 | data loader: 1.18
10.0.2.2:  iteration      356/    1000 | elapsed time per iteration (ms): 11030.4 | learning rate 4.814E-06 | lm loss 6.989979E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2742.27 | backward: 7086.26 | optimizer: 985.38 | batch generator: 4.61 | data loader: 0.64
10.0.2.2:  iteration      357/    1000 | elapsed time per iteration (ms): 10932.2 | learning rate 4.829E-06 | lm loss 6.447406E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2712.12 | backward: 6997.57 | optimizer: 999.91 | batch generator: 4.02 | data loader: 0.96
10.0.2.2:  iteration      358/    1000 | elapsed time per iteration (ms): 11045.5 | learning rate 4.843E-06 | lm loss 6.740821E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2915.85 | backward: 7087.13 | optimizer: 988.08 | batch generator: 4.47 | data loader: 0.90
10.0.2.2:  iteration      359/    1000 | elapsed time per iteration (ms): 10972.3 | learning rate 4.857E-06 | lm loss 6.142960E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2945.98 | backward: 7007.55 | optimizer: 987.60 | batch generator: 4.27 | data loader: 1.19
10.0.2.2:  iteration      360/    1000 | elapsed time per iteration (ms): 10172.4 | learning rate 4.871E-06 | lm loss 6.931982E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2687.19 | backward: 6485.08 | optimizer: 982.80 | batch generator: 3.59 | data loader: 0.76
10.0.2.2:  iteration      361/    1000 | elapsed time per iteration (ms): 10791.5 | learning rate 4.886E-06 | lm loss 6.995489E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2592.22 | backward: 6916.75 | optimizer: 990.24 | batch generator: 4.89 | data loader: 0.88
10.0.2.2:  iteration      362/    1000 | elapsed time per iteration (ms): 11093.6 | learning rate 4.900E-06 | lm loss 6.530928E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2682.06 | backward: 7101.75 | optimizer: 985.95 | batch generator: 3.65 | data loader: 0.73
10.0.2.2:  iteration      363/    1000 | elapsed time per iteration (ms): 11007.0 | learning rate 4.914E-06 | lm loss 6.249776E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2675.86 | backward: 7052.61 | optimizer: 986.29 | batch generator: 3.78 | data loader: 1.04
10.0.2.2:  iteration      364/    1000 | elapsed time per iteration (ms): 11113.5 | learning rate 4.929E-06 | lm loss 6.805869E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2957.35 | backward: 7142.71 | optimizer: 994.76 | batch generator: 4.08 | data loader: 0.89
10.0.2.2:  iteration      365/    1000 | elapsed time per iteration (ms): 11083.7 | learning rate 4.943E-06 | lm loss 6.311080E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2919.41 | backward: 7103.13 | optimizer: 1002.06 | batch generator: 5.52 | data loader: 0.74
10.0.2.2:  iteration      366/    1000 | elapsed time per iteration (ms): 11089.6 | learning rate 4.957E-06 | lm loss 6.530681E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2906.01 | backward: 7101.64 | optimizer: 981.06 | batch generator: 3.13 | data loader: 0.57
10.0.2.2:  iteration      367/    1000 | elapsed time per iteration (ms): 10973.4 | learning rate 4.971E-06 | lm loss 6.699738E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2592.97 | backward: 7005.94 | optimizer: 988.17 | batch generator: 3.77 | data loader: 0.92
10.0.2.2:  iteration      368/    1000 | elapsed time per iteration (ms): 11048.2 | learning rate 4.986E-06 | lm loss 5.931723E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2921.58 | backward: 7083.21 | optimizer: 984.62 | batch generator: 3.86 | data loader: 0.77
10.0.2.2:  iteration      369/    1000 | elapsed time per iteration (ms): 10918.4 | learning rate 5.000E-06 | lm loss 6.595892E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2666.57 | backward: 6982.55 | optimizer: 996.80 | batch generator: 3.85 | data loader: 0.75
10.0.2.2:  iteration      370/    1000 | elapsed time per iteration (ms): 11009.0 | learning rate 5.014E-06 | lm loss 6.469988E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2905.27 | backward: 7041.65 | optimizer: 1006.99 | batch generator: 3.46 | data loader: 0.65
10.0.2.2:  iteration      371/    1000 | elapsed time per iteration (ms): 10969.9 | learning rate 5.029E-06 | lm loss 6.325379E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2633.88 | backward: 7016.47 | optimizer: 984.39 | batch generator: 3.40 | data loader: 0.65
10.0.2.2:  iteration      372/    1000 | elapsed time per iteration (ms): 10930.7 | learning rate 5.043E-06 | lm loss 6.505020E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2692.23 | backward: 6983.73 | optimizer: 995.96 | batch generator: 3.64 | data loader: 0.77
10.0.2.2:  iteration      373/    1000 | elapsed time per iteration (ms): 10935.2 | learning rate 5.057E-06 | lm loss 6.675408E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2937.68 | backward: 7002.25 | optimizer: 992.62 | batch generator: 4.16 | data loader: 0.55
10.0.2.2:  iteration      374/    1000 | elapsed time per iteration (ms): 11016.9 | learning rate 5.071E-06 | lm loss 6.738503E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2563.60 | backward: 7062.14 | optimizer: 990.32 | batch generator: 4.47 | data loader: 0.70
10.0.2.2:  iteration      375/    1000 | elapsed time per iteration (ms): 11033.8 | learning rate 5.086E-06 | lm loss 6.319489E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2936.42 | backward: 7054.57 | optimizer: 991.45 | batch generator: 3.67 | data loader: 0.64
10.0.2.2:  iteration      376/    1000 | elapsed time per iteration (ms): 11087.6 | learning rate 5.100E-06 | lm loss 6.050016E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2918.79 | backward: 7123.47 | optimizer: 983.54 | batch generator: 6.33 | data loader: 0.74
10.0.2.2:  iteration      377/    1000 | elapsed time per iteration (ms): 10220.8 | learning rate 5.114E-06 | lm loss 7.274890E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2693.56 | backward: 6529.82 | optimizer: 994.66 | batch generator: 4.41 | data loader: 0.67
10.0.2.2:  iteration      378/    1000 | elapsed time per iteration (ms): 11055.4 | learning rate 5.129E-06 | lm loss 6.724733E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2932.08 | backward: 7100.90 | optimizer: 993.07 | batch generator: 5.19 | data loader: 0.57
10.0.2.2:  iteration      379/    1000 | elapsed time per iteration (ms): 10944.4 | learning rate 5.143E-06 | lm loss 6.443717E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2640.57 | backward: 7023.35 | optimizer: 985.70 | batch generator: 3.50 | data loader: 0.69
10.0.2.2:  iteration      380/    1000 | elapsed time per iteration (ms): 10978.2 | learning rate 5.157E-06 | lm loss 6.382656E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2932.67 | backward: 7052.89 | optimizer: 988.71 | batch generator: 3.34 | data loader: 0.72
10.0.2.2:  iteration      381/    1000 | elapsed time per iteration (ms): 11021.9 | learning rate 5.171E-06 | lm loss 6.031218E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2959.67 | backward: 7065.70 | optimizer: 993.29 | batch generator: 3.54 | data loader: 0.77
10.0.2.2:  iteration      382/    1000 | elapsed time per iteration (ms): 10944.2 | learning rate 5.186E-06 | lm loss 6.471789E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2560.16 | backward: 6995.07 | optimizer: 983.73 | batch generator: 4.04 | data loader: 0.82
10.0.2.2:  iteration      383/    1000 | elapsed time per iteration (ms): 10901.9 | learning rate 5.200E-06 | lm loss 6.797354E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2931.13 | backward: 6977.09 | optimizer: 990.12 | batch generator: 8.21 | data loader: 0.79
10.0.2.2:  iteration      384/    1000 | elapsed time per iteration (ms): 11048.9 | learning rate 5.214E-06 | lm loss 6.328251E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2924.99 | backward: 7067.97 | optimizer: 988.69 | batch generator: 4.32 | data loader: 1.11
10.0.2.2:  iteration      385/    1000 | elapsed time per iteration (ms): 11051.9 | learning rate 5.229E-06 | lm loss 6.253026E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2953.56 | backward: 7073.18 | optimizer: 996.52 | batch generator: 4.81 | data loader: 0.96
10.0.2.2:  iteration      386/    1000 | elapsed time per iteration (ms): 11088.3 | learning rate 5.243E-06 | lm loss 6.362501E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2950.13 | backward: 7120.60 | optimizer: 1003.41 | batch generator: 3.45 | data loader: 0.70
10.0.2.2:  iteration      387/    1000 | elapsed time per iteration (ms): 11048.3 | learning rate 5.257E-06 | lm loss 6.498020E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2580.36 | backward: 7087.41 | optimizer: 984.68 | batch generator: 3.49 | data loader: 0.86
10.0.2.2:  iteration      388/    1000 | elapsed time per iteration (ms): 11061.1 | learning rate 5.271E-06 | lm loss 6.176410E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2962.39 | backward: 7106.48 | optimizer: 989.51 | batch generator: 4.93 | data loader: 0.61
10.0.2.2:  iteration      389/    1000 | elapsed time per iteration (ms): 10943.9 | learning rate 5.286E-06 | lm loss 6.517990E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2766.02 | backward: 6984.59 | optimizer: 989.15 | batch generator: 8.19 | data loader: 0.56
10.0.2.2:  iteration      390/    1000 | elapsed time per iteration (ms): 10885.7 | learning rate 5.300E-06 | lm loss 6.911996E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2931.61 | backward: 6966.62 | optimizer: 983.64 | batch generator: 4.27 | data loader: 0.75
10.0.2.2:  iteration      391/    1000 | elapsed time per iteration (ms): 11075.2 | learning rate 5.314E-06 | lm loss 6.452643E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2826.62 | backward: 7106.59 | optimizer: 985.24 | batch generator: 3.96 | data loader: 0.61
10.0.2.2:  iteration      392/    1000 | elapsed time per iteration (ms): 10878.7 | learning rate 5.329E-06 | lm loss 6.921221E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2611.17 | backward: 6947.22 | optimizer: 984.57 | batch generator: 5.20 | data loader: 0.68
10.0.2.2:  iteration      393/    1000 | elapsed time per iteration (ms): 11009.3 | learning rate 5.343E-06 | lm loss 6.669347E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2637.15 | backward: 7054.96 | optimizer: 994.71 | batch generator: 3.87 | data loader: 0.71
10.0.2.2:  iteration      394/    1000 | elapsed time per iteration (ms): 10865.1 | learning rate 5.357E-06 | lm loss 6.419301E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2930.61 | backward: 6937.22 | optimizer: 994.23 | batch generator: 4.52 | data loader: 1.08
10.0.2.2:  iteration      395/    1000 | elapsed time per iteration (ms): 11074.0 | learning rate 5.371E-06 | lm loss 6.383555E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2777.13 | backward: 7092.28 | optimizer: 990.53 | batch generator: 5.79 | data loader: 0.66
10.0.2.2:  iteration      396/    1000 | elapsed time per iteration (ms): 11056.0 | learning rate 5.386E-06 | lm loss 6.162215E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2573.34 | backward: 7044.08 | optimizer: 991.51 | batch generator: 3.86 | data loader: 0.81
10.0.2.2:  iteration      397/    1000 | elapsed time per iteration (ms): 10918.6 | learning rate 5.400E-06 | lm loss 6.418066E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2809.28 | backward: 6987.29 | optimizer: 983.42 | batch generator: 8.27 | data loader: 0.58
10.0.2.2:  iteration      398/    1000 | elapsed time per iteration (ms): 10986.5 | learning rate 5.414E-06 | lm loss 6.711433E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2973.32 | backward: 7017.75 | optimizer: 993.51 | batch generator: 5.19 | data loader: 0.72
10.0.2.2:  iteration      399/    1000 | elapsed time per iteration (ms): 11058.5 | learning rate 5.429E-06 | lm loss 6.067584E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2904.30 | backward: 7091.45 | optimizer: 985.82 | batch generator: 3.01 | data loader: 0.59
10.0.2.2: [2022-04-01 16:16:45,004] [INFO] [logging.py:60:log_dist] [Rank 0] step=400, skipped=19, lr=[5.428571428571428e-06, 5.428571428571428e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 16:16:45,193] [INFO] [timer.py:164:stop] 0/400, SamplesPerSec=7.315362449621419
10.0.2.2:  iteration      400/    1000 | elapsed time per iteration (ms): 10880.2 | learning rate 5.443E-06 | lm loss 6.917873E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2910.35 | backward: 6975.37 | optimizer: 990.84 | batch generator: 3.27 | data loader: 0.53
10.0.2.2:  iteration      401/    1000 | elapsed time per iteration (ms): 10118.2 | learning rate 5.457E-06 | lm loss 7.346680E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 1565.15 | backward: 6456.88 | optimizer: 991.22 | batch generator: 3.19 | data loader: 0.49
10.0.2.2:  iteration      402/    1000 | elapsed time per iteration (ms): 12435.6 | learning rate 5.471E-06 | lm loss 6.227561E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2955.92 | backward: 8401.44 | optimizer: 1054.90 | batch generator: 5.03 | data loader: 1.68
10.0.2.2:  iteration      403/    1000 | elapsed time per iteration (ms): 11135.6 | learning rate 5.486E-06 | lm loss 6.050868E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2695.45 | backward: 7173.37 | optimizer: 985.86 | batch generator: 7.21 | data loader: 1.19
10.0.2.2:  iteration      404/    1000 | elapsed time per iteration (ms): 10943.1 | learning rate 5.500E-06 | lm loss 6.409994E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2968.67 | backward: 6978.44 | optimizer: 991.60 | batch generator: 9.95 | data loader: 3.04
10.0.2.2:  iteration      405/    1000 | elapsed time per iteration (ms): 11102.5 | learning rate 5.514E-06 | lm loss 6.706634E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2706.47 | backward: 7117.56 | optimizer: 987.58 | batch generator: 3.75 | data loader: 0.83
10.0.2.2:  iteration      406/    1000 | elapsed time per iteration (ms): 12061.5 | learning rate 5.529E-06 | lm loss 6.527123E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2757.90 | backward: 7434.83 | optimizer: 1659.27 | batch generator: 3.32 | data loader: 0.72
10.0.2.2:  iteration      407/    1000 | elapsed time per iteration (ms): 11218.3 | learning rate 5.543E-06 | lm loss 6.382692E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2962.70 | backward: 7194.54 | optimizer: 1016.16 | batch generator: 15.12 | data loader: 1.37
10.0.2.2:  iteration      408/    1000 | elapsed time per iteration (ms): 10916.4 | learning rate 5.557E-06 | lm loss 6.038361E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2952.90 | backward: 6969.19 | optimizer: 990.74 | batch generator: 4.18 | data loader: 0.85
10.0.2.2:  iteration      409/    1000 | elapsed time per iteration (ms): 10922.7 | learning rate 5.571E-06 | lm loss 6.663738E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2591.37 | backward: 7015.13 | optimizer: 1000.49 | batch generator: 4.26 | data loader: 0.66
10.0.2.2:  iteration      410/    1000 | elapsed time per iteration (ms): 11504.8 | learning rate 5.586E-06 | lm loss 6.605553E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2636.26 | backward: 7473.79 | optimizer: 1044.71 | batch generator: 15.47 | data loader: 0.78
10.0.2.2:  iteration      411/    1000 | elapsed time per iteration (ms): 11658.8 | learning rate 5.600E-06 | lm loss 6.914615E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2605.51 | backward: 7113.19 | optimizer: 1571.52 | batch generator: 4.77 | data loader: 1.18
10.0.2.2:  iteration      412/    1000 | elapsed time per iteration (ms): 11976.9 | learning rate 5.614E-06 | lm loss 6.461100E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2600.39 | backward: 7470.93 | optimizer: 1509.71 | batch generator: 15.96 | data loader: 0.99
10.0.2.2:  iteration      413/    1000 | elapsed time per iteration (ms): 12356.2 | learning rate 5.629E-06 | lm loss 6.298543E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 1474.94 | backward: 7993.94 | optimizer: 1356.14 | batch generator: 4.73 | data loader: 1.14
10.0.2.2:  iteration      414/    1000 | elapsed time per iteration (ms): 11149.3 | learning rate 5.643E-06 | lm loss 6.703850E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2979.71 | backward: 7137.15 | optimizer: 1027.23 | batch generator: 4.29 | data loader: 1.06
10.0.2.2:  iteration      415/    1000 | elapsed time per iteration (ms): 11095.6 | learning rate 5.657E-06 | lm loss 6.486474E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2919.08 | backward: 7082.66 | optimizer: 1036.09 | batch generator: 4.34 | data loader: 0.70
10.0.2.2:  iteration      416/    1000 | elapsed time per iteration (ms): 11188.2 | learning rate 5.671E-06 | lm loss 6.763147E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2685.61 | backward: 7174.81 | optimizer: 1034.02 | batch generator: 7.73 | data loader: 1.65
10.0.2.2:  iteration      417/    1000 | elapsed time per iteration (ms): 11333.2 | learning rate 5.686E-06 | lm loss 6.425112E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 3022.31 | backward: 7258.62 | optimizer: 1046.37 | batch generator: 9.21 | data loader: 4.55
10.0.2.2:  iteration      418/    1000 | elapsed time per iteration (ms): 11097.3 | learning rate 5.700E-06 | lm loss 6.489347E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2588.59 | backward: 7068.22 | optimizer: 1038.54 | batch generator: 6.51 | data loader: 1.24
10.0.2.2:  iteration      419/    1000 | elapsed time per iteration (ms): 10948.0 | learning rate 5.714E-06 | lm loss 6.684645E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2645.68 | backward: 6939.89 | optimizer: 1059.81 | batch generator: 11.03 | data loader: 1.18
10.0.2.2:  iteration      420/    1000 | elapsed time per iteration (ms): 11029.6 | learning rate 5.729E-06 | lm loss 6.087705E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2895.81 | backward: 7065.32 | optimizer: 997.24 | batch generator: 5.34 | data loader: 1.44
10.0.2.2:  iteration      421/    1000 | elapsed time per iteration (ms): 11126.5 | learning rate 5.743E-06 | lm loss 6.336264E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2875.54 | backward: 7149.83 | optimizer: 990.66 | batch generator: 4.13 | data loader: 0.78
10.0.2.2:  iteration      422/    1000 | elapsed time per iteration (ms): 11003.4 | learning rate 5.757E-06 | lm loss 6.405828E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2953.94 | backward: 7055.74 | optimizer: 990.50 | batch generator: 6.47 | data loader: 0.68
10.0.2.2:  iteration      423/    1000 | elapsed time per iteration (ms): 10865.5 | learning rate 5.771E-06 | lm loss 6.757768E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2596.29 | backward: 6942.73 | optimizer: 991.02 | batch generator: 3.66 | data loader: 0.42
10.0.2.2:  iteration      424/    1000 | elapsed time per iteration (ms): 10985.0 | learning rate 5.786E-06 | lm loss 6.578541E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2937.09 | backward: 7058.24 | optimizer: 986.46 | batch generator: 4.08 | data loader: 0.81
10.0.2.2:  iteration      425/    1000 | elapsed time per iteration (ms): 11187.6 | learning rate 5.800E-06 | lm loss 6.278067E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2935.88 | backward: 7187.50 | optimizer: 986.08 | batch generator: 3.64 | data loader: 0.63
10.0.2.2:  iteration      426/    1000 | elapsed time per iteration (ms): 10134.1 | learning rate 5.814E-06 | lm loss 7.168260E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2680.27 | backward: 6468.59 | optimizer: 982.28 | batch generator: 3.61 | data loader: 0.71
10.0.2.2:  iteration      427/    1000 | elapsed time per iteration (ms): 11035.3 | learning rate 5.829E-06 | lm loss 6.587605E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2966.72 | backward: 7075.00 | optimizer: 986.05 | batch generator: 7.09 | data loader: 0.92
10.0.2.2:  iteration      428/    1000 | elapsed time per iteration (ms): 11012.6 | learning rate 5.843E-06 | lm loss 6.718490E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2681.38 | backward: 7044.62 | optimizer: 988.67 | batch generator: 8.44 | data loader: 0.82
10.0.2.2:  iteration      429/    1000 | elapsed time per iteration (ms): 21551.2 | learning rate 5.857E-06 | lm loss 6.287520E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2972.80 | backward: 17561.03 | optimizer: 992.31 | batch generator: 5.09 | data loader: 0.59
10.0.2.2:  iteration      430/    1000 | elapsed time per iteration (ms): 11008.4 | learning rate 5.871E-06 | lm loss 6.706228E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2433.89 | backward: 7050.06 | optimizer: 985.19 | batch generator: 3.42 | data loader: 0.66
10.0.2.2:  iteration      431/    1000 | elapsed time per iteration (ms): 11072.1 | learning rate 5.886E-06 | lm loss 6.251253E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2706.08 | backward: 7086.63 | optimizer: 1009.08 | batch generator: 4.56 | data loader: 0.92
10.0.2.2:  iteration      432/    1000 | elapsed time per iteration (ms): 11009.8 | learning rate 5.900E-06 | lm loss 6.125418E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2932.96 | backward: 7042.02 | optimizer: 986.57 | batch generator: 4.28 | data loader: 0.70
10.0.2.2:  iteration      433/    1000 | elapsed time per iteration (ms): 11037.1 | learning rate 5.914E-06 | lm loss 6.421014E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2914.58 | backward: 7081.48 | optimizer: 990.88 | batch generator: 3.02 | data loader: 0.58
10.0.2.2:  iteration      434/    1000 | elapsed time per iteration (ms): 11054.3 | learning rate 5.929E-06 | lm loss 6.087973E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2942.61 | backward: 7072.76 | optimizer: 986.73 | batch generator: 4.51 | data loader: 0.60
10.0.2.2:  iteration      435/    1000 | elapsed time per iteration (ms): 11010.9 | learning rate 5.943E-06 | lm loss 6.350677E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2929.55 | backward: 7051.90 | optimizer: 993.64 | batch generator: 3.51 | data loader: 0.68
10.0.2.2:  iteration      436/    1000 | elapsed time per iteration (ms): 10892.5 | learning rate 5.957E-06 | lm loss 6.819119E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2561.09 | backward: 6964.73 | optimizer: 994.94 | batch generator: 4.62 | data loader: 0.76
10.0.2.2:  iteration      437/    1000 | elapsed time per iteration (ms): 11051.6 | learning rate 5.971E-06 | lm loss 6.530205E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2764.49 | backward: 7074.37 | optimizer: 1000.66 | batch generator: 3.52 | data loader: 0.77
10.0.2.2:  iteration      438/    1000 | elapsed time per iteration (ms): 10934.9 | learning rate 5.986E-06 | lm loss 6.784375E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2963.21 | backward: 6985.26 | optimizer: 982.62 | batch generator: 8.50 | data loader: 0.56
10.0.2.2:  iteration      439/    1000 | elapsed time per iteration (ms): 11025.2 | learning rate 6.000E-06 | lm loss 5.949805E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2983.35 | backward: 7055.11 | optimizer: 983.39 | batch generator: 4.66 | data loader: 0.64
10.0.2.2:  iteration      440/    1000 | elapsed time per iteration (ms): 10901.5 | learning rate 6.014E-06 | lm loss 6.621852E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2673.40 | backward: 6978.64 | optimizer: 993.25 | batch generator: 3.88 | data loader: 0.78
10.0.2.2:  iteration      441/    1000 | elapsed time per iteration (ms): 11020.9 | learning rate 6.029E-06 | lm loss 6.304528E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2963.09 | backward: 7064.01 | optimizer: 988.19 | batch generator: 4.46 | data loader: 0.70
10.0.2.2:  iteration      442/    1000 | elapsed time per iteration (ms): 11032.9 | learning rate 6.043E-06 | lm loss 6.025668E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2919.90 | backward: 7090.81 | optimizer: 980.77 | batch generator: 5.18 | data loader: 1.65
10.0.2.2:  iteration      443/    1000 | elapsed time per iteration (ms): 11044.4 | learning rate 6.057E-06 | lm loss 6.694098E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2986.51 | backward: 7059.21 | optimizer: 995.24 | batch generator: 6.49 | data loader: 0.76
10.0.2.2:  iteration      444/    1000 | elapsed time per iteration (ms): 11078.1 | learning rate 6.071E-06 | lm loss 5.919462E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2916.55 | backward: 7105.47 | optimizer: 991.00 | batch generator: 3.48 | data loader: 0.64
10.0.2.2:  iteration      445/    1000 | elapsed time per iteration (ms): 10999.9 | learning rate 6.086E-06 | lm loss 6.382327E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2619.83 | backward: 7049.58 | optimizer: 986.27 | batch generator: 4.03 | data loader: 0.85
10.0.2.2:  iteration      446/    1000 | elapsed time per iteration (ms): 10991.4 | learning rate 6.100E-06 | lm loss 5.956599E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2965.20 | backward: 7017.08 | optimizer: 1006.25 | batch generator: 8.22 | data loader: 0.71
10.0.2.2:  iteration      447/    1000 | elapsed time per iteration (ms): 11077.5 | learning rate 6.114E-06 | lm loss 6.210183E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2726.08 | backward: 7102.82 | optimizer: 991.26 | batch generator: 5.23 | data loader: 0.58
10.0.2.2:  iteration      448/    1000 | elapsed time per iteration (ms): 10914.7 | learning rate 6.129E-06 | lm loss 6.646703E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2925.35 | backward: 6979.71 | optimizer: 1006.01 | batch generator: 4.51 | data loader: 1.01
10.0.2.2:  iteration      449/    1000 | elapsed time per iteration (ms): 11066.5 | learning rate 6.143E-06 | lm loss 6.732132E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2578.37 | backward: 7080.98 | optimizer: 988.60 | batch generator: 6.72 | data loader: 0.92
10.0.2.2: [2022-04-01 16:26:11,333] [INFO] [logging.py:60:log_dist] [Rank 0] step=450, skipped=19, lr=[6.142857142857143e-06, 6.142857142857143e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 16:26:11,537] [INFO] [timer.py:164:stop] 0/450, SamplesPerSec=7.327052784622669
10.0.2.2:  iteration      450/    1000 | elapsed time per iteration (ms): 11020.9 | learning rate 6.157E-06 | lm loss 6.585422E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2945.85 | backward: 7073.54 | optimizer: 981.13 | batch generator: 5.32 | data loader: 2.12
10.0.2.2:  iteration      451/    1000 | elapsed time per iteration (ms): 10984.7 | learning rate 6.171E-06 | lm loss 5.808023E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2912.28 | backward: 7028.19 | optimizer: 992.79 | batch generator: 4.54 | data loader: 0.48
10.0.2.2:  iteration      452/    1000 | elapsed time per iteration (ms): 11064.2 | learning rate 6.186E-06 | lm loss 5.777733E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2917.37 | backward: 7089.06 | optimizer: 989.36 | batch generator: 3.10 | data loader: 0.53
10.0.2.2:  iteration      453/    1000 | elapsed time per iteration (ms): 11037.5 | learning rate 6.200E-06 | lm loss 6.813383E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2024.30 | backward: 7065.05 | optimizer: 990.66 | batch generator: 3.84 | data loader: 0.66
10.0.2.2:  iteration      454/    1000 | elapsed time per iteration (ms): 11021.3 | learning rate 6.214E-06 | lm loss 6.457751E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2952.80 | backward: 7047.48 | optimizer: 985.04 | batch generator: 5.94 | data loader: 0.65
10.0.2.2:  iteration      455/    1000 | elapsed time per iteration (ms): 10916.9 | learning rate 6.229E-06 | lm loss 6.663416E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2545.35 | backward: 6980.55 | optimizer: 989.94 | batch generator: 6.01 | data loader: 0.68
10.0.2.2:  iteration      456/    1000 | elapsed time per iteration (ms): 10939.5 | learning rate 6.243E-06 | lm loss 6.414878E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2938.19 | backward: 6984.09 | optimizer: 1010.68 | batch generator: 3.41 | data loader: 0.69
10.0.2.2:  iteration      457/    1000 | elapsed time per iteration (ms): 11081.7 | learning rate 6.257E-06 | lm loss 6.331559E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2585.21 | backward: 7097.24 | optimizer: 989.85 | batch generator: 10.77 | data loader: 0.60
10.0.2.2:  iteration      458/    1000 | elapsed time per iteration (ms): 10945.8 | learning rate 6.271E-06 | lm loss 6.218001E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2758.51 | backward: 7022.48 | optimizer: 988.66 | batch generator: 8.79 | data loader: 0.70
10.0.2.2:  iteration      459/    1000 | elapsed time per iteration (ms): 10853.3 | learning rate 6.286E-06 | lm loss 6.341593E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2795.49 | backward: 6933.49 | optimizer: 985.31 | batch generator: 4.81 | data loader: 0.66
10.0.2.2:  iteration      460/    1000 | elapsed time per iteration (ms): 10963.9 | learning rate 6.300E-06 | lm loss 5.991395E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2596.90 | backward: 7020.56 | optimizer: 996.74 | batch generator: 8.12 | data loader: 0.54
10.0.2.2:  iteration      461/    1000 | elapsed time per iteration (ms): 10943.9 | learning rate 6.314E-06 | lm loss 6.650718E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2954.50 | backward: 6995.23 | optimizer: 991.79 | batch generator: 9.73 | data loader: 1.60
10.0.2.2:  iteration      462/    1000 | elapsed time per iteration (ms): 11082.8 | learning rate 6.329E-06 | lm loss 6.411935E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2924.20 | backward: 7084.80 | optimizer: 986.21 | batch generator: 10.45 | data loader: 0.69
10.0.2.2:  iteration      463/    1000 | elapsed time per iteration (ms): 11035.7 | learning rate 6.343E-06 | lm loss 6.092049E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2980.00 | backward: 7064.56 | optimizer: 988.61 | batch generator: 6.55 | data loader: 1.12
10.0.2.2:  iteration      464/    1000 | elapsed time per iteration (ms): 11028.4 | learning rate 6.357E-06 | lm loss 6.625560E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2644.75 | backward: 7063.81 | optimizer: 988.35 | batch generator: 4.35 | data loader: 0.69
10.0.2.2:  iteration      465/    1000 | elapsed time per iteration (ms): 11026.9 | learning rate 6.371E-06 | lm loss 6.544593E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2629.09 | backward: 7058.11 | optimizer: 986.63 | batch generator: 4.29 | data loader: 0.72
10.0.2.2:  iteration      466/    1000 | elapsed time per iteration (ms): 11004.6 | learning rate 6.386E-06 | lm loss 6.655640E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2676.20 | backward: 7036.60 | optimizer: 990.13 | batch generator: 5.43 | data loader: 0.63
10.0.2.2:  iteration      467/    1000 | elapsed time per iteration (ms): 10883.8 | learning rate 6.400E-06 | lm loss 6.682514E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2591.32 | backward: 6984.23 | optimizer: 979.97 | batch generator: 4.13 | data loader: 1.00
10.0.2.2:  iteration      468/    1000 | elapsed time per iteration (ms): 10851.6 | learning rate 6.414E-06 | lm loss 6.450031E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2901.77 | backward: 6950.38 | optimizer: 990.12 | batch generator: 4.50 | data loader: 1.61
10.0.2.2:  iteration      469/    1000 | elapsed time per iteration (ms): 10803.4 | learning rate 6.429E-06 | lm loss 6.921890E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2614.66 | backward: 6900.15 | optimizer: 983.50 | batch generator: 3.43 | data loader: 0.86
10.0.2.2:  iteration      470/    1000 | elapsed time per iteration (ms): 11000.9 | learning rate 6.443E-06 | lm loss 5.667749E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2959.87 | backward: 7049.73 | optimizer: 988.62 | batch generator: 3.87 | data loader: 0.80
10.0.2.2:  iteration      471/    1000 | elapsed time per iteration (ms): 10962.8 | learning rate 6.457E-06 | lm loss 6.508074E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2714.72 | backward: 7023.50 | optimizer: 988.32 | batch generator: 3.35 | data loader: 0.69
10.0.2.2:  iteration      472/    1000 | elapsed time per iteration (ms): 11074.2 | learning rate 6.471E-06 | lm loss 6.793488E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2607.67 | backward: 7101.91 | optimizer: 993.97 | batch generator: 6.79 | data loader: 0.52
10.0.2.2:  iteration      473/    1000 | elapsed time per iteration (ms): 11068.6 | learning rate 6.486E-06 | lm loss 5.845522E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2973.06 | backward: 7109.68 | optimizer: 981.88 | batch generator: 12.45 | data loader: 0.63
10.0.2.2:  iteration      474/    1000 | elapsed time per iteration (ms): 11015.3 | learning rate 6.500E-06 | lm loss 6.472600E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2622.74 | backward: 7046.11 | optimizer: 987.82 | batch generator: 14.53 | data loader: 0.79
10.0.2.2:  iteration      475/    1000 | elapsed time per iteration (ms): 11092.4 | learning rate 6.514E-06 | lm loss 6.413827E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2951.05 | backward: 7107.58 | optimizer: 992.37 | batch generator: 3.98 | data loader: 0.79
10.0.2.2:  iteration      476/    1000 | elapsed time per iteration (ms): 11077.8 | learning rate 6.529E-06 | lm loss 6.263101E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2896.92 | backward: 7096.48 | optimizer: 1003.21 | batch generator: 8.06 | data loader: 0.67
10.0.2.2:  iteration      477/    1000 | elapsed time per iteration (ms): 11014.9 | learning rate 6.543E-06 | lm loss 5.930465E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2710.89 | backward: 7027.95 | optimizer: 996.44 | batch generator: 3.63 | data loader: 0.65
10.0.2.2:  iteration      478/    1000 | elapsed time per iteration (ms): 11028.5 | learning rate 6.557E-06 | lm loss 5.979396E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2677.95 | backward: 7075.83 | optimizer: 998.06 | batch generator: 4.14 | data loader: 0.73
10.0.2.2:  iteration      479/    1000 | elapsed time per iteration (ms): 11065.2 | learning rate 6.571E-06 | lm loss 5.727385E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2626.30 | backward: 7093.68 | optimizer: 983.82 | batch generator: 3.33 | data loader: 0.66
10.0.2.2:  iteration      480/    1000 | elapsed time per iteration (ms): 11090.1 | learning rate 6.586E-06 | lm loss 6.528430E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2630.77 | backward: 7101.47 | optimizer: 989.73 | batch generator: 4.45 | data loader: 0.65
10.0.2.2:  iteration      481/    1000 | elapsed time per iteration (ms): 11042.9 | learning rate 6.600E-06 | lm loss 5.935334E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2925.95 | backward: 7075.83 | optimizer: 999.07 | batch generator: 3.24 | data loader: 0.37
10.0.2.2:  iteration      482/    1000 | elapsed time per iteration (ms): 10992.0 | learning rate 6.614E-06 | lm loss 6.227427E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2951.03 | backward: 7044.82 | optimizer: 992.57 | batch generator: 3.22 | data loader: 0.67
10.0.2.2:  iteration      483/    1000 | elapsed time per iteration (ms): 11071.1 | learning rate 6.629E-06 | lm loss 6.139342E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2701.16 | backward: 7082.99 | optimizer: 987.43 | batch generator: 5.09 | data loader: 0.63
10.0.2.2:  iteration      484/    1000 | elapsed time per iteration (ms): 11051.2 | learning rate 6.643E-06 | lm loss 6.550932E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2606.99 | backward: 7099.13 | optimizer: 981.47 | batch generator: 7.82 | data loader: 0.66
10.0.2.2:  iteration      485/    1000 | elapsed time per iteration (ms): 11077.8 | learning rate 6.657E-06 | lm loss 6.138205E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2917.83 | backward: 7089.88 | optimizer: 997.51 | batch generator: 4.05 | data loader: 0.70
10.0.2.2:  iteration      486/    1000 | elapsed time per iteration (ms): 11015.0 | learning rate 6.671E-06 | lm loss 6.020795E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2958.38 | backward: 7070.34 | optimizer: 982.10 | batch generator: 4.13 | data loader: 0.66
10.0.2.2:  iteration      487/    1000 | elapsed time per iteration (ms): 10947.4 | learning rate 6.686E-06 | lm loss 5.956886E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2692.56 | backward: 6994.92 | optimizer: 993.07 | batch generator: 7.77 | data loader: 0.71
10.0.2.2:  iteration      488/    1000 | elapsed time per iteration (ms): 10994.0 | learning rate 6.700E-06 | lm loss 5.979309E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2675.87 | backward: 7024.83 | optimizer: 993.79 | batch generator: 7.36 | data loader: 0.73
10.0.2.2:  iteration      489/    1000 | elapsed time per iteration (ms): 10367.3 | learning rate 6.714E-06 | lm loss 6.478565E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2696.07 | backward: 6638.83 | optimizer: 990.76 | batch generator: 5.19 | data loader: 0.65
10.0.2.2:  iteration      490/    1000 | elapsed time per iteration (ms): 10973.8 | learning rate 6.729E-06 | lm loss 6.462100E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2712.70 | backward: 7000.18 | optimizer: 993.27 | batch generator: 3.13 | data loader: 0.73
10.0.2.2:  iteration      491/    1000 | elapsed time per iteration (ms): 11045.2 | learning rate 6.743E-06 | lm loss 6.369493E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2957.95 | backward: 7048.78 | optimizer: 987.40 | batch generator: 11.06 | data loader: 0.76
10.0.2.2:  iteration      492/    1000 | elapsed time per iteration (ms): 11116.8 | learning rate 6.757E-06 | lm loss 6.150118E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2902.51 | backward: 7132.68 | optimizer: 989.17 | batch generator: 4.22 | data loader: 0.64
10.0.2.2:  iteration      493/    1000 | elapsed time per iteration (ms): 10972.7 | learning rate 6.771E-06 | lm loss 6.238270E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2729.16 | backward: 7025.34 | optimizer: 989.49 | batch generator: 4.46 | data loader: 0.69
10.0.2.2:  iteration      494/    1000 | elapsed time per iteration (ms): 11017.2 | learning rate 6.786E-06 | lm loss 6.479430E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2888.37 | backward: 7046.17 | optimizer: 985.80 | batch generator: 3.80 | data loader: 0.86
10.0.2.2:  iteration      495/    1000 | elapsed time per iteration (ms): 11037.8 | learning rate 6.800E-06 | lm loss 6.466253E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2971.18 | backward: 7073.19 | optimizer: 989.99 | batch generator: 3.64 | data loader: 0.84
10.0.2.2:  iteration      496/    1000 | elapsed time per iteration (ms): 10831.8 | learning rate 6.814E-06 | lm loss 6.366892E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2370.85 | backward: 6923.81 | optimizer: 993.56 | batch generator: 4.74 | data loader: 0.63
10.0.2.2:  iteration      497/    1000 | elapsed time per iteration (ms): 11001.7 | learning rate 6.829E-06 | lm loss 6.334172E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2961.64 | backward: 7049.38 | optimizer: 988.45 | batch generator: 5.56 | data loader: 0.69
10.0.2.2:  iteration      498/    1000 | elapsed time per iteration (ms): 11067.0 | learning rate 6.843E-06 | lm loss 6.671381E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2725.16 | backward: 7092.33 | optimizer: 1001.96 | batch generator: 4.48 | data loader: 0.87
10.0.2.2:  iteration      499/    1000 | elapsed time per iteration (ms): 11027.0 | learning rate 6.857E-06 | lm loss 6.498074E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2964.22 | backward: 7055.29 | optimizer: 985.41 | batch generator: 3.66 | data loader: 0.67
10.0.2.2: [2022-04-01 16:35:21,103] [INFO] [logging.py:60:log_dist] [Rank 0] step=500, skipped=19, lr=[6.857142857142857e-06, 6.857142857142857e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 16:35:21,104] [INFO] [timer.py:164:stop] 0/500, SamplesPerSec=7.358030629429298
10.0.2.2:  iteration      500/    1000 | elapsed time per iteration (ms): 10951.8 | learning rate 6.871E-06 | lm loss 6.428685E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2664.35 | backward: 7020.11 | optimizer: 999.37 | batch generator: 4.85 | data loader: 0.85
10.0.2.2:  iteration      501/    1000 | elapsed time per iteration (ms): 11074.3 | learning rate 6.886E-06 | lm loss 6.277158E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2963.83 | backward: 7123.14 | optimizer: 984.09 | batch generator: 3.33 | data loader: 0.40
10.0.2.2:  iteration      502/    1000 | elapsed time per iteration (ms): 10980.9 | learning rate 6.900E-06 | lm loss 6.605827E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2942.35 | backward: 7032.07 | optimizer: 1002.83 | batch generator: 5.40 | data loader: 0.75
10.0.2.2:  iteration      503/    1000 | elapsed time per iteration (ms): 11021.8 | learning rate 6.914E-06 | lm loss 6.233206E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2920.36 | backward: 7026.75 | optimizer: 993.64 | batch generator: 6.73 | data loader: 0.63
10.0.2.2:  iteration      504/    1000 | elapsed time per iteration (ms): 11044.0 | learning rate 6.929E-06 | lm loss 6.577866E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2933.43 | backward: 7047.44 | optimizer: 1005.02 | batch generator: 3.52 | data loader: 0.67
10.0.2.2:  iteration      505/    1000 | elapsed time per iteration (ms): 10833.6 | learning rate 6.943E-06 | lm loss 6.896587E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2583.17 | backward: 6928.04 | optimizer: 1000.87 | batch generator: 7.54 | data loader: 0.63
10.0.2.2:  iteration      506/    1000 | elapsed time per iteration (ms): 10986.7 | learning rate 6.957E-06 | lm loss 6.589495E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2691.39 | backward: 7031.85 | optimizer: 987.25 | batch generator: 3.11 | data loader: 0.58
10.0.2.2:  iteration      507/    1000 | elapsed time per iteration (ms): 11017.8 | learning rate 6.971E-06 | lm loss 6.147558E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2899.70 | backward: 7055.26 | optimizer: 990.45 | batch generator: 4.45 | data loader: 0.65
10.0.2.2:  iteration      508/    1000 | elapsed time per iteration (ms): 10741.1 | learning rate 6.986E-06 | lm loss 6.712218E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2886.53 | backward: 6848.76 | optimizer: 1002.40 | batch generator: 3.49 | data loader: 0.56
10.0.2.2:  iteration      509/    1000 | elapsed time per iteration (ms): 10876.6 | learning rate 7.000E-06 | lm loss 6.840847E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2660.04 | backward: 6956.20 | optimizer: 987.89 | batch generator: 4.76 | data loader: 1.03
10.0.2.2:  iteration      510/    1000 | elapsed time per iteration (ms): 10985.9 | learning rate 7.014E-06 | lm loss 6.829178E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2695.86 | backward: 7026.60 | optimizer: 994.25 | batch generator: 4.62 | data loader: 1.76
10.0.2.2:  iteration      511/    1000 | elapsed time per iteration (ms): 11027.2 | learning rate 7.029E-06 | lm loss 6.264082E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2663.69 | backward: 7058.67 | optimizer: 987.69 | batch generator: 3.38 | data loader: 0.72
10.0.2.2:  iteration      512/    1000 | elapsed time per iteration (ms): 11004.2 | learning rate 7.043E-06 | lm loss 6.227390E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2930.90 | backward: 7041.23 | optimizer: 993.93 | batch generator: 3.33 | data loader: 0.68
10.0.2.2:  iteration      513/    1000 | elapsed time per iteration (ms): 11038.3 | learning rate 7.057E-06 | lm loss 6.543939E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2712.84 | backward: 7076.20 | optimizer: 999.10 | batch generator: 7.75 | data loader: 0.48
10.0.2.2:  iteration      514/    1000 | elapsed time per iteration (ms): 11105.8 | learning rate 7.071E-06 | lm loss 6.166399E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2901.99 | backward: 7129.70 | optimizer: 985.32 | batch generator: 3.21 | data loader: 0.51
10.0.2.2:  iteration      515/    1000 | elapsed time per iteration (ms): 11029.5 | learning rate 7.086E-06 | lm loss 6.392856E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2584.47 | backward: 7051.04 | optimizer: 998.78 | batch generator: 3.74 | data loader: 0.76
10.0.2.2:  iteration      516/    1000 | elapsed time per iteration (ms): 11087.3 | learning rate 7.100E-06 | lm loss 6.272594E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2950.51 | backward: 7116.20 | optimizer: 985.80 | batch generator: 3.64 | data loader: 0.65
10.0.2.2:  iteration      517/    1000 | elapsed time per iteration (ms): 11076.8 | learning rate 7.114E-06 | lm loss 6.278539E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2948.38 | backward: 7103.69 | optimizer: 998.30 | batch generator: 4.53 | data loader: 0.90
10.0.2.2:  iteration      518/    1000 | elapsed time per iteration (ms): 10881.9 | learning rate 7.129E-06 | lm loss 6.477906E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2590.68 | backward: 6967.24 | optimizer: 980.58 | batch generator: 5.11 | data loader: 0.61
10.0.2.2:  iteration      519/    1000 | elapsed time per iteration (ms): 10940.7 | learning rate 7.143E-06 | lm loss 5.595585E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2675.72 | backward: 7004.17 | optimizer: 995.15 | batch generator: 4.89 | data loader: 1.42
10.0.2.2:  iteration      520/    1000 | elapsed time per iteration (ms): 11012.2 | learning rate 7.157E-06 | lm loss 6.233228E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2663.29 | backward: 7062.99 | optimizer: 983.37 | batch generator: 3.77 | data loader: 0.85
10.0.2.2:  iteration      521/    1000 | elapsed time per iteration (ms): 11060.3 | learning rate 7.171E-06 | lm loss 6.115073E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2655.94 | backward: 7076.53 | optimizer: 990.97 | batch generator: 5.00 | data loader: 0.79
10.0.2.2:  iteration      522/    1000 | elapsed time per iteration (ms): 11041.8 | learning rate 7.186E-06 | lm loss 5.797628E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2954.54 | backward: 7054.08 | optimizer: 988.54 | batch generator: 3.79 | data loader: 1.04
10.0.2.2:  iteration      523/    1000 | elapsed time per iteration (ms): 11066.5 | learning rate 7.200E-06 | lm loss 6.110550E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2585.66 | backward: 7105.35 | optimizer: 989.53 | batch generator: 3.53 | data loader: 0.74
10.0.2.2:  iteration      524/    1000 | elapsed time per iteration (ms): 10944.9 | learning rate 7.214E-06 | lm loss 6.695978E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2684.64 | backward: 7003.91 | optimizer: 991.61 | batch generator: 3.47 | data loader: 0.75
10.0.2.2:  iteration      525/    1000 | elapsed time per iteration (ms): 11049.5 | learning rate 7.229E-06 | lm loss 6.337567E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2116.08 | backward: 7071.32 | optimizer: 988.43 | batch generator: 5.46 | data loader: 2.31
10.0.2.2:  iteration      526/    1000 | elapsed time per iteration (ms): 11063.1 | learning rate 7.243E-06 | lm loss 6.006813E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2966.12 | backward: 7077.51 | optimizer: 989.05 | batch generator: 4.20 | data loader: 0.79
10.0.2.2:  iteration      527/    1000 | elapsed time per iteration (ms): 10148.2 | learning rate 7.257E-06 | lm loss 6.556594E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2611.99 | backward: 6505.94 | optimizer: 997.87 | batch generator: 3.86 | data loader: 0.71
10.0.2.2:  iteration      528/    1000 | elapsed time per iteration (ms): 11016.6 | learning rate 7.271E-06 | lm loss 6.553371E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2954.72 | backward: 7053.05 | optimizer: 1004.68 | batch generator: 3.64 | data loader: 0.70
10.0.2.2:  iteration      529/    1000 | elapsed time per iteration (ms): 11078.4 | learning rate 7.286E-06 | lm loss 6.389863E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2611.04 | backward: 7074.51 | optimizer: 991.70 | batch generator: 4.15 | data loader: 0.72
10.0.2.2:  iteration      530/    1000 | elapsed time per iteration (ms): 10948.9 | learning rate 7.300E-06 | lm loss 6.992350E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2951.14 | backward: 7008.88 | optimizer: 984.46 | batch generator: 4.46 | data loader: 0.87
10.0.2.2:  iteration      531/    1000 | elapsed time per iteration (ms): 11125.6 | learning rate 7.314E-06 | lm loss 6.677711E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2964.18 | backward: 7155.29 | optimizer: 986.88 | batch generator: 3.84 | data loader: 0.68
10.0.2.2:  iteration      532/    1000 | elapsed time per iteration (ms): 10970.1 | learning rate 7.329E-06 | lm loss 6.452032E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2523.11 | backward: 7009.89 | optimizer: 993.53 | batch generator: 5.73 | data loader: 0.91
10.0.2.2:  iteration      533/    1000 | elapsed time per iteration (ms): 11113.5 | learning rate 7.343E-06 | lm loss 6.097629E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2958.35 | backward: 7113.24 | optimizer: 993.68 | batch generator: 6.49 | data loader: 0.85
10.0.2.2:  iteration      534/    1000 | elapsed time per iteration (ms): 10964.6 | learning rate 7.357E-06 | lm loss 6.353411E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2678.60 | backward: 6988.94 | optimizer: 1001.43 | batch generator: 3.48 | data loader: 0.67
10.0.2.2:  iteration      535/    1000 | elapsed time per iteration (ms): 10914.5 | learning rate 7.371E-06 | lm loss 6.318305E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2925.82 | backward: 6992.00 | optimizer: 985.36 | batch generator: 3.45 | data loader: 0.70
10.0.2.2:  iteration      536/    1000 | elapsed time per iteration (ms): 30969.1 | learning rate 7.386E-06 | lm loss 6.939980E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2552.13 | backward: 21168.34 | optimizer: 6811.54 | batch generator: 4.13 | data loader: 0.68
10.0.2.2:  iteration      537/    1000 | elapsed time per iteration (ms): 11508.4 | learning rate 7.400E-06 | lm loss 6.166800E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2907.72 | backward: 7490.45 | optimizer: 1050.56 | batch generator: 7.36 | data loader: 1.47
10.0.2.2:  iteration      538/    1000 | elapsed time per iteration (ms): 11120.9 | learning rate 7.414E-06 | lm loss 5.999192E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2745.81 | backward: 7093.23 | optimizer: 1076.58 | batch generator: 9.40 | data loader: 0.58
10.0.2.2:  iteration      539/    1000 | elapsed time per iteration (ms): 11159.1 | learning rate 7.429E-06 | lm loss 6.184834E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2952.00 | backward: 7139.11 | optimizer: 1057.53 | batch generator: 4.45 | data loader: 1.22
10.0.2.2:  iteration      540/    1000 | elapsed time per iteration (ms): 11139.2 | learning rate 7.443E-06 | lm loss 6.334529E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2909.89 | backward: 7100.47 | optimizer: 1072.45 | batch generator: 5.50 | data loader: 1.57
10.0.2.2:  iteration      541/    1000 | elapsed time per iteration (ms): 11183.8 | learning rate 7.457E-06 | lm loss 6.010987E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2954.94 | backward: 7149.82 | optimizer: 1058.37 | batch generator: 5.59 | data loader: 1.24
10.0.2.2:  iteration      542/    1000 | elapsed time per iteration (ms): 11039.8 | learning rate 7.471E-06 | lm loss 6.902102E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2598.30 | backward: 7057.89 | optimizer: 1033.29 | batch generator: 4.64 | data loader: 1.24
10.0.2.2:  iteration      543/    1000 | elapsed time per iteration (ms): 11141.2 | learning rate 7.486E-06 | lm loss 5.850216E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2969.27 | backward: 7130.82 | optimizer: 1033.67 | batch generator: 7.05 | data loader: 1.32
10.0.2.2:  iteration      544/    1000 | elapsed time per iteration (ms): 11089.6 | learning rate 7.500E-06 | lm loss 5.913627E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2953.74 | backward: 7115.91 | optimizer: 992.74 | batch generator: 7.93 | data loader: 1.26
10.0.2.2:  iteration      545/    1000 | elapsed time per iteration (ms): 11031.4 | learning rate 7.514E-06 | lm loss 5.905450E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2972.34 | backward: 7059.67 | optimizer: 993.46 | batch generator: 6.22 | data loader: 1.80
10.0.2.2:  iteration      546/    1000 | elapsed time per iteration (ms): 21765.4 | learning rate 7.529E-06 | lm loss 5.769257E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2941.02 | backward: 17822.29 | optimizer: 983.09 | batch generator: 3.85 | data loader: 0.82
10.0.2.2:  iteration      547/    1000 | elapsed time per iteration (ms): 10930.2 | learning rate 7.543E-06 | lm loss 5.990268E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2912.25 | backward: 6977.97 | optimizer: 988.59 | batch generator: 9.83 | data loader: 0.81
10.0.2.2:  iteration      548/    1000 | elapsed time per iteration (ms): 11084.7 | learning rate 7.557E-06 | lm loss 6.092566E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2934.32 | backward: 7105.53 | optimizer: 987.74 | batch generator: 8.01 | data loader: 1.19
10.0.2.2:  iteration      549/    1000 | elapsed time per iteration (ms): 11028.7 | learning rate 7.571E-06 | lm loss 6.090223E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2972.34 | backward: 7057.25 | optimizer: 994.71 | batch generator: 3.60 | data loader: 0.74
10.0.2.2: [2022-04-01 16:45:02,429] [INFO] [logging.py:60:log_dist] [Rank 0] step=550, skipped=19, lr=[7.571428571428572e-06, 7.571428571428572e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 16:45:02,627] [INFO] [timer.py:164:stop] 0/550, SamplesPerSec=7.345876702785868
10.0.2.2:  iteration      550/    1000 | elapsed time per iteration (ms): 11054.6 | learning rate 7.586E-06 | lm loss 6.326656E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2655.00 | backward: 7080.08 | optimizer: 995.27 | batch generator: 8.60 | data loader: 0.69
10.0.2.2:  iteration      551/    1000 | elapsed time per iteration (ms): 10990.0 | learning rate 7.600E-06 | lm loss 6.155129E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2956.00 | backward: 7037.59 | optimizer: 991.68 | batch generator: 4.42 | data loader: 0.73
10.0.2.2:  iteration      552/    1000 | elapsed time per iteration (ms): 11057.0 | learning rate 7.614E-06 | lm loss 6.380802E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2736.51 | backward: 7052.98 | optimizer: 999.76 | batch generator: 4.21 | data loader: 0.64
10.0.2.2:  iteration      553/    1000 | elapsed time per iteration (ms): 11042.3 | learning rate 7.629E-06 | lm loss 6.336292E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2923.89 | backward: 7089.15 | optimizer: 989.92 | batch generator: 3.45 | data loader: 0.68
10.0.2.2:  iteration      554/    1000 | elapsed time per iteration (ms): 11062.7 | learning rate 7.643E-06 | lm loss 6.389866E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2971.91 | backward: 7095.01 | optimizer: 992.66 | batch generator: 4.43 | data loader: 0.88
10.0.2.2:  iteration      555/    1000 | elapsed time per iteration (ms): 11013.8 | learning rate 7.657E-06 | lm loss 6.217105E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2951.60 | backward: 7075.19 | optimizer: 983.47 | batch generator: 3.03 | data loader: 0.56
10.0.2.2:  iteration      556/    1000 | elapsed time per iteration (ms): 10993.5 | learning rate 7.671E-06 | lm loss 6.396679E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2645.12 | backward: 7039.89 | optimizer: 991.33 | batch generator: 6.33 | data loader: 0.54
10.0.2.2:  iteration      557/    1000 | elapsed time per iteration (ms): 11056.1 | learning rate 7.686E-06 | lm loss 6.178160E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2559.74 | backward: 7070.11 | optimizer: 996.18 | batch generator: 3.76 | data loader: 0.87
10.0.2.2:  iteration      558/    1000 | elapsed time per iteration (ms): 10996.8 | learning rate 7.700E-06 | lm loss 6.585916E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2778.02 | backward: 7031.38 | optimizer: 999.43 | batch generator: 3.14 | data loader: 0.53
10.0.2.2:  iteration      559/    1000 | elapsed time per iteration (ms): 11086.6 | learning rate 7.714E-06 | lm loss 6.298740E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2947.18 | backward: 7101.93 | optimizer: 997.85 | batch generator: 8.22 | data loader: 0.63
10.0.2.2:  iteration      560/    1000 | elapsed time per iteration (ms): 11055.5 | learning rate 7.729E-06 | lm loss 5.897580E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2928.10 | backward: 7084.80 | optimizer: 987.43 | batch generator: 3.26 | data loader: 0.73
10.0.2.2:  iteration      561/    1000 | elapsed time per iteration (ms): 11060.1 | learning rate 7.743E-06 | lm loss 6.128260E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2943.31 | backward: 7080.30 | optimizer: 993.31 | batch generator: 5.43 | data loader: 0.59
10.0.2.2:  iteration      562/    1000 | elapsed time per iteration (ms): 10719.9 | learning rate 7.757E-06 | lm loss 6.771331E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2669.93 | backward: 6864.31 | optimizer: 984.51 | batch generator: 3.27 | data loader: 0.51
10.0.2.2:  iteration      563/    1000 | elapsed time per iteration (ms): 10871.4 | learning rate 7.771E-06 | lm loss 6.512725E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2588.77 | backward: 6939.45 | optimizer: 1003.28 | batch generator: 5.46 | data loader: 2.33
10.0.2.2:  iteration      564/    1000 | elapsed time per iteration (ms): 11023.1 | learning rate 7.786E-06 | lm loss 6.097442E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2838.91 | backward: 7062.50 | optimizer: 982.97 | batch generator: 3.80 | data loader: 0.58
10.0.2.2:  iteration      565/    1000 | elapsed time per iteration (ms): 11017.8 | learning rate 7.800E-06 | lm loss 6.075664E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2624.79 | backward: 7054.49 | optimizer: 996.21 | batch generator: 5.11 | data loader: 2.08
10.0.2.2:  iteration      566/    1000 | elapsed time per iteration (ms): 11095.6 | learning rate 7.814E-06 | lm loss 5.983513E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2706.21 | backward: 7095.07 | optimizer: 995.98 | batch generator: 6.28 | data loader: 0.82
10.0.2.2:  iteration      567/    1000 | elapsed time per iteration (ms): 10952.6 | learning rate 7.829E-06 | lm loss 6.353048E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2655.51 | backward: 7004.34 | optimizer: 988.37 | batch generator: 8.04 | data loader: 0.92
10.0.2.2:  iteration      568/    1000 | elapsed time per iteration (ms): 11038.1 | learning rate 7.843E-06 | lm loss 6.003261E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2810.56 | backward: 7079.88 | optimizer: 979.83 | batch generator: 3.69 | data loader: 0.82
10.0.2.2:  iteration      569/    1000 | elapsed time per iteration (ms): 10956.1 | learning rate 7.857E-06 | lm loss 6.422665E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2612.65 | backward: 7001.87 | optimizer: 991.37 | batch generator: 4.46 | data loader: 0.85
10.0.2.2:  iteration      570/    1000 | elapsed time per iteration (ms): 11135.4 | learning rate 7.871E-06 | lm loss 5.947520E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2660.13 | backward: 7157.14 | optimizer: 993.95 | batch generator: 3.25 | data loader: 0.67
10.0.2.2:  iteration      571/    1000 | elapsed time per iteration (ms): 11071.6 | learning rate 7.886E-06 | lm loss 6.183335E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2585.41 | backward: 7128.20 | optimizer: 986.48 | batch generator: 3.65 | data loader: 0.73
10.0.2.2:  iteration      572/    1000 | elapsed time per iteration (ms): 11038.2 | learning rate 7.900E-06 | lm loss 6.293977E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2854.60 | backward: 7070.72 | optimizer: 984.49 | batch generator: 3.71 | data loader: 0.56
10.0.2.2:  iteration      573/    1000 | elapsed time per iteration (ms): 11002.5 | learning rate 7.914E-06 | lm loss 6.415872E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2622.50 | backward: 7049.38 | optimizer: 987.89 | batch generator: 4.98 | data loader: 0.74
10.0.2.2:  iteration      574/    1000 | elapsed time per iteration (ms): 11033.9 | learning rate 7.929E-06 | lm loss 5.803193E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2785.20 | backward: 7078.93 | optimizer: 994.96 | batch generator: 7.61 | data loader: 0.85
10.0.2.2:  iteration      575/    1000 | elapsed time per iteration (ms): 11130.1 | learning rate 7.943E-06 | lm loss 5.830130E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2990.89 | backward: 7135.91 | optimizer: 994.69 | batch generator: 3.91 | data loader: 0.67
10.0.2.2:  iteration      576/    1000 | elapsed time per iteration (ms): 10957.9 | learning rate 7.957E-06 | lm loss 5.988843E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2954.38 | backward: 7005.89 | optimizer: 993.42 | batch generator: 3.92 | data loader: 0.81
10.0.2.2:  iteration      577/    1000 | elapsed time per iteration (ms): 10807.4 | learning rate 7.971E-06 | lm loss 6.636289E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2895.22 | backward: 6927.00 | optimizer: 980.83 | batch generator: 3.38 | data loader: 0.72
10.0.2.2:  iteration      578/    1000 | elapsed time per iteration (ms): 11001.3 | learning rate 7.986E-06 | lm loss 6.352675E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2964.89 | backward: 7039.36 | optimizer: 992.85 | batch generator: 6.56 | data loader: 0.73
10.0.2.2:  iteration      579/    1000 | elapsed time per iteration (ms): 10963.9 | learning rate 8.000E-06 | lm loss 6.310839E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2636.19 | backward: 7027.31 | optimizer: 984.63 | batch generator: 3.23 | data loader: 0.60
10.0.2.2:  iteration      580/    1000 | elapsed time per iteration (ms): 10908.4 | learning rate 8.014E-06 | lm loss 5.973102E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2653.96 | backward: 6976.66 | optimizer: 995.91 | batch generator: 4.06 | data loader: 0.82
10.0.2.2:  iteration      581/    1000 | elapsed time per iteration (ms): 10970.2 | learning rate 8.029E-06 | lm loss 6.398882E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2691.03 | backward: 7045.25 | optimizer: 991.82 | batch generator: 3.23 | data loader: 0.76
10.0.2.2:  iteration      582/    1000 | elapsed time per iteration (ms): 29937.6 | learning rate 8.043E-06 | lm loss 6.576668E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2939.54 | backward: 21787.29 | optimizer: 5205.70 | batch generator: 3.62 | data loader: 0.48
10.0.2.2:  iteration      583/    1000 | elapsed time per iteration (ms): 11398.2 | learning rate 8.057E-06 | lm loss 6.303817E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2985.48 | backward: 7325.44 | optimizer: 1063.91 | batch generator: 15.69 | data loader: 1.76
10.0.2.2:  iteration      584/    1000 | elapsed time per iteration (ms): 10382.8 | learning rate 8.071E-06 | lm loss 6.767146E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2659.65 | backward: 6636.14 | optimizer: 1055.54 | batch generator: 5.14 | data loader: 1.36
10.0.2.2:  iteration      585/    1000 | elapsed time per iteration (ms): 11060.8 | learning rate 8.086E-06 | lm loss 6.674373E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2659.85 | backward: 7057.41 | optimizer: 1074.73 | batch generator: 5.75 | data loader: 1.05
10.0.2.2:  iteration      586/    1000 | elapsed time per iteration (ms): 11155.5 | learning rate 8.100E-06 | lm loss 6.177554E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2654.30 | backward: 7145.10 | optimizer: 1055.19 | batch generator: 4.94 | data loader: 1.27
10.0.2.2:  iteration      587/    1000 | elapsed time per iteration (ms): 11145.8 | learning rate 8.114E-06 | lm loss 6.193710E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2933.96 | backward: 7122.37 | optimizer: 1015.79 | batch generator: 4.76 | data loader: 1.23
10.0.2.2:  iteration      588/    1000 | elapsed time per iteration (ms): 11094.3 | learning rate 8.129E-06 | lm loss 6.081407E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2873.37 | backward: 7066.03 | optimizer: 1040.40 | batch generator: 7.14 | data loader: 1.04
10.0.2.2:  iteration      589/    1000 | elapsed time per iteration (ms): 10405.3 | learning rate 8.143E-06 | lm loss 6.254849E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2683.41 | backward: 6647.45 | optimizer: 1030.96 | batch generator: 5.73 | data loader: 0.97
10.0.2.2:  iteration      590/    1000 | elapsed time per iteration (ms): 11166.7 | learning rate 8.157E-06 | lm loss 6.172581E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2981.81 | backward: 7197.49 | optimizer: 983.25 | batch generator: 4.27 | data loader: 1.12
10.0.2.2:  iteration      591/    1000 | elapsed time per iteration (ms): 11128.0 | learning rate 8.171E-06 | lm loss 5.361551E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2905.67 | backward: 7150.53 | optimizer: 986.51 | batch generator: 6.51 | data loader: 1.00
10.0.2.2:  iteration      592/    1000 | elapsed time per iteration (ms): 11012.1 | learning rate 8.186E-06 | lm loss 5.815957E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2962.56 | backward: 7054.86 | optimizer: 987.02 | batch generator: 4.58 | data loader: 0.65
10.0.2.2:  iteration      593/    1000 | elapsed time per iteration (ms): 21154.9 | learning rate 8.200E-06 | lm loss 6.204409E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 13168.30 | backward: 6993.54 | optimizer: 989.24 | batch generator: 10606.19 | data loader: 0.76
10.0.2.2:  iteration      594/    1000 | elapsed time per iteration (ms): 10342.4 | learning rate 8.214E-06 | lm loss 6.989276E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2584.33 | backward: 6591.43 | optimizer: 988.10 | batch generator: 3.65 | data loader: 0.77
10.0.2.2:  iteration      595/    1000 | elapsed time per iteration (ms): 11043.1 | learning rate 8.229E-06 | lm loss 5.709893E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2919.02 | backward: 7098.33 | optimizer: 983.35 | batch generator: 4.11 | data loader: 0.92
10.0.2.2:  iteration      596/    1000 | elapsed time per iteration (ms): 20755.5 | learning rate 8.243E-06 | lm loss 6.472040E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2702.65 | backward: 16797.19 | optimizer: 984.04 | batch generator: 5.68 | data loader: 0.74
10.0.2.2:  iteration      597/    1000 | elapsed time per iteration (ms): 10893.7 | learning rate 8.257E-06 | lm loss 6.516736E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2931.75 | backward: 6968.86 | optimizer: 988.98 | batch generator: 6.82 | data loader: 0.74
10.0.2.2:  iteration      598/    1000 | elapsed time per iteration (ms): 11132.4 | learning rate 8.271E-06 | lm loss 6.104316E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2903.02 | backward: 7141.33 | optimizer: 981.99 | batch generator: 4.31 | data loader: 0.67
10.0.2.2:  iteration      599/    1000 | elapsed time per iteration (ms): 11035.2 | learning rate 8.286E-06 | lm loss 6.328571E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2977.53 | backward: 7060.61 | optimizer: 994.42 | batch generator: 3.62 | data loader: 0.95
10.0.2.2: [2022-04-01 16:54:50,929] [INFO] [logging.py:60:log_dist] [Rank 0] step=600, skipped=19, lr=[8.285714285714287e-06, 8.285714285714287e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
10.0.2.2: [2022-04-01 16:54:50,930] [INFO] [timer.py:164:stop] 0/600, SamplesPerSec=7.339876218645109
10.0.2.2:  iteration      600/    1000 | elapsed time per iteration (ms): 10942.8 | learning rate 8.300E-06 | lm loss 6.225661E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2697.87 | backward: 7029.72 | optimizer: 988.95 | batch generator: 3.46 | data loader: 0.86
10.0.2.2:  iteration      601/    1000 | elapsed time per iteration (ms): 11017.0 | learning rate 8.314E-06 | lm loss 6.485181E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2973.58 | backward: 7043.59 | optimizer: 997.62 | batch generator: 6.48 | data loader: 0.39
10.0.2.2:  iteration      602/    1000 | elapsed time per iteration (ms): 11080.9 | learning rate 8.329E-06 | lm loss 5.470654E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2612.73 | backward: 7105.23 | optimizer: 981.93 | batch generator: 4.08 | data loader: 0.62
10.0.2.2:  iteration      603/    1000 | elapsed time per iteration (ms): 11067.2 | learning rate 8.343E-06 | lm loss 6.296871E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2961.27 | backward: 7099.11 | optimizer: 990.72 | batch generator: 4.81 | data loader: 0.66
10.0.2.2:  iteration      604/    1000 | elapsed time per iteration (ms): 11004.1 | learning rate 8.357E-06 | lm loss 6.129250E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2970.14 | backward: 7039.23 | optimizer: 980.41 | batch generator: 3.43 | data loader: 0.65
10.0.2.2:  iteration      605/    1000 | elapsed time per iteration (ms): 10936.6 | learning rate 8.371E-06 | lm loss 6.358054E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2943.40 | backward: 7001.66 | optimizer: 987.15 | batch generator: 6.05 | data loader: 0.73
10.0.2.2:  iteration      606/    1000 | elapsed time per iteration (ms): 10999.0 | learning rate 8.386E-06 | lm loss 6.207593E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2762.57 | backward: 7034.97 | optimizer: 1010.07 | batch generator: 5.31 | data loader: 0.97
10.0.2.2:  iteration      607/    1000 | elapsed time per iteration (ms): 10985.9 | learning rate 8.400E-06 | lm loss 6.142091E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2718.15 | backward: 7032.05 | optimizer: 993.38 | batch generator: 3.63 | data loader: 0.80
10.0.2.2:  iteration      608/    1000 | elapsed time per iteration (ms): 10949.4 | learning rate 8.414E-06 | lm loss 6.000024E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2719.70 | backward: 7010.10 | optimizer: 992.69 | batch generator: 5.11 | data loader: 0.76
10.0.2.2:  iteration      609/    1000 | elapsed time per iteration (ms): 10948.1 | learning rate 8.429E-06 | lm loss 6.726332E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2551.92 | backward: 6991.03 | optimizer: 996.25 | batch generator: 3.69 | data loader: 0.68
10.0.2.2:  iteration      610/    1000 | elapsed time per iteration (ms): 10973.1 | learning rate 8.443E-06 | lm loss 6.175786E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2665.94 | backward: 7029.26 | optimizer: 993.16 | batch generator: 2.99 | data loader: 0.57
10.0.2.2:  iteration      611/    1000 | elapsed time per iteration (ms): 11074.1 | learning rate 8.457E-06 | lm loss 6.418377E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2679.78 | backward: 7089.76 | optimizer: 991.33 | batch generator: 3.98 | data loader: 0.67
10.0.2.2:  iteration      612/    1000 | elapsed time per iteration (ms): 10998.6 | learning rate 8.471E-06 | lm loss 5.897173E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2689.96 | backward: 7032.67 | optimizer: 988.71 | batch generator: 3.85 | data loader: 0.72
10.0.2.2:  iteration      613/    1000 | elapsed time per iteration (ms): 10388.6 | learning rate 8.486E-06 | lm loss 6.919213E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2583.40 | backward: 6642.12 | optimizer: 1017.20 | batch generator: 3.84 | data loader: 0.71
10.0.2.2:  iteration      614/    1000 | elapsed time per iteration (ms): 11005.4 | learning rate 8.500E-06 | lm loss 6.044069E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2934.60 | backward: 7050.17 | optimizer: 986.72 | batch generator: 4.67 | data loader: 0.65
10.0.2.2:  iteration      615/    1000 | elapsed time per iteration (ms): 11055.4 | learning rate 8.514E-06 | lm loss 6.679047E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2716.54 | backward: 7091.20 | optimizer: 981.90 | batch generator: 3.12 | data loader: 0.57
10.0.2.2:  iteration      616/    1000 | elapsed time per iteration (ms): 10894.5 | learning rate 8.529E-06 | lm loss 5.956374E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2900.72 | backward: 6966.32 | optimizer: 991.32 | batch generator: 5.77 | data loader: 0.71
10.0.2.2:  iteration      617/    1000 | elapsed time per iteration (ms): 10879.0 | learning rate 8.543E-06 | lm loss 6.544362E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2572.27 | backward: 6962.53 | optimizer: 993.68 | batch generator: 2.95 | data loader: 0.56
10.0.2.2:  iteration      618/    1000 | elapsed time per iteration (ms): 10972.9 | learning rate 8.557E-06 | lm loss 6.293725E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2976.04 | backward: 7014.91 | optimizer: 979.68 | batch generator: 3.55 | data loader: 0.53
10.0.2.2:  iteration      619/    1000 | elapsed time per iteration (ms): 10882.6 | learning rate 8.571E-06 | lm loss 6.521215E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2928.72 | backward: 6960.31 | optimizer: 989.94 | batch generator: 5.57 | data loader: 0.72
10.0.2.2:  iteration      620/    1000 | elapsed time per iteration (ms): 10901.0 | learning rate 8.586E-06 | lm loss 6.619790E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2632.53 | backward: 6962.32 | optimizer: 992.46 | batch generator: 4.11 | data loader: 1.05
10.0.2.2:  iteration      621/    1000 | elapsed time per iteration (ms): 10882.5 | learning rate 8.600E-06 | lm loss 6.363366E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2645.20 | backward: 6962.01 | optimizer: 992.96 | batch generator: 3.39 | data loader: 0.72
10.0.2.2:  iteration      622/    1000 | elapsed time per iteration (ms): 10979.8 | learning rate 8.614E-06 | lm loss 6.214638E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2922.12 | backward: 7023.78 | optimizer: 988.11 | batch generator: 3.76 | data loader: 0.71
10.0.2.2:  iteration      623/    1000 | elapsed time per iteration (ms): 10987.5 | learning rate 8.629E-06 | lm loss 6.382585E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2971.40 | backward: 7024.56 | optimizer: 987.47 | batch generator: 5.88 | data loader: 0.68
10.0.2.2:  iteration      624/    1000 | elapsed time per iteration (ms): 10774.5 | learning rate 8.643E-06 | lm loss 6.314897E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2629.86 | backward: 6886.08 | optimizer: 986.22 | batch generator: 3.01 | data loader: 0.66
10.0.2.2:  iteration      625/    1000 | elapsed time per iteration (ms): 10934.2 | learning rate 8.657E-06 | lm loss 6.629543E+00 | loss scale 16384.0 |
10.0.2.2: time (ms) | forward: 2580.42 | backward: 6999.21 | optimizer: 991.17 | batch generator: 4.02 | data loader: 0.59
