diff --git a/run_bert_triple_classifier.py b/run_bert_triple_classifier.py
index b23b84f..b13eabc 100644
--- a/run_bert_triple_classifier.py
+++ b/run_bert_triple_classifier.py
@@ -41,8 +41,6 @@ from pytorch_pretrained_bert.modeling import BertForSequenceClassification, Bert
 from pytorch_pretrained_bert.tokenization import BertTokenizer
 from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule
 
-os.environ['CUDA_VISIBLE_DEVICES']= '6'
-#torch.backends.cudnn.deterministic = True
 
 logger = logging.getLogger(__name__)
 
@@ -484,6 +482,7 @@ def main():
                              "Positive power of 2: static loss scaling value.\n")
     parser.add_argument('--server_ip', type=str, default='', help="Can be used for distant debugging.")
     parser.add_argument('--server_port', type=str, default='', help="Can be used for distant debugging.")
+    parser.add_argument("--iters", type=int, default=200, help="the iteration where to stop")
     args = parser.parse_args()
 
     if args.server_ip and args.server_port:
@@ -530,10 +529,12 @@ def main():
     if not args.do_train and not args.do_eval:
         raise ValueError("At least one of `do_train` or `do_eval` must be True.")
 
-    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:
-        raise ValueError("Output directory ({}) already exists and is not empty.".format(args.output_dir))
-    if not os.path.exists(args.output_dir):
+    #if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:
+    #    raise ValueError("Output directory ({}) already exists and is not empty.".format(args.output_dir))
+    try:
         os.makedirs(args.output_dir)
+    except:
+        pass
 
     task_name = args.task_name.lower()
 
@@ -569,13 +570,13 @@ def main():
     model.to(device)
     if args.local_rank != -1:
         try:
-            from apex.parallel import DistributedDataParallel as DDP
+            from torch.nn.parallel import DistributedDataParallel as DDP
         except ImportError:
             raise ImportError("Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.")
 
         model = DDP(model)
-    elif n_gpu > 1:
-        model = torch.nn.DataParallel(model)
+    # elif n_gpu > 1:
+        #model = torch.nn.DataParallel(model)
         #model = torch.nn.parallel.data_parallel(model)
     # Prepare optimizer
     param_optimizer = list(model.named_parameters())
@@ -638,6 +639,10 @@ def main():
             tr_loss = 0
             nb_tr_examples, nb_tr_steps = 0, 0
             for step, batch in enumerate(tqdm(train_dataloader, desc="Iteration")):
+
+                if args.num_train_epochs == 1.0 and step == args.iters:
+                    break                
+
                 batch = tuple(t.to(device) for t in batch)
                 input_ids, input_mask, segment_ids, label_ids = batch
 
